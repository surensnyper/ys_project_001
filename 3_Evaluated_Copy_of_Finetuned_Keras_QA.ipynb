{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3. Evaluated_Copy of Finetuned_Keras_QA.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "qH4ySjYTxxyI",
        "yvk0NddfsrNJ",
        "dnAhc8NNaEkO",
        "iSFE_SznKUgT",
        "IBZgY4YWu3fO",
        "NE5v3CmhzrhV"
      ],
      "authorship_tag": "ABX9TyPiOFiBgl+xu80oogGWD1k0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/surensnyper/ys_project_001/blob/main/3_Evaluated_Copy_of_Finetuned_Keras_QA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pre-trained Model\n",
        "\n",
        "1.   video : https://www.youtube.com/watch?v=LuApA264Wbs&t=947s\n",
        "2.   code : https://github.com/kimwoonggon/publicservant_AI/blob/master/(Uncased_Squad_V1_1)_%EC%BC%80%EB%9D%BC%EC%8A%A4%EB%A1%9C_Q%26A_%EA%B5%AC%ED%98%84%ED%95%98%EA%B8%B0.ipynb"
      ],
      "metadata": {
        "id": "qH4ySjYTxxyI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1s3L3AYmRa6H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75f41959-5850-4778-965b-1fe7cdd8d1cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-30 21:11:01--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.204.128, 172.217.203.128, 172.253.123.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.204.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 407727028 (389M) [application/zip]\n",
            "Saving to: ‘uncased_L-12_H-768_A-12.zip’\n",
            "\n",
            "uncased_L-12_H-768_ 100%[===================>] 388.84M   261MB/s    in 1.5s    \n",
            "\n",
            "2022-01-30 21:11:02 (261 MB/s) - ‘uncased_L-12_H-768_A-12.zip’ saved [407727028/407727028]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# BERT Model Download\n",
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
        "\n",
        "# bert folder construction in the Colab\n",
        "if \"bert\" not in os.listdir():\n",
        "  os.makedirs(\"bert\")\n",
        "else:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the Model zip file in the bert folder\n",
        "bert_zip  = zipfile.ZipFile('uncased_L-12_H-768_A-12.zip')\n",
        "bert_zip.extractall('bert')\n",
        "\n",
        "bert_zip.close()"
      ],
      "metadata": {
        "id": "KGCk0CNuUo68"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Functoin Construction for copying the folder\n",
        "def copytree(src, dst, symlinks=False, ignore=None):\n",
        "  for item in os.listdir(src):\n",
        "    s = os.path.join(src, item)\n",
        "    d = os.path.join(dst, item)\n",
        "    if os.path.isdir(s):\n",
        "      shutil.copytrees(s, d, symlinks, ignore)\n",
        "    else:\n",
        "      shutil.copy2(s, d)"
      ],
      "metadata": {
        "id": "MfAmqbVFUwnz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The unzipped folder is \"bert/uncased_L-12_H-768_A-12\", but copying & moving the model to the bert folder for easier analysis\n",
        "copytree(\"bert/uncased_L-12_H-768_A-12\", \"bert\")"
      ],
      "metadata": {
        "id": "y_8xHilJVz_U"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Google drive and Colab Connecting\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlEKI_0IW42k",
        "outputId": "1ecb3b22-d46a-4481-ff10-2f7f92956bb8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Required modules are imported, e.g. Tensorflow, Pandas, Numpy, Keras etc\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "import keras as keras\n",
        "from keras.models import load_model\n",
        "from keras import backend as K\n",
        "from keras import Input, Model\n",
        "from keras import optimizers\n",
        "from keras.layers import Layer\n",
        "\n",
        "import codecs\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "import json"
      ],
      "metadata": {
        "id": "cdZWE1XhXx6q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To make various logs nor popped up\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "os.environ['TF_CPP_WIN_LOG_LEVEL']='3'\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR) # Version updated (tensorflow logging tf.logging -> tf.compat.v1.logging)"
      ],
      "metadata": {
        "id": "u2NAZBYMYl8e"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A module 'keras-bert' install, for using bert easily in keras\n",
        "!pip install keras-bert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ET_bXlFHZIyk",
        "outputId": "b70b82db-5c41-4132-f78a-5dab379879b1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-bert\n",
            "  Downloading keras-bert-0.89.0.tar.gz (25 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-bert) (1.19.5)\n",
            "Collecting keras-transformer==0.40.0\n",
            "  Downloading keras-transformer-0.40.0.tar.gz (9.7 kB)\n",
            "Collecting keras-pos-embd==0.13.0\n",
            "  Downloading keras-pos-embd-0.13.0.tar.gz (5.6 kB)\n",
            "Collecting keras-multi-head==0.29.0\n",
            "  Downloading keras-multi-head-0.29.0.tar.gz (13 kB)\n",
            "Collecting keras-layer-normalization==0.16.0\n",
            "  Downloading keras-layer-normalization-0.16.0.tar.gz (3.9 kB)\n",
            "Collecting keras-position-wise-feed-forward==0.8.0\n",
            "  Downloading keras-position-wise-feed-forward-0.8.0.tar.gz (4.1 kB)\n",
            "Collecting keras-embed-sim==0.10.0\n",
            "  Downloading keras-embed-sim-0.10.0.tar.gz (3.6 kB)\n",
            "Collecting keras-self-attention==0.51.0\n",
            "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
            "Building wheels for collected packages: keras-bert, keras-transformer, keras-embed-sim, keras-layer-normalization, keras-multi-head, keras-pos-embd, keras-position-wise-feed-forward, keras-self-attention\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.89.0-py3-none-any.whl size=33517 sha256=e7de3771803f88cd08d1f2d4952f249363653396c8bcd6f9c681621b35883876\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/e8/45/842b3a39831261aef9154b907eacbc4ac99499a99ae829b06f\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.40.0-py3-none-any.whl size=12305 sha256=20386c41715347432a05e698faa49454840553e48b4a8b658745f72d835e6ee8\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/68/26/692ed21edd832833c3b0a0e21615bcacd99ca458b3f9ed571f\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.10.0-py3-none-any.whl size=3960 sha256=d7115ab1bc5c2ec5c986acd4eaeb5badd1f77686ecdda9d0e943ef423f7d1c3d\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/67/b5/d847588d075895281e1cf5590f819bd4cf076a554872268bd5\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.16.0-py3-none-any.whl size=4668 sha256=6ea053ff25328a1960a2989c9ae2ddca2c415f2db8cb3bf118513d4d723aae8c\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/5d/1c/2e619f594f69fbcf8bc20943b27d414871c409be053994813e\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.29.0-py3-none-any.whl size=14993 sha256=d96bedfde43054d5cbf7212f9514d8e717a37842c3e3fd8a7f0624eb29f589e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/aa/3c/9d15d24005179dae08ff291ce99c754b296347817d076fd9fb\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.13.0-py3-none-any.whl size=6962 sha256=7c1464c119db2efcff5dd0ffbb3534057b30a2d490b27de0cb9e92aecf88bb69\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/c1/a0/dc44fcf68c857b7ff6be9a97e675e5adf51022eff1169b042f\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.8.0-py3-none-any.whl size=4983 sha256=29a2b1b97de9ac53279680df9bbeba2b379c171f00cef925474bb8ada71fddb4\n",
            "  Stored in directory: /root/.cache/pip/wheels/c2/75/6f/d42f6e051506f442daeba53ff1e2d21a5f20ef8c411610f2bb\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18912 sha256=72ecad5b88c516fe69863b4e66d3f0ddbdc9b22544ab18dde70e3a27c917fdc8\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/b1/a8/5ee00cc137940b2f6fa198212e8f45d813d0e0d9c3a04035a3\n",
            "Successfully built keras-bert keras-transformer keras-embed-sim keras-layer-normalization keras-multi-head keras-pos-embd keras-position-wise-feed-forward keras-self-attention\n",
            "Installing collected packages: keras-self-attention, keras-position-wise-feed-forward, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-embed-sim, keras-transformer, keras-bert\n",
            "Successfully installed keras-bert-0.89.0 keras-embed-sim-0.10.0 keras-layer-normalization-0.16.0 keras-multi-head-0.29.0 keras-pos-embd-0.13.0 keras-position-wise-feed-forward-0.8.0 keras-self-attention-0.51.0 keras-transformer-0.40.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# An optimizer 'keras-radam' import, which is an updated optimizer of 'Adam'\n",
        "# 'radam' is to update learning rate from 0 to target value 'gradually', so training session is more stable\n",
        "!pip install keras-radam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVYNcL3Jcs2E",
        "outputId": "62f6ed02-ea54-4805-ef51-437a703d981c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-radam\n",
            "  Downloading keras-radam-0.15.0.tar.gz (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-radam) (1.19.5)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (from keras-radam) (2.7.0)\n",
            "Building wheels for collected packages: keras-radam\n",
            "  Building wheel for keras-radam (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-radam: filename=keras_radam-0.15.0-py3-none-any.whl size=14686 sha256=f77b91e909badfd21f95e2e4d1183e33b7d8e77d1d90be544da7fac25dfc58aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/6a/5f/d674f3b7b4d504b03148abd675e3703ba00c31763c04a2fc20\n",
            "Successfully built keras-radam\n",
            "Installing collected packages: keras-radam\n",
            "Successfully installed keras-radam-0.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Other libraries for using bert model in the keras-bert libraries\n",
        "from keras_bert import load_trained_model_from_checkpoint, load_vocabulary\n",
        "from keras_bert import Tokenizer\n",
        "from keras_bert import AdamWarmup, calc_train_steps\n",
        "\n",
        "from keras_radam.training import RAdamOptimizer # Original RAdam changed to RAdamOptimizer\n",
        "# from keras_radam import RAdam\n",
        "# https://github.com/CyberZHG/keras-radam (KERAS-RADAM)"
      ],
      "metadata": {
        "id": "bR6OnIj-cxsr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters set up\n",
        "\n",
        "# Input sequence maximum length setup (Question + Context)\n",
        "SEQ_LEN = 384\n",
        "\n",
        "# Batch size (training size at one time)\n",
        "BATCH_SIZE = 12\n",
        "\n",
        "EPOCH = 2\n",
        "\n",
        "# Learning rate is set in very small number like 1.5e-5.\n",
        "# In fine tuning session, Learning rate must be very small \n",
        "# (Optimizer RAdam starts from LR 0 to 1.5e-5)\n",
        "LR = 1.5e-5\n",
        "\n",
        "# Model location designation\n",
        "pretrained_path = \"bert\"\n",
        "# Pretrained hyperparameters like weights\n",
        "checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
        "# Word corpus file location designation\n",
        "vocab_path =  os.path.join(pretrained_path, 'vocab.txt')\n",
        "\n",
        "# Configuration file designation\n",
        "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
        "\n",
        "# Column name of context\n",
        "DATA_COLUMN = \"context\"\n",
        "# Column name of question\n",
        "QUESTION_COLUMN = \"question\"\n",
        "# Column name of answer ('text')\n",
        "TEXT = \"text\""
      ],
      "metadata": {
        "id": "orCUPU80ei-c"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary Construction for indexing each word\n",
        "token_dict = {}\n",
        "with codecs.open(vocab_path, 'r', 'utf8') as reader:\n",
        "  for line in reader:\n",
        "    token = line.strip()\n",
        "    token_dict[token] = len(token_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "4lAIX0YSPg9V",
        "outputId": "85468b27-ec47-4195-bdd3-08493c0b6186"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer Construction\n",
        "tokenizer = Tokenizer(token_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "jd6_hhR5Peqp",
        "outputId": "a38f9e57-8465-42c1-b038-3788a20f1228"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Pretrained Model\n",
        "model = load_trained_model_from_checkpoint (\n",
        "    config_path,\n",
        "    checkpoint_path,\n",
        "    training = False,\n",
        "    trainable = True,\n",
        "    seq_len = SEQ_LEN\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSBvWN7g9nZU",
        "outputId": "243e707f-df62-40af-d881-7468b9471727"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Input-Token (InputLayer)       [(None, 384)]        0           []                               \n",
            "                                                                                                  \n",
            " Input-Segment (InputLayer)     [(None, 384)]        0           []                               \n",
            "                                                                                                  \n",
            " Embedding-Token (TokenEmbeddin  [(None, 384, 768),  23440896    ['Input-Token[0][0]']            \n",
            " g)                              (30522, 768)]                                                    \n",
            "                                                                                                  \n",
            " Embedding-Segment (Embedding)  (None, 384, 768)     1536        ['Input-Segment[0][0]']          \n",
            "                                                                                                  \n",
            " Embedding-Token-Segment (Add)  (None, 384, 768)     0           ['Embedding-Token[0][0]',        \n",
            "                                                                  'Embedding-Segment[0][0]']      \n",
            "                                                                                                  \n",
            " Embedding-Position (PositionEm  (None, 384, 768)    294912      ['Embedding-Token-Segment[0][0]']\n",
            " bedding)                                                                                         \n",
            "                                                                                                  \n",
            " Embedding-Dropout (Dropout)    (None, 384, 768)     0           ['Embedding-Position[0][0]']     \n",
            "                                                                                                  \n",
            " Embedding-Norm (LayerNormaliza  (None, 384, 768)    1536        ['Embedding-Dropout[0][0]']      \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " Encoder-1-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Embedding-Norm[0][0]']         \n",
            " on (MultiHeadAttention)                                                                          \n",
            "                                                                                                  \n",
            " Encoder-1-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-1-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-1-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Embedding-Norm[0][0]',         \n",
            " on-Add (Add)                                                     'Encoder-1-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-1-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-1-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-1-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-1-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-1-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-1-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-1-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-1-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-1-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-1-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-1-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-2-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-1-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-2-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-2-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-2-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-1-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-2-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-2-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-2-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-2-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-2-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-2-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-2-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-2-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-2-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-2-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-2-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-2-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-3-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-2-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-3-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-3-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-3-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-2-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-3-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-3-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-3-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-3-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-3-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-3-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-3-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-3-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-3-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-3-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-3-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-3-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-4-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-3-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-4-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-4-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-4-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-3-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-4-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-4-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-4-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-4-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-4-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-4-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-4-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-4-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-4-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-4-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-4-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-4-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-5-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-4-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-5-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-5-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-5-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-4-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-5-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-5-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-5-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-5-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-5-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-5-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-5-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-5-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-5-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-5-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-5-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-5-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-6-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-5-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-6-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-6-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-6-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-5-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-6-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-6-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-6-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-6-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-6-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-6-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-6-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-6-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-6-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-6-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-6-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-6-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-7-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-6-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-7-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-7-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-7-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-6-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-7-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-7-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-7-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-7-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-7-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-7-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-7-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-7-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-7-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-7-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-7-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-7-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-8-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-7-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-8-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-8-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-8-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-7-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-8-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-8-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-8-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-8-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-8-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-8-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-8-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-8-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-8-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-8-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-8-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-8-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-9-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-8-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-9-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-9-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-9-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-8-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-9-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-9-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-9-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-9-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-9-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-9-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-9-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-9-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-9-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-9-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-9-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-9-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-10-MultiHeadSelfAttent  (None, 384, 768)    2362368     ['Encoder-9-FeedForward-Norm[0][0\n",
            " ion (MultiHeadAttention)                                        ]']                              \n",
            "                                                                                                  \n",
            " Encoder-10-MultiHeadSelfAttent  (None, 384, 768)    0           ['Encoder-10-MultiHeadSelfAttenti\n",
            " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
            "                                                                                                  \n",
            " Encoder-10-MultiHeadSelfAttent  (None, 384, 768)    0           ['Encoder-9-FeedForward-Norm[0][0\n",
            " ion-Add (Add)                                                   ]',                              \n",
            "                                                                  'Encoder-10-MultiHeadSelfAttenti\n",
            "                                                                 on-Dropout[0][0]']               \n",
            "                                                                                                  \n",
            " Encoder-10-MultiHeadSelfAttent  (None, 384, 768)    1536        ['Encoder-10-MultiHeadSelfAttenti\n",
            " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-10-FeedForward (FeedFo  (None, 384, 768)    4722432     ['Encoder-10-MultiHeadSelfAttenti\n",
            " rward)                                                          on-Norm[0][0]']                  \n",
            "                                                                                                  \n",
            " Encoder-10-FeedForward-Dropout  (None, 384, 768)    0           ['Encoder-10-FeedForward[0][0]'] \n",
            "  (Dropout)                                                                                       \n",
            "                                                                                                  \n",
            " Encoder-10-FeedForward-Add (Ad  (None, 384, 768)    0           ['Encoder-10-MultiHeadSelfAttenti\n",
            " d)                                                              on-Norm[0][0]',                  \n",
            "                                                                  'Encoder-10-FeedForward-Dropout[\n",
            "                                                                 0][0]']                          \n",
            "                                                                                                  \n",
            " Encoder-10-FeedForward-Norm (L  (None, 384, 768)    1536        ['Encoder-10-FeedForward-Add[0][0\n",
            " ayerNormalization)                                              ]']                              \n",
            "                                                                                                  \n",
            " Encoder-11-MultiHeadSelfAttent  (None, 384, 768)    2362368     ['Encoder-10-FeedForward-Norm[0][\n",
            " ion (MultiHeadAttention)                                        0]']                             \n",
            "                                                                                                  \n",
            " Encoder-11-MultiHeadSelfAttent  (None, 384, 768)    0           ['Encoder-11-MultiHeadSelfAttenti\n",
            " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
            "                                                                                                  \n",
            " Encoder-11-MultiHeadSelfAttent  (None, 384, 768)    0           ['Encoder-10-FeedForward-Norm[0][\n",
            " ion-Add (Add)                                                   0]',                             \n",
            "                                                                  'Encoder-11-MultiHeadSelfAttenti\n",
            "                                                                 on-Dropout[0][0]']               \n",
            "                                                                                                  \n",
            " Encoder-11-MultiHeadSelfAttent  (None, 384, 768)    1536        ['Encoder-11-MultiHeadSelfAttenti\n",
            " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-11-FeedForward (FeedFo  (None, 384, 768)    4722432     ['Encoder-11-MultiHeadSelfAttenti\n",
            " rward)                                                          on-Norm[0][0]']                  \n",
            "                                                                                                  \n",
            " Encoder-11-FeedForward-Dropout  (None, 384, 768)    0           ['Encoder-11-FeedForward[0][0]'] \n",
            "  (Dropout)                                                                                       \n",
            "                                                                                                  \n",
            " Encoder-11-FeedForward-Add (Ad  (None, 384, 768)    0           ['Encoder-11-MultiHeadSelfAttenti\n",
            " d)                                                              on-Norm[0][0]',                  \n",
            "                                                                  'Encoder-11-FeedForward-Dropout[\n",
            "                                                                 0][0]']                          \n",
            "                                                                                                  \n",
            " Encoder-11-FeedForward-Norm (L  (None, 384, 768)    1536        ['Encoder-11-FeedForward-Add[0][0\n",
            " ayerNormalization)                                              ]']                              \n",
            "                                                                                                  \n",
            " Encoder-12-MultiHeadSelfAttent  (None, 384, 768)    2362368     ['Encoder-11-FeedForward-Norm[0][\n",
            " ion (MultiHeadAttention)                                        0]']                             \n",
            "                                                                                                  \n",
            " Encoder-12-MultiHeadSelfAttent  (None, 384, 768)    0           ['Encoder-12-MultiHeadSelfAttenti\n",
            " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
            "                                                                                                  \n",
            " Encoder-12-MultiHeadSelfAttent  (None, 384, 768)    0           ['Encoder-11-FeedForward-Norm[0][\n",
            " ion-Add (Add)                                                   0]',                             \n",
            "                                                                  'Encoder-12-MultiHeadSelfAttenti\n",
            "                                                                 on-Dropout[0][0]']               \n",
            "                                                                                                  \n",
            " Encoder-12-MultiHeadSelfAttent  (None, 384, 768)    1536        ['Encoder-12-MultiHeadSelfAttenti\n",
            " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-12-FeedForward (FeedFo  (None, 384, 768)    4722432     ['Encoder-12-MultiHeadSelfAttenti\n",
            " rward)                                                          on-Norm[0][0]']                  \n",
            "                                                                                                  \n",
            " Encoder-12-FeedForward-Dropout  (None, 384, 768)    0           ['Encoder-12-FeedForward[0][0]'] \n",
            "  (Dropout)                                                                                       \n",
            "                                                                                                  \n",
            " Encoder-12-FeedForward-Add (Ad  (None, 384, 768)    0           ['Encoder-12-MultiHeadSelfAttenti\n",
            " d)                                                              on-Norm[0][0]',                  \n",
            "                                                                  'Encoder-12-FeedForward-Dropout[\n",
            "                                                                 0][0]']                          \n",
            "                                                                                                  \n",
            " Encoder-12-FeedForward-Norm (L  (None, 384, 768)    1536        ['Encoder-12-FeedForward-Add[0][0\n",
            " ayerNormalization)                                              ]']                              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,793,344\n",
            "Trainable params: 108,793,344\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For Transfer Learning, Customized Layer needs to be added after 12 Encoder\n",
        "# by defining 'Non-masking' function, BERT Model's masked tensors disclosed\n",
        "\n",
        "class NonMasking(Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    self.supports_masking = True\n",
        "    super(NonMasking, self).__init__(**kwargs)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    input_shape = input_shape\n",
        "  \n",
        "  def compute_mask(self, input, input_mask = None):\n",
        "    return None\n",
        "\n",
        "  def call(self, x, mask = None):\n",
        "    return x\n",
        "\n",
        "  def get_output_shape_for(self, input_shape):\n",
        "    return input_shape"
      ],
      "metadata": {
        "id": "p8OuSxoJYLBd"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation function definition\n",
        "def bert_gelu(x):\n",
        "  \"\"\"Gaussian Error Linear Unit.\n",
        "  This is a smoother version of the RELU.\n",
        "  Original paper: https://arxiv.org/abs/1606.08415\n",
        "  Args:\n",
        "    x : float Tensor to perform activation\n",
        "  Returns:\n",
        "    'x' with the GELU activation applied.\n",
        "  \"\"\"\n",
        "  cdf = 0.5*(1.0+ K.tanh(\n",
        "      (np.sqrt(2/np.pi)*(x+0.044715 * K.pow(x,3)))))\n",
        "  return x*cdf"
      ],
      "metadata": {
        "id": "91iL3th6aiU1"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation Function Layer attached to Transformer\n",
        "class Start_End_Prediction(Layer):\n",
        "  def __init__(self, seq_len, **kwargs):\n",
        "    self.seq_len = SEQ_LEN\n",
        "    self.supports_masking = True\n",
        "    super(Start_End_Prediction, self).__init__(**kwargs)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    # A tensor ('self.W') multiplied with the final layer (12 encoder, batch_size, 384, 768)\n",
        "    # Making Output tensor as (384, 2 dimension (768->2))\n",
        "    self.W = self.add_weight(name='kernel',\n",
        "                             shape = (input_shape[2],2),\n",
        "                             initializer = 'uniform',\n",
        "                             trainable = True)\n",
        "    super(Start_End_Prediction, self).build(input_shape)\n",
        "\n",
        "  def call(self, x):\n",
        "    # Redifine Output dimension as (384 * 768)\n",
        "    x = K.reshape(x, shape=(-1, self.seq_len, K.shape(x)[2]))\n",
        "    # Dot production between self.W and x\n",
        "    # (batch_size, 384, 768) * (384, 2) = (batch_size, 384, 2)\n",
        "    x = K.dot(x, self.W)\n",
        "    # (batch_size, 384, 2) -> (2, batch_size, 384)\n",
        "    x = K.permute_dimensions(x, (2, 0, 1))\n",
        "\n",
        "    # Split the (2, batch_size, 384) into 2 (batch_size, 384)  --> start_logits & end_logits\n",
        "    # start_logits = (batch_size, 384)\n",
        "    # end_logits = (batch_size, 384)\n",
        "    self.start_logits, self.end_logits = x[0], x[1]\n",
        "\n",
        "    # Fed into, Passed by the GELU layer\n",
        "    self.start_logits = bert_gelu(self.start_logits)\n",
        "    self.end_logits = bert_gelu(self.end_logits)\n",
        "\n",
        "    # Fed into, Passed by the Softmax layer\n",
        "    # Getting probability for 384 tokens\n",
        "    self.start_logits = K.softmax(self.start_logits, axis=-1)\n",
        "    self.end_logits = K.softmax(self.end_logits, axis=-1)\n",
        "\n",
        "    return [self.start_logits, self.end_logits]\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    # In Keras, when defining custom layer, \n",
        "    # the output dimension must be defined in compute_output_shape function\n",
        "    return [(input_shape[0], self.seq_len), (input_shape[0], self.seq_len)]"
      ],
      "metadata": {
        "id": "ykF0O6J4cEnB"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BERT model\n",
        "from keras.layers import merge, dot, concatenate\n",
        "from keras import metrics\n",
        "import numpy as np\n",
        "\n",
        "# Define a functoin loading BERT model\n",
        "def get_bert_finetuning_model(model):\n",
        "  # Input data is token-embedding and segment-embedding\n",
        "  inputs = model.inputs[:2]\n",
        "\n",
        "  # Output of Transformer : (batch_size, 384, 768)\n",
        "  bert_transformer = model.layers[-1].output\n",
        "\n",
        "  # Unmask the maksed tensors using NonMasking function\n",
        "  x = NonMasking()(bert_transformer)\n",
        "\n",
        "  # Show the start and end token of answer at last\n",
        "  outputs_start, outputs_end = Start_End_Prediction(SEQ_LEN)(x)\n",
        "\n",
        "  bert_model = keras.models.Model(inputs, [outputs_start, outputs_end])\n",
        "\n",
        "  # Optimizer defined growing Learning_rate from 0 to 1.5e-5 gradually\n",
        "  # Original RAdam changed to RAdamOptimizer for some issue\n",
        "  optimizer_warmup = RAdamOptimizer(learning_rate = 1.5e-5, warmup_proportion=0.2, epsilon=1e-6, weight_decay = 0.01)\n",
        "\n",
        "  # Final Model compile\n",
        "  bert_model.compile(\n",
        "      optimizer = optimizer_warmup,\n",
        "      loss = 'sparse_categorical_crossentropy',\n",
        "      metrics = ['accuracy']\n",
        "  )\n",
        "\n",
        "  return bert_model"
      ],
      "metadata": {
        "id": "jV46fdelhJSr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display format customize in wrap for Google Colab (Youngsun)\n",
        "from IPython.display import HTML, display\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "2RhVBoRou8ZK"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for loading pretrained BERT and fine_tuned trained model\n",
        "bert_model = get_bert_finetuning_model(model)\n",
        "path = \"gdrive/MyDrive/Colab Notebooks/squad\"\n",
        "bert_model.load_weights(path+\"/(Uncased)Squad.h5\")"
      ],
      "metadata": {
        "id": "LfNqTwMPw2i9"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Baseline Testing (Without Re-training)"
      ],
      "metadata": {
        "id": "yvk0NddfsrNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # SQuAD Test Dataset Setup\n",
        "# # Since test dataset is different in form with train dataset, (question, context format is same, but answer is different)\n",
        "# # A function make format unification required to be made\n",
        "\n",
        "# def squad_json_to_dataframe_dev(input_file_path, record_path = ['data', 'paragraphs', 'qas', 'answers'], verbos = 1):\n",
        "#   \"\"\"\n",
        "#   input_file_path: path to the squad json file.\n",
        "#   record_path: path to deepest level in json file default value is\n",
        "#   ['data','paragraphs','qas','answers']\n",
        "#   verbose: 0 to suppress it default is 1\n",
        "#   \"\"\"\n",
        "#   if verbose:\n",
        "#     print(\"Reading the json file\")\n",
        "#   file = json.loads(open(input_file_path).read())\n",
        "#   if verbose:\n",
        "#     print(\"processing...\")\n",
        "  \n",
        "#   # Parsing different Level's in the json file\n",
        "#   js = pd.io.json.json_normalize(file, record_path)\n",
        "#   m = pd.io.json.json_normalize(file, record_path[:-1])\n",
        "#   r = pd.io.json.json_normalize(file, record_path[:-2])\n",
        "\n",
        "#   # Combining it into single dataframe\n",
        "#   idx = np.repeat(r['context'].values, r.qas.str.len())\n",
        "#   m['context'] = idx\n",
        "#   main = m[['id', 'question', 'context', 'answers']].set_index('id').reset_index()\n",
        "#   main['c_id'] = main['context'].factorize()[0]\n",
        "#   if verbose:\n",
        "#     print(\"shape of the dataframe is {}\".format(main.shape))\n",
        "#     print(\"Done\")\n",
        "#   return main"
      ],
      "metadata": {
        "id": "y1vRTO5Aq7v9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "8cf6f09b-eafa-439a-9c2e-302212a4c785"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Test dataset loading\n",
        "# input_file_path = 'dev-v1.1.json'\n",
        "# record_path = ['data', 'paragraphs', 'qas', 'answers']\n",
        "# verbose = 0\n",
        "# dev = squad_json_to_dataframe_dev(input_file_path=input_file_path, record_path=record_path)\n",
        "\n",
        "import pandas as pd\n",
        "test_context = pd.read_excel('Test_QA_mod_without0.xlsx')"
      ],
      "metadata": {
        "id": "bY4FqGKsvCXI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "5fcf012b-1924-4a72-c589-f6dff2c9a32d"
      },
      "execution_count": 379,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if test dataset is loaded well\n",
        "# ** Why answer needs to be in test dataset??? (Youngsun)\n",
        "test_context"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 837
        },
        "id": "8NmiyjisvWQ9",
        "outputId": "5dcab581-3c36-42fa-c7e1-2ee2439c1420"
      },
      "execution_count": 380,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-cfadf13f-0584-449f-a902-372a2400db1e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Index</th>\n",
              "      <th>Paper</th>\n",
              "      <th>Category</th>\n",
              "      <th>Question</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>371</td>\n",
              "      <td>Ginkgo biloba prevents mobile phone-induced ox...</td>\n",
              "      <td>Research Subject</td>\n",
              "      <td>What animal has been used?</td>\n",
              "      <td>Background: The widespread use of mobile phone...</td>\n",
              "      <td>Rats</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>371</td>\n",
              "      <td>Ginkgo biloba prevents mobile phone-induced ox...</td>\n",
              "      <td>Radio Frequency</td>\n",
              "      <td>What is the signal frequency?</td>\n",
              "      <td>Background: The widespread use of mobile phone...</td>\n",
              "      <td>900 MHz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>372</td>\n",
              "      <td>Effects of radiofrequency exposure on the GABA...</td>\n",
              "      <td>Research Subject</td>\n",
              "      <td>What animal has been used?</td>\n",
              "      <td>The widespread use of cellular phones raises t...</td>\n",
              "      <td>rats</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>372</td>\n",
              "      <td>Effects of radiofrequency exposure on the GABA...</td>\n",
              "      <td>Radio Frequency</td>\n",
              "      <td>What is the signal frequency?</td>\n",
              "      <td>The widespread use of cellular phones raises t...</td>\n",
              "      <td>900 MHz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>372</td>\n",
              "      <td>Effects of radiofrequency exposure on the GABA...</td>\n",
              "      <td>Other Units of Exposure Level</td>\n",
              "      <td>How much W/kg was used?</td>\n",
              "      <td>The widespread use of cellular phones raises t...</td>\n",
              "      <td>pulsed emission with a specific absorption rat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>449</td>\n",
              "      <td>Expression of the immediate early gene, c-fos,...</td>\n",
              "      <td>Radio Frequency</td>\n",
              "      <td>What is the signal frequency?</td>\n",
              "      <td>Aims: To study the effect of acute exposure to...</td>\n",
              "      <td>900 MHz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>453</td>\n",
              "      <td>Bone morphogenetic protein expression in newbo...</td>\n",
              "      <td>Research Subject</td>\n",
              "      <td>What animal has been used?</td>\n",
              "      <td>Effects of nonthermal radiofrequency radiation...</td>\n",
              "      <td>rats</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>453</td>\n",
              "      <td>Bone morphogenetic protein expression in newbo...</td>\n",
              "      <td>Radio Frequency</td>\n",
              "      <td>What is the signal frequency?</td>\n",
              "      <td>Effects of nonthermal radiofrequency radiation...</td>\n",
              "      <td>9.4 GHz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>455</td>\n",
              "      <td>IRIDIUM exposure increases c-fos expression in...</td>\n",
              "      <td>Research Subject</td>\n",
              "      <td>What animal has been used?</td>\n",
              "      <td>With the rapid development of wireless communi...</td>\n",
              "      <td>mice</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>455</td>\n",
              "      <td>IRIDIUM exposure increases c-fos expression in...</td>\n",
              "      <td>Radio Frequency</td>\n",
              "      <td>What is the signal frequency?</td>\n",
              "      <td>With the rapid development of wireless communi...</td>\n",
              "      <td>1.6-GHz</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>72 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cfadf13f-0584-449f-a902-372a2400db1e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cfadf13f-0584-449f-a902-372a2400db1e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cfadf13f-0584-449f-a902-372a2400db1e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    Index  ...                                             Answer\n",
              "0     371  ...                                               Rats\n",
              "1     371  ...                                            900 MHz\n",
              "2     372  ...                                               rats\n",
              "3     372  ...                                            900 MHz\n",
              "4     372  ...  pulsed emission with a specific absorption rat...\n",
              "..    ...  ...                                                ...\n",
              "67    449  ...                                            900 MHz\n",
              "68    453  ...                                               rats\n",
              "69    453  ...                                            9.4 GHz\n",
              "70    455  ...                                               mice\n",
              "71    455  ...                                            1.6-GHz\n",
              "\n",
              "[72 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 380
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A function definition converting answers into list type for easy handling\n",
        "# text_len : Number of answers\n",
        "def get_text(text_len, answers):\n",
        "  texts = []\n",
        "  for i in range(text_len):\n",
        "    texts.append(answers[i]['Answer'])\n",
        "  return texts"
      ],
      "metadata": {
        "id": "n98MC0ezx0Q9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "7a777a12-fdbc-4e4f-825e-9626985f8fa5"
      },
      "execution_count": 381,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_context.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "Mj_adSrb5AzK",
        "outputId": "ba252d9a-d8a7-4658-fc20-e79e59a6f06c"
      },
      "execution_count": 382,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8b849812-94aa-403c-b3d4-a0154078e44d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Index</th>\n",
              "      <th>Paper</th>\n",
              "      <th>Category</th>\n",
              "      <th>Question</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>371</td>\n",
              "      <td>Ginkgo biloba prevents mobile phone-induced ox...</td>\n",
              "      <td>Research Subject</td>\n",
              "      <td>What animal has been used?</td>\n",
              "      <td>Background: The widespread use of mobile phone...</td>\n",
              "      <td>Rats</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>371</td>\n",
              "      <td>Ginkgo biloba prevents mobile phone-induced ox...</td>\n",
              "      <td>Radio Frequency</td>\n",
              "      <td>What is the signal frequency?</td>\n",
              "      <td>Background: The widespread use of mobile phone...</td>\n",
              "      <td>900 MHz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>372</td>\n",
              "      <td>Effects of radiofrequency exposure on the GABA...</td>\n",
              "      <td>Research Subject</td>\n",
              "      <td>What animal has been used?</td>\n",
              "      <td>The widespread use of cellular phones raises t...</td>\n",
              "      <td>rats</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>372</td>\n",
              "      <td>Effects of radiofrequency exposure on the GABA...</td>\n",
              "      <td>Radio Frequency</td>\n",
              "      <td>What is the signal frequency?</td>\n",
              "      <td>The widespread use of cellular phones raises t...</td>\n",
              "      <td>900 MHz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>372</td>\n",
              "      <td>Effects of radiofrequency exposure on the GABA...</td>\n",
              "      <td>Other Units of Exposure Level</td>\n",
              "      <td>How much W/kg was used?</td>\n",
              "      <td>The widespread use of cellular phones raises t...</td>\n",
              "      <td>pulsed emission with a specific absorption rat...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8b849812-94aa-403c-b3d4-a0154078e44d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8b849812-94aa-403c-b3d4-a0154078e44d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8b849812-94aa-403c-b3d4-a0154078e44d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Index  ...                                             Answer\n",
              "0    371  ...                                               Rats\n",
              "1    371  ...                                            900 MHz\n",
              "2    372  ...                                               rats\n",
              "3    372  ...                                            900 MHz\n",
              "4    372  ...  pulsed emission with a specific absorption rat...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 382
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Making a column for the number of answers\n",
        "# test_context['answer_len'] = test_context['Answer'].map(lambda x: x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "GqH5qWwFTEtA",
        "outputId": "bd18037f-0527-448f-925b-33c309e177b8"
      },
      "execution_count": 383,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_context['Answer'][0])\n",
        "# print()\n",
        "# print(test_context['Answer'][0][0]['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "9SCB7OOxTRAP",
        "outputId": "3ad7d678-2871-4aa6-b551-c7d574d8e670"
      },
      "execution_count": 384,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rats\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A function definition converting answers into list type for easy handling\n",
        "# text_len : Number of answers\n",
        "def get_text(answers):\n",
        "  texts = []\n",
        "  texts.append(answers)\n",
        "  return texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Ll0gSpbeSH8k",
        "outputId": "047ddb5b-3acc-4579-8841-62665c601300"
      },
      "execution_count": 385,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the above function\n",
        "# Taking the num of answers & answers as input, and converting them into List type\n",
        "# get_text(1, test_context['Answer'][0])\n",
        "get_text(test_context['Answer'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "Bx6QedYaSRQR",
        "outputId": "0e54bb53-fbd0-4acb-b22b-6ca4d3cc6817"
      },
      "execution_count": 386,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Rats']"
            ]
          },
          "metadata": {},
          "execution_count": 386
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Doing the above job to all the test dataset\n",
        "test_context['Answer_list'] = test_context.apply(lambda x: get_text(x['Answer']), axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "xJdaDidgUrhp",
        "outputId": "aa1eb47b-6813-4e4e-f5ca-a15c57fe9932"
      },
      "execution_count": 387,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_context.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "id": "Fep_IPV3VCXU",
        "outputId": "82069a3d-f92a-4238-ed2e-915105dfd962"
      },
      "execution_count": 388,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-90295975-5f53-4946-a48e-fa163e6e3379\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Index</th>\n",
              "      <th>Paper</th>\n",
              "      <th>Category</th>\n",
              "      <th>Question</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Answer</th>\n",
              "      <th>Answer_list</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>371</td>\n",
              "      <td>Ginkgo biloba prevents mobile phone-induced ox...</td>\n",
              "      <td>Research Subject</td>\n",
              "      <td>What animal has been used?</td>\n",
              "      <td>Background: The widespread use of mobile phone...</td>\n",
              "      <td>Rats</td>\n",
              "      <td>[Rats]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>371</td>\n",
              "      <td>Ginkgo biloba prevents mobile phone-induced ox...</td>\n",
              "      <td>Radio Frequency</td>\n",
              "      <td>What is the signal frequency?</td>\n",
              "      <td>Background: The widespread use of mobile phone...</td>\n",
              "      <td>900 MHz</td>\n",
              "      <td>[900 MHz]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>372</td>\n",
              "      <td>Effects of radiofrequency exposure on the GABA...</td>\n",
              "      <td>Research Subject</td>\n",
              "      <td>What animal has been used?</td>\n",
              "      <td>The widespread use of cellular phones raises t...</td>\n",
              "      <td>rats</td>\n",
              "      <td>[rats]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>372</td>\n",
              "      <td>Effects of radiofrequency exposure on the GABA...</td>\n",
              "      <td>Radio Frequency</td>\n",
              "      <td>What is the signal frequency?</td>\n",
              "      <td>The widespread use of cellular phones raises t...</td>\n",
              "      <td>900 MHz</td>\n",
              "      <td>[900 MHz]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>372</td>\n",
              "      <td>Effects of radiofrequency exposure on the GABA...</td>\n",
              "      <td>Other Units of Exposure Level</td>\n",
              "      <td>How much W/kg was used?</td>\n",
              "      <td>The widespread use of cellular phones raises t...</td>\n",
              "      <td>pulsed emission with a specific absorption rat...</td>\n",
              "      <td>[pulsed emission with a specific absorption ra...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-90295975-5f53-4946-a48e-fa163e6e3379')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-90295975-5f53-4946-a48e-fa163e6e3379 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-90295975-5f53-4946-a48e-fa163e6e3379');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Index  ...                                        Answer_list\n",
              "0    371  ...                                             [Rats]\n",
              "1    371  ...                                          [900 MHz]\n",
              "2    372  ...                                             [rats]\n",
              "3    372  ...                                          [900 MHz]\n",
              "4    372  ...  [pulsed emission with a specific absorption ra...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 388
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_context['Answer_list']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "_HEIqbnG5VZV",
        "outputId": "9cdfa03e-198a-4365-a70b-ee0b0997fd38"
      },
      "execution_count": 389,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                                [Rats]\n",
              "1                                             [900 MHz]\n",
              "2                                                [rats]\n",
              "3                                             [900 MHz]\n",
              "4     [pulsed emission with a specific absorption ra...\n",
              "                            ...                        \n",
              "67                                            [900 MHz]\n",
              "68                                               [rats]\n",
              "69                                            [9.4 GHz]\n",
              "70                                               [mice]\n",
              "71                                            [1.6-GHz]\n",
              "Name: Answer_list, Length: 72, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 389
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Making Input Data (Test Dataset) for BERT Model -- Embedding\n",
        "# Utilizaing the above Input model in the past, but without answers!\n",
        "\n",
        "# Column name of context\n",
        "FINE_DATA_COLUMN = \"Abstract\"\n",
        "# Column name of question\n",
        "FINE_QUESTION_COLUMN = \"Question\"\n",
        "# Column name of answer ('text')\n",
        "FINE_TEXT = \"Answer_list\""
      ],
      "metadata": {
        "id": "aBFwTIPs5aAH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "78617d7d-736e-4d1d-b7a6-de1fe3b66c6e"
      },
      "execution_count": 390,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copied & Pasted, and a bit modified from the above function\n",
        "\n",
        "# Define a function calculating the length of answer in context dataset all at once\n",
        "def convert_data(data_df):\n",
        "  global tokenizer\n",
        "  indices, segments, target_start, target_end = [], [], [], []\n",
        "\n",
        "  for i in tqdm(range(len(data_df))):\n",
        "    # que : List of tokenized question\n",
        "    que, _ = tokenizer.encode(data_df[FINE_QUESTION_COLUMN][i])\n",
        "    # doc : List of tokenized context\n",
        "    doc, _ = tokenizer.encode(data_df[FINE_DATA_COLUMN][i])\n",
        "\n",
        "    # [CLS] token deleted in context\n",
        "    doc.pop(0)\n",
        "\n",
        "    # Length of question & context\n",
        "    que_len = len(que)\n",
        "    doc_len = len(doc)\n",
        "    # 1. Length of question\n",
        "    # The question is cut by the length of 64\n",
        "    if que_len > 64:\n",
        "      que = que[:63]\n",
        "      que.append(102) # [SEP] token added to make it clear the question block\n",
        "    # 2. Total length of question and context\n",
        "    # The total input is cut by the length of 384\n",
        "    if len(que+doc) > SEQ_LEN:\n",
        "      while len(que+doc) != SEQ_LEN:\n",
        "        doc.pop(-1)\n",
        "      doc.pop(-1)\n",
        "      doc.append(102)\n",
        "\n",
        "    # Segment embedding\n",
        "    # Question : 0 / Context 1 / Padding : 0 (remaining part for short sentences)\n",
        "        \n",
        "    ############################\n",
        "    ###### Segment 예시 ########\n",
        "    ############################\n",
        "    \n",
        "    # question, context, padding\n",
        "    # 00000000, 1111111, 0000000\n",
        "    \n",
        "    segment = [0]*len(que) + [1]*len(doc) + [0]*(SEQ_LEN-len(que)-len(doc))\n",
        "\n",
        "    # Padding\n",
        "    if len(que+doc) <= SEQ_LEN:\n",
        "      while len(que+doc) != SEQ_LEN:\n",
        "        doc.append(0)\n",
        "\n",
        "    # Final Input 'ids' (Question + Context)\n",
        "    ids = que + doc\n",
        "\n",
        "    # Sliding Part\n",
        "    texts = data_df[FINE_TEXT][i]\n",
        "    \n",
        "    for text_element in texts:\n",
        "      text = tokenizer.encode(text_element)[0]\n",
        "      text_slide_len = len(text[1:-1]) # text_slide_len = 8\n",
        "\n",
        "      for j in range(0, (len(doc))):\n",
        "        # exist_flag : showing whether it is answerable or not (similar with is_unanswerable in SimpleTransforemr)\n",
        "        # 0 : No answer / 1 : Having answer\n",
        "        exist_flag = 0 \n",
        "        if text[1:-1] == doc[j:j+text_slide_len]: # [0:8]->[1:9]->[2:10]->..->[159:160]\n",
        "          # Assign the location of answer (start, end)\n",
        "          ans_start = j + len(que)\n",
        "          ans_end = j + text_slide_len - 1 + len(que)\n",
        "          # If matched, exist_flag changed to 1\n",
        "          exist_flag = 1\n",
        "          break\n",
        "\n",
        "      # When no answer case (exist_flag = 0), starting & ending value become SEQ_LEN\n",
        "      # All the data of starting, ending = 384 (SEQ_LEN) will be deleted from the list\n",
        "      if exist_flag == 0:\n",
        "        ans_start = SEQ_LEN\n",
        "        ans_end = SEQ_LEN\n",
        "\n",
        "    # Input(ids), Segment saving into list type (indices, segments)\n",
        "    indices.append(ids)\n",
        "    segments.append(segment)\n",
        "    # Starting and ending info saving into list type (target_start, target_end)\n",
        "    target_start.append(ans_start)\n",
        "    target_end.append(ans_end)\n",
        "\n",
        "  # Converting the 4 lists into numpy array\n",
        "  indices_x = np.array(indices)\n",
        "  segments = np.array(segments)\n",
        "  target_start = np.array(target_start)\n",
        "  target_end = np.array(target_end)\n",
        "\n",
        "  # The cut part saved in del_list and deleted from data\n",
        "  del_list = np.where(target_start != SEQ_LEN)[0]\n",
        "  not_del_list = np.where(target_start == SEQ_LEN)[0]\n",
        "  indices_x = indices_x[del_list]\n",
        "  segments = segments[del_list]\n",
        "  \n",
        "  target_start = target_start[del_list]\n",
        "  target_end = target_end[del_list]\n",
        "\n",
        "  return [indices_x, segments], del_list\n",
        "\n",
        "\n",
        "\n",
        "# # Copied & Pasted, and a bit modified from the above function\n",
        "# # Define a function calculating the length of answer in context dataset all at once\n",
        "# def convert_data(test_context, test_question):\n",
        "#   global tokenizer\n",
        "#   indices, segments = [], []\n",
        "\n",
        "#   que, _ = tokenizer.encode(test_question['Question'])\n",
        "#   doc, _ = tokenizer.encode(test_context['Abstract'])\n",
        "#   doc.pop(0)\n",
        "\n",
        "#   que_len = len(que)\n",
        "#   doc_len = len(doc)\n",
        "\n",
        "#   # 1. Length of question\n",
        "#   # The question is cut by the length of 64\n",
        "#   if que_len > 64:\n",
        "#     que = que[:63]\n",
        "#     que.append(102) # [SEP] token added to make it clear the question block\n",
        "  \n",
        "#   # 2. Total length of question and context\n",
        "#   # The total input is cut by the length of 384\n",
        "#   if len(que+doc) > SEQ_LEN:\n",
        "#     while len(que+doc) != SEQ_LEN:\n",
        "#       doc.pop(-1)\n",
        "#     doc.pop(-1)\n",
        "#     doc.append(102)\n",
        "\n",
        "#     # Segment embedding\n",
        "#     # Question : 0 / Context 1 / Padding : 0 (remaining part for short sentences)\n",
        "        \n",
        "#     ############################\n",
        "#     ###### Segment 예시 ########\n",
        "#     ############################\n",
        "    \n",
        "#     # question, context, padding\n",
        "#     # 00000000, 1111111, 0000000\n",
        "    \n",
        "#   segment = [0]*len(que) + [1]*len(doc) + [0]*(SEQ_LEN-len(que)-len(doc))\n",
        "\n",
        "#   # Padding\n",
        "#   if len(que+doc) <= SEQ_LEN:\n",
        "#     while len(que+doc) != SEQ_LEN:\n",
        "#       doc.append(0)\n",
        "\n",
        "#   # Final Input 'ids' (Question + Context)\n",
        "#   ids = que + doc\n",
        "\n",
        "#   # Input(ids), Segment saving into list type (indices, segments)\n",
        "#   indices.append(ids)\n",
        "#   segments.append(segment)\n",
        "\n",
        "#   # Converting the 4 lists into numpy array\n",
        "#   indices = np.array(indices)\n",
        "#   segments = np.array(segments)\n",
        "  \n",
        "#   # print(indices, segments)\n",
        "#   return [indices, segments]"
      ],
      "metadata": {
        "id": "VB_PBnkw7BK3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "d960d515-56bd-425b-dc80-e8384c75e86b"
      },
      "execution_count": 394,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load of the Calculator Function\n",
        "def load_data(pandas_dataframe):\n",
        "  data_df = pandas_dataframe\n",
        "  data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n",
        "  data_df[QUESTION_COLUMN] = data_df[QUESTION_COLUMN].astype(str)\n",
        "  data_x, data_y, del_list = convert_data(data_df)\n",
        "\n",
        "  return data_x, data_y, del_list"
      ],
      "metadata": {
        "id": "rx8nxmqB_giz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "a7b3ef74-c0e0-467b-c67e-44965420d2ed"
      },
      "execution_count": 395,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the test dataset into BERT input format\n",
        "dev_bert_input = convert_data(test_context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "X7ZZGcn8_t2A",
        "outputId": "b9d28930-efae-43c7-e362-c8f3e5e2a49d"
      },
      "execution_count": 396,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 72/72 [00:00<00:00, 254.44it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Format : Token + Segment\n",
        "dev_bert_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "jdFavy3uAdNq",
        "outputId": "4fae4471-b772-4575-ab5e-b05fe3128f10"
      },
      "execution_count": 397,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([array([[  101,  2054,  4111, ...,     0,     0,     0],\n",
              "         [  101,  2054,  2003, ...,     0,     0,     0],\n",
              "         [  101,  2054,  4111, ..., 11721,  3676,   102],\n",
              "         ...,\n",
              "         [  101,  2054,  2003, ...,  1012,  1006,   102],\n",
              "         [  101,  2054,  4111, ...,     0,     0,     0],\n",
              "         [  101,  2054,  2003, ...,     0,     0,     0]]),\n",
              "  array([[0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 1, 1, 1],\n",
              "         ...,\n",
              "         [0, 0, 0, ..., 1, 1, 1],\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0]])],\n",
              " array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
              "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
              "        51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
              "        68, 69, 70, 71]))"
            ]
          },
          "metadata": {},
          "execution_count": 397
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cut the part over length of 384 ('del_list')\n",
        "dev_bert_input, del_list = dev_bert_input[0], dev_bert_input[1]\n",
        "dev = test_context.iloc[del_list]\n",
        "dev = test_context.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "C7o6t9sGAuup",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "1ec7bb76-cce3-470a-822f-0f222630b65f"
      },
      "execution_count": 398,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 911
        },
        "id": "hm1tLr2OkHET",
        "outputId": "b508c381-891b-4156-ebce-48b6ca05748f"
      },
      "execution_count": 399,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-23441412-f818-42c4-8b6e-670f64cee7e0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Index</th>\n",
              "      <th>Paper</th>\n",
              "      <th>Category</th>\n",
              "      <th>Question</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Answer</th>\n",
              "      <th>Answer_list</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>371</td>\n",
              "      <td>Ginkgo biloba prevents mobile phone-induced ox...</td>\n",
              "      <td>Research Subject</td>\n",
              "      <td>What animal has been used?</td>\n",
              "      <td>Background: The widespread use of mobile phone...</td>\n",
              "      <td>Rats</td>\n",
              "      <td>[Rats]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>371</td>\n",
              "      <td>Ginkgo biloba prevents mobile phone-induced ox...</td>\n",
              "      <td>Radio Frequency</td>\n",
              "      <td>What is the signal frequency?</td>\n",
              "      <td>Background: The widespread use of mobile phone...</td>\n",
              "      <td>900 MHz</td>\n",
              "      <td>[900 MHz]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>372</td>\n",
              "      <td>Effects of radiofrequency exposure on the GABA...</td>\n",
              "      <td>Research Subject</td>\n",
              "      <td>What animal has been used?</td>\n",
              "      <td>The widespread use of cellular phones raises t...</td>\n",
              "      <td>rats</td>\n",
              "      <td>[rats]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>372</td>\n",
              "      <td>Effects of radiofrequency exposure on the GABA...</td>\n",
              "      <td>Radio Frequency</td>\n",
              "      <td>What is the signal frequency?</td>\n",
              "      <td>The widespread use of cellular phones raises t...</td>\n",
              "      <td>900 MHz</td>\n",
              "      <td>[900 MHz]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>372</td>\n",
              "      <td>Effects of radiofrequency exposure on the GABA...</td>\n",
              "      <td>Other Units of Exposure Level</td>\n",
              "      <td>How much W/kg was used?</td>\n",
              "      <td>The widespread use of cellular phones raises t...</td>\n",
              "      <td>pulsed emission with a specific absorption rat...</td>\n",
              "      <td>[pulsed emission with a specific absorption ra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>449</td>\n",
              "      <td>Expression of the immediate early gene, c-fos,...</td>\n",
              "      <td>Radio Frequency</td>\n",
              "      <td>What is the signal frequency?</td>\n",
              "      <td>Aims: To study the effect of acute exposure to...</td>\n",
              "      <td>900 MHz</td>\n",
              "      <td>[900 MHz]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>453</td>\n",
              "      <td>Bone morphogenetic protein expression in newbo...</td>\n",
              "      <td>Research Subject</td>\n",
              "      <td>What animal has been used?</td>\n",
              "      <td>Effects of nonthermal radiofrequency radiation...</td>\n",
              "      <td>rats</td>\n",
              "      <td>[rats]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>453</td>\n",
              "      <td>Bone morphogenetic protein expression in newbo...</td>\n",
              "      <td>Radio Frequency</td>\n",
              "      <td>What is the signal frequency?</td>\n",
              "      <td>Effects of nonthermal radiofrequency radiation...</td>\n",
              "      <td>9.4 GHz</td>\n",
              "      <td>[9.4 GHz]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>455</td>\n",
              "      <td>IRIDIUM exposure increases c-fos expression in...</td>\n",
              "      <td>Research Subject</td>\n",
              "      <td>What animal has been used?</td>\n",
              "      <td>With the rapid development of wireless communi...</td>\n",
              "      <td>mice</td>\n",
              "      <td>[mice]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>455</td>\n",
              "      <td>IRIDIUM exposure increases c-fos expression in...</td>\n",
              "      <td>Radio Frequency</td>\n",
              "      <td>What is the signal frequency?</td>\n",
              "      <td>With the rapid development of wireless communi...</td>\n",
              "      <td>1.6-GHz</td>\n",
              "      <td>[1.6-GHz]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>72 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-23441412-f818-42c4-8b6e-670f64cee7e0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-23441412-f818-42c4-8b6e-670f64cee7e0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-23441412-f818-42c4-8b6e-670f64cee7e0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    Index  ...                                        Answer_list\n",
              "0     371  ...                                             [Rats]\n",
              "1     371  ...                                          [900 MHz]\n",
              "2     372  ...                                             [rats]\n",
              "3     372  ...                                          [900 MHz]\n",
              "4     372  ...  [pulsed emission with a specific absorption ra...\n",
              "..    ...  ...                                                ...\n",
              "67    449  ...                                          [900 MHz]\n",
              "68    453  ...                                             [rats]\n",
              "69    453  ...                                          [9.4 GHz]\n",
              "70    455  ...                                             [mice]\n",
              "71    455  ...                                          [1.6-GHz]\n",
              "\n",
              "[72 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 399
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the only tokenized sentences input (1~10527)\n",
        "# Without the segment part\n",
        "indexes = dev_bert_input[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "uXyKyKakkHfS",
        "outputId": "d1421b90-f8c4-42a1-c267-70cefc9593fa"
      },
      "execution_count": 400,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indexes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "hVmYAQvakWGb",
        "outputId": "863c5ee4-3695-4c27-aa5c-2860304c9500"
      },
      "execution_count": 401,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  101,  2054,  4111, ...,     0,     0,     0],\n",
              "       [  101,  2054,  2003, ...,     0,     0,     0],\n",
              "       [  101,  2054,  4111, ..., 11721,  3676,   102],\n",
              "       ...,\n",
              "       [  101,  2054,  2003, ...,  1012,  1006,   102],\n",
              "       [  101,  2054,  4111, ...,     0,     0,     0],\n",
              "       [  101,  2054,  2003, ...,     0,     0,     0]])"
            ]
          },
          "metadata": {},
          "execution_count": 401
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT prediction start to all the test dataset (10,527 * 384 tokens * 787 features), taking about 2 mins\n",
        "bert_predictions = bert_model.predict(dev_bert_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "N68yUPvkkY1a",
        "outputId": "7cea1f96-501c-408a-f4c8-1657d199f7f5"
      },
      "execution_count": 402,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "id": "NBxf1px-k74j",
        "outputId": "3f94d55a-3568-4cb8-9829-c43cd4b6ea7a"
      },
      "execution_count": 403,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[2.1831365e-06, 5.2993890e-04, 3.5733028e-05, ..., 1.7509587e-06,\n",
              "         1.7437283e-06, 1.7432195e-06],\n",
              "        [2.6194368e-07, 3.2619182e-06, 2.1690602e-07, ..., 2.0423164e-07,\n",
              "         2.0465181e-07, 2.0481698e-07],\n",
              "        [9.2186965e-06, 3.5764650e-03, 2.6527347e-04, ..., 6.2507711e-06,\n",
              "         6.2582508e-06, 6.1225614e-06],\n",
              "        ...,\n",
              "        [7.5015841e-08, 5.2319854e-07, 6.7134835e-08, ..., 6.9372497e-08,\n",
              "         8.2671967e-08, 6.7741446e-08],\n",
              "        [9.2734663e-06, 1.2762017e-03, 1.5390942e-04, ..., 6.4712258e-06,\n",
              "         6.4667647e-06, 6.4754668e-06],\n",
              "        [8.1038785e-08, 1.6732034e-07, 8.0639438e-08, ..., 8.0691137e-08,\n",
              "         8.0757346e-08, 8.0760877e-08]], dtype=float32),\n",
              " array([[2.2052884e-06, 2.5964607e-04, 8.6874381e-05, ..., 2.0166447e-06,\n",
              "         2.0076218e-06, 2.0068044e-06],\n",
              "        [8.9909190e-07, 2.3353562e-06, 8.9983473e-07, ..., 8.5442639e-07,\n",
              "         8.5832454e-07, 8.6056298e-07],\n",
              "        [1.0944999e-05, 2.9103600e-03, 1.3444157e-03, ..., 7.8976245e-06,\n",
              "         9.3744948e-06, 9.4989473e-06],\n",
              "        ...,\n",
              "        [2.2513889e-07, 4.3278700e-07, 2.4434607e-07, ..., 7.1380618e-06,\n",
              "         7.7529677e-07, 3.4759765e-07],\n",
              "        [5.0036042e-06, 3.1869151e-04, 1.6558349e-04, ..., 2.1475641e-06,\n",
              "         2.1439153e-06, 2.1423966e-06],\n",
              "        [1.9863887e-07, 1.9517211e-07, 2.2506157e-07, ..., 1.9782105e-07,\n",
              "         1.9838576e-07, 1.9867696e-07]], dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 403
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Among 384 tokens, selecting the one having the largest probability\n",
        "start_indexes = np.argmax(bert_predictions[0], axis=-1)\n",
        "end_indexes = np.argmax(bert_predictions[1], axis=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "dJuptNQmmNJq",
        "outputId": "61e9a4c4-4c74-482b-fc7b-0f51ce1cf236"
      },
      "execution_count": 404,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete some non-sense case \n",
        "# e.g. position of starting token (start_indexes) is bigger than the ending token (end_indexes)\n",
        "not_del_list = np.where(start_indexes <= end_indexes)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "rGlaOya4nMm6",
        "outputId": "78c60d28-7c34-4741-a05d-03f79f7ae6c9"
      },
      "execution_count": 405,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "not_del_list.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "qqosoatnntdC",
        "outputId": "8d53b6eb-f8e1-4dff-ad8a-a063b7ab658e"
      },
      "execution_count": 406,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(68,)"
            ]
          },
          "metadata": {},
          "execution_count": 406
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_indexes = start_indexes[not_del_list]\n",
        "end_indexes = end_indexes[not_del_list]\n",
        "indexes = indexes[not_del_list]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "j_G3kgAAnvKa",
        "outputId": "f095cc00-b5ce-4cdd-f127-83b2feebbe8f"
      },
      "execution_count": 407,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Positions of starting tokens\n",
        "start_indexes[0:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "nJMLjaVIn_Xy",
        "outputId": "b3b453d9-c37d-4935-9b35-a0569b140b64"
      },
      "execution_count": 408,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([108, 121, 100, 241, 282,  47,  59, 129, 125, 137, 160,  38, 203,\n",
              "        43,  41,  77, 108, 101,  85, 101, 103, 232,  49, 101,  91,  31,\n",
              "        99,  22,  63,  62,  73, 115, 101, 163,  81,  90, 108,  47, 164,\n",
              "        68,  49,  80, 101,  98,  73, 294,  57,  68,  54,  68,  70,  97,\n",
              "        63,  87,   8,  15,  19,  32,  46,  97,  66,  83,  57,  54,  82,\n",
              "        92,  74,  77])"
            ]
          },
          "metadata": {},
          "execution_count": 408
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Positions of end tokens\n",
        "end_indexes[0:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "tNWw3uUxoF3q",
        "outputId": "831cc85c-9eea-4044-9333-7a77203eb63d"
      },
      "execution_count": 409,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([108, 122, 100, 242, 282,  51,  59, 129, 125, 145, 166,  42, 205,\n",
              "        47,  47,  79, 108, 105,  85, 106, 105, 233,  49, 101,  94,  34,\n",
              "       108,  26,  69,  62,  74, 115, 103, 165,  86,  92, 110,  47, 164,\n",
              "        71,  49,  83, 105,  98,  74, 298,  63,  69,  54,  68,  70, 103,\n",
              "        63,  87,   8,  15,  19,  33,  48,  97,  68,  84,  57,  55,  82,\n",
              "        95,  74,  81])"
            ]
          },
          "metadata": {},
          "execution_count": 409
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After deleting the non-sense case, rearranging dataset\n",
        "dev = dev.iloc[not_del_list].reset_index(drop=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "OyCuSwWjoJd5",
        "outputId": "03aefc4d-a77f-4fe1-ced4-849a395aab18"
      },
      "execution_count": 410,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 911
        },
        "id": "Pk3_Ba_Dob18",
        "outputId": "d01192d3-cc94-483e-ef20-5dc79fc76791"
      },
      "execution_count": 411,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-64686a3c-4d1e-48e7-bd99-df36024ee29d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Index</th>\n",
              "      <th>Paper</th>\n",
              "      <th>Category</th>\n",
              "      <th>Question</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Answer</th>\n",
              "      <th>Answer_list</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>371</td>\n",
              "      <td>Ginkgo biloba prevents mobile phone-induced ox...</td>\n",
              "      <td>Research Subject</td>\n",
              "      <td>What animal has been used?</td>\n",
              "      <td>Background: The widespread use of mobile phone...</td>\n",
              "      <td>Rats</td>\n",
              "      <td>[Rats]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>371</td>\n",
              "      <td>Ginkgo biloba prevents mobile phone-induced ox...</td>\n",
              "      <td>Radio Frequency</td>\n",
              "      <td>What is the signal frequency?</td>\n",
              "      <td>Background: The widespread use of mobile phone...</td>\n",
              "      <td>900 MHz</td>\n",
              "      <td>[900 MHz]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>372</td>\n",
              "      <td>Effects of radiofrequency exposure on the GABA...</td>\n",
              "      <td>Research Subject</td>\n",
              "      <td>What animal has been used?</td>\n",
              "      <td>The widespread use of cellular phones raises t...</td>\n",
              "      <td>rats</td>\n",
              "      <td>[rats]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>372</td>\n",
              "      <td>Effects of radiofrequency exposure on the GABA...</td>\n",
              "      <td>Radio Frequency</td>\n",
              "      <td>What is the signal frequency?</td>\n",
              "      <td>The widespread use of cellular phones raises t...</td>\n",
              "      <td>900 MHz</td>\n",
              "      <td>[900 MHz]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>372</td>\n",
              "      <td>Effects of radiofrequency exposure on the GABA...</td>\n",
              "      <td>Other Units of Exposure Level</td>\n",
              "      <td>How much W/kg was used?</td>\n",
              "      <td>The widespread use of cellular phones raises t...</td>\n",
              "      <td>pulsed emission with a specific absorption rat...</td>\n",
              "      <td>[pulsed emission with a specific absorption ra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>449</td>\n",
              "      <td>Expression of the immediate early gene, c-fos,...</td>\n",
              "      <td>Radio Frequency</td>\n",
              "      <td>What is the signal frequency?</td>\n",
              "      <td>Aims: To study the effect of acute exposure to...</td>\n",
              "      <td>900 MHz</td>\n",
              "      <td>[900 MHz]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>453</td>\n",
              "      <td>Bone morphogenetic protein expression in newbo...</td>\n",
              "      <td>Research Subject</td>\n",
              "      <td>What animal has been used?</td>\n",
              "      <td>Effects of nonthermal radiofrequency radiation...</td>\n",
              "      <td>rats</td>\n",
              "      <td>[rats]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>453</td>\n",
              "      <td>Bone morphogenetic protein expression in newbo...</td>\n",
              "      <td>Radio Frequency</td>\n",
              "      <td>What is the signal frequency?</td>\n",
              "      <td>Effects of nonthermal radiofrequency radiation...</td>\n",
              "      <td>9.4 GHz</td>\n",
              "      <td>[9.4 GHz]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>455</td>\n",
              "      <td>IRIDIUM exposure increases c-fos expression in...</td>\n",
              "      <td>Research Subject</td>\n",
              "      <td>What animal has been used?</td>\n",
              "      <td>With the rapid development of wireless communi...</td>\n",
              "      <td>mice</td>\n",
              "      <td>[mice]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>455</td>\n",
              "      <td>IRIDIUM exposure increases c-fos expression in...</td>\n",
              "      <td>Radio Frequency</td>\n",
              "      <td>What is the signal frequency?</td>\n",
              "      <td>With the rapid development of wireless communi...</td>\n",
              "      <td>1.6-GHz</td>\n",
              "      <td>[1.6-GHz]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>68 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-64686a3c-4d1e-48e7-bd99-df36024ee29d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-64686a3c-4d1e-48e7-bd99-df36024ee29d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-64686a3c-4d1e-48e7-bd99-df36024ee29d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    Index  ...                                        Answer_list\n",
              "0     371  ...                                             [Rats]\n",
              "1     371  ...                                          [900 MHz]\n",
              "2     372  ...                                             [rats]\n",
              "3     372  ...                                          [900 MHz]\n",
              "4     372  ...  [pulsed emission with a specific absorption ra...\n",
              "..    ...  ...                                                ...\n",
              "63    449  ...                                          [900 MHz]\n",
              "64    453  ...                                             [rats]\n",
              "65    453  ...                                          [9.4 GHz]\n",
              "66    455  ...                                             [mice]\n",
              "67    455  ...                                          [1.6-GHz]\n",
              "\n",
              "[68 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 411
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the length of test data\n",
        "length = len(dev)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "fMLRMyxyochr",
        "outputId": "e6111c21-d190-4b7d-8534-e1d8d8a2bc48"
      },
      "execution_count": 412,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "7G1_Yl7coxJ6",
        "outputId": "db354b7f-5ed7-436d-e709-a4df7db38601"
      },
      "execution_count": 413,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "68"
            ]
          },
          "metadata": {},
          "execution_count": 413
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "untokenized = []\n",
        "\n",
        "for j in range(len(start_indexes)):\n",
        "  sentence = []\n",
        "  # Saving each tokenized word into the list of sentence (sentence = [])\n",
        "  for i in range(start_indexes[j], end_indexes[j]+1):\n",
        "    token_based_word = reverse_token_dict[indexes[j][i]]\n",
        "    sentence.append(token_based_word)\n",
        "  sentence_string = \"\"\n",
        "\n",
        "  for w in sentence:\n",
        "    # Special token ## delete, if a token starts with ##\n",
        "    if w.startswith(\"##\"):\n",
        "      w = w.replace(\"##\", \"\")\n",
        "    # If a token has no ##, a space added to the word\n",
        "    else:\n",
        "      w = \" \" + w\n",
        "    # Putting together all the tokens in list format\n",
        "    sentence_string += w\n",
        "  \n",
        "  # If senetence_string starts with a space (\" \"), delete the space\n",
        "  if sentence_string.startswith(\" \"):\n",
        "    sentence_string = \"\" + sentence_string[1:]\n",
        "  \n",
        "  # After putting together all the list of tokens, and assigning it in the list of the 'Untokenized'\n",
        "  untokenized.append(sentence_string)\n",
        "  sentences.append(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "XIUrx6nRoyrq",
        "outputId": "e3f95810-9b9e-4aca-c56a-f61bee828831"
      },
      "execution_count": 416,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences[:30])\n",
        "print('\\n')\n",
        "print(untokenized[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "b3qHcL01uUgT",
        "outputId": "cf7e4137-f911-409f-e184-d25b2ee566f4"
      },
      "execution_count": 417,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['rats'], ['900', 'mhz'], ['rats'], ['900', 'mhz'], ['32'], ['bal', '##b', '/', 'c', 'mice'], ['8'], ['rats'], ['96'], ['900', 'mhz', ',', '1800', 'mhz', 'and', '245', '##0', 'mhz'], ['5', '.', '84', '×', '10', '-', '4'], ['gs', '##m', '-', '1800', 'mhz'], ['3', '.', '22'], ['wi', '##star', 'al', '##bino', 'rats'], ['24', 'male', 'wi', '##star', 'al', '##bino', 'rats'], ['210', '##0', 'mhz'], ['mice'], ['c', '##57', '##bl', '/', '6'], ['mice'], ['4', '.', '0', 'w', '/', 'kg'], ['4', '.', '0'], ['sham', 'rats'], ['mice'], ['twelve'], ['900', 'and', '1800', 'mhz'], ['1800', '##m', '##h', '##z'], ['rats', 'were', 'divided', 'into', 'the', 'following', 'groups', ':', 'sham', 'rats'], ['900', 'and', '210', '##0', 'mhz'], ['sp', '##rag', '##ue', '-', 'da', '##wley', 'rats'], ['20']]\n",
            "\n",
            "\n",
            "['rats', '900 mhz', 'rats', '900 mhz', '32', 'balb / c mice', '8', 'rats', '96', '900 mhz , 1800 mhz and 2450 mhz', '5 . 84 × 10 - 4', 'gsm - 1800 mhz', '3 . 22', 'wistar albino rats', '24 male wistar albino rats', '2100 mhz', 'mice', 'c57bl / 6', 'mice', '4 . 0 w / kg', '4 . 0', 'sham rats', 'mice', 'twelve', '900 and 1800 mhz', '1800mhz', 'rats were divided into the following groups : sham rats', '900 and 2100 mhz', 'sprague - dawley rats', '20']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the answers into list format\n",
        "dev_answers = []\n",
        "for i in range(length):\n",
        "  dev_answer = []\n",
        "  texts_dict = test_context['Answer']\n",
        "\n",
        "  for j in range(len(texts_dict)):\n",
        "    dev_answer.append(texts_dict[j])\n",
        "  dev_answers.append(dev_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "dzMeuZfhugH6",
        "outputId": "e1d4638b-a5a5-40fe-c45f-11ebe32eb1dc"
      },
      "execution_count": 422,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev_answers[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1XJqdtH6v0Mj",
        "outputId": "3087c332-7e06-40c7-9130-88856f24a41a"
      },
      "execution_count": 436,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rats',\n",
              " '900 mhz',\n",
              " 'rats',\n",
              " '900 mhz',\n",
              " 'pulsed emission with a specific absorption rate ( sar ) of 4 w / kg and continuous emission with high sar ( 32 w / kg )',\n",
              " 'balb / c mice',\n",
              " 'forty - eight',\n",
              " 'wistar rats',\n",
              " '96',\n",
              " '900 mhz , 1800 mhz and 2450 mhz',\n",
              " '5 . 84 × 10 - 4 w / kg , 5 . 94 × 10 - 4 w / kg and 6 . 4 × 10 - 4 w / kg',\n",
              " 'rats',\n",
              " '1800 mhz',\n",
              " '3 . 22 w / kg',\n",
              " 'wistar albino rats',\n",
              " '24',\n",
              " '2100 mhz',\n",
              " 'mice',\n",
              " 'whole - body exposed ( nexp = 8 ) for 2 hr to gsm 1800 mhz mobile phone radiation at an average electric field intensity range of 4 . 3 - 17 . 5 v / m or sham - exposed ( nse = 8 )',\n",
              " '1800 mhz',\n",
              " 'c57bl / 6 mice',\n",
              " '835 mhz',\n",
              " '4 . 0 w / kg',\n",
              " 'rats',\n",
              " '900 mhz',\n",
              " 'balb / c mice',\n",
              " 'twelve',\n",
              " '900 and 1800 mhz',\n",
              " 'rats',\n",
              " '1800mhz',\n",
              " 'rats',\n",
              " '900 and 2100 mhz',\n",
              " 'sprague - dawley rats',\n",
              " '20',\n",
              " '900 mhz',\n",
              " 'mice',\n",
              " '835 mhz',\n",
              " '4 . 0 w / kg',\n",
              " 'c57bl / 6 mice',\n",
              " '835 mhz',\n",
              " '4 . 0 w / kg',\n",
              " 'chicken eggs',\n",
              " '450 eggs per treatment in three successive rounds per treatment',\n",
              " 'gsm ( 1 . 8 ghz ) , dect ( 1 . 88 ghz ) , umts ( 2 . 1 ghz ) , and wlan ( 5 . 6 ghz )',\n",
              " 'flies',\n",
              " '0 . 15 w / kg',\n",
              " 'rats',\n",
              " '14',\n",
              " '900 mhz',\n",
              " '0 . 0369 w / kg',\n",
              " 'sprague - dawley rats',\n",
              " '900 mhz',\n",
              " 'wistar rat',\n",
              " 'balb / c mice',\n",
              " 'three equally divided groups of animals ( 6 animals / group ) were used',\n",
              " 'the first group was exposed to a typical mobile phone , at a sar level range of 0 . 17 - 0 . 37 w / kg for 3 h daily for 8 months , the second group was exposed to a wireless dect base ( digital enhanced cordless telecommunications / telephone ) at a sar level range of 0 . 012 - 0 . 028 w / kg',\n",
              " 'rats',\n",
              " 'group 1 is composed of 3g - emr - exposed rats ( n = 9 ) and group 2 is the control group ( n = 9 )',\n",
              " 'rats',\n",
              " 'sprague - dawley rats',\n",
              " 'mice',\n",
              " '1800 mhz',\n",
              " '1 . 1 w / kg',\n",
              " 'rats',\n",
              " '915 mhz',\n",
              " '0 . 4 mw / g',\n",
              " 'mice',\n",
              " '900 mhz',\n",
              " 'rats',\n",
              " '9 . 4 ghz',\n",
              " 'mice',\n",
              " '1 . 6 - ghz']"
            ]
          },
          "metadata": {},
          "execution_count": 436
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the answers\n",
        "dev_tokens = []\n",
        "for i in dev_answers:\n",
        "  dev_tokened = []\n",
        "  for j in i:\n",
        "    temp_token = tokenizer.tokenize(j)\n",
        "    # Tokenize an answer\n",
        "    temp_token.pop(0)\n",
        "    # [CLS] elimination\n",
        "    temp_token.pop(-1)\n",
        "    # [SEP] elimination\n",
        "    dev_tokened.append(temp_token)\n",
        "  dev_tokens.append(dev_tokened)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "13fZ9vwLv1cb",
        "outputId": "15486f5d-ecc5-41cc-8bc6-4e8de98f8f83"
      },
      "execution_count": 426,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dev_tokens[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IJZx3WvLwl6a",
        "outputId": "50e69ffc-2265-45fb-823a-4da39db58ef9"
      },
      "execution_count": 427,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[['rats'], ['900', 'mhz'], ['rats'], ['900', 'mhz'], ['pulsed', 'emission', 'with', 'a', 'specific', 'absorption', 'rate', '(', 'sar', ')', 'of', '4', 'w', '/', 'kg', 'and', 'continuous', 'emission', 'with', 'high', 'sar', '(', '32', 'w', '/', 'kg', ')'], ['bal', '##b', '/', 'c', 'mice'], ['forty', '-', 'eight'], ['wi', '##star', 'rats'], ['96'], ['900', 'mhz', ',', '1800', 'mhz', 'and', '245', '##0', 'mhz'], ['5', '.', '84', '×', '10', '-', '4', 'w', '/', 'kg', ',', '5', '.', '94', '×', '10', '-', '4', 'w', '/', 'kg', 'and', '6', '.', '4', '×', '10', '-', '4', 'w', '/', 'kg'], ['rats'], ['1800', 'mhz'], ['3', '.', '22', 'w', '/', 'kg'], ['wi', '##star', 'al', '##bino', 'rats'], ['24'], ['210', '##0', 'mhz'], ['mice'], ['whole', '-', 'body', 'exposed', '(', 'ne', '##x', '##p', '=', '8', ')', 'for', '2', 'hr', 'to', 'gs', '##m', '1800', 'mhz', 'mobile', 'phone', 'radiation', 'at', 'an', 'average', 'electric', 'field', 'intensity', 'range', 'of', '4', '.', '3', '-', '17', '.', '5', 'v', '/', 'm', 'or', 'sham', '-', 'exposed', '(', 'ns', '##e', '=', '8', ')'], ['1800', 'mhz'], ['c', '##57', '##bl', '/', '6', 'mice'], ['83', '##5', 'mhz'], ['4', '.', '0', 'w', '/', 'kg'], ['rats'], ['900', 'mhz'], ['bal', '##b', '/', 'c', 'mice'], ['twelve'], ['900', 'and', '1800', 'mhz'], ['rats'], ['1800', '##m', '##h', '##z'], ['rats'], ['900', 'and', '210', '##0', 'mhz'], ['sp', '##rag', '##ue', '-', 'da', '##wley', 'rats'], ['20'], ['900', 'mhz'], ['mice'], ['83', '##5', 'mhz'], ['4', '.', '0', 'w', '/', 'kg'], ['c', '##57', '##bl', '/', '6', 'mice'], ['83', '##5', 'mhz'], ['4', '.', '0', 'w', '/', 'kg'], ['chicken', 'eggs'], ['450', 'eggs', 'per', 'treatment', 'in', 'three', 'successive', 'rounds', 'per', 'treatment'], ['gs', '##m', '(', '1', '.', '8', 'ghz', ')', ',', 'dec', '##t', '(', '1', '.', '88', 'ghz', ')', ',', 'um', '##ts', '(', '2', '.', '1', 'ghz', ')', ',', 'and', 'w', '##lan', '(', '5', '.', '6', 'ghz', ')'], ['flies'], ['0', '.', '15', 'w', '/', 'kg'], ['rats'], ['14'], ['900', 'mhz'], ['0', '.', '03', '##6', '##9', 'w', '/', 'kg'], ['sp', '##rag', '##ue', '-', 'da', '##wley', 'rats'], ['900', 'mhz'], ['wi', '##star', 'rat'], ['bal', '##b', '/', 'c', 'mice'], ['three', 'equally', 'divided', 'groups', 'of', 'animals', '(', '6', 'animals', '/', 'group', ')', 'were', 'used'], ['the', 'first', 'group', 'was', 'exposed', 'to', 'a', 'typical', 'mobile', 'phone', ',', 'at', 'a', 'sar', 'level', 'range', 'of', '0', '.', '17', '-', '0', '.', '37', 'w', '/', 'kg', 'for', '3', 'h', 'daily', 'for', '8', 'months', ',', 'the', 'second', 'group', 'was', 'exposed', 'to', 'a', 'wireless', 'dec', '##t', 'base', '(', 'digital', 'enhanced', 'cord', '##less', 'telecommunications', '/', 'telephone', ')', 'at', 'a', 'sar', 'level', 'range', 'of', '0', '.', '01', '##2', '-', '0', '.', '02', '##8', 'w', '/', 'kg'], ['rats'], ['group', '1', 'is', 'composed', 'of', '3', '##g', '-', 'em', '##r', '-', 'exposed', 'rats', '(', 'n', '=', '9', ')', 'and', 'group', '2', 'is', 'the', 'control', 'group', '(', 'n', '=', '9', ')'], ['rats'], ['sp', '##rag', '##ue', '-', 'da', '##wley', 'rats'], ['mice'], ['1800', 'mhz'], ['1', '.', '1', 'w', '/', 'kg'], ['rats'], ['91', '##5', 'mhz'], ['0', '.', '4', 'mw', '/', 'g'], ['mice'], ['900', 'mhz'], ['rats'], ['9', '.', '4', 'ghz'], ['mice'], ['1', '.', '6', '-', 'ghz']], [['rats'], ['900', 'mhz'], ['rats'], ['900', 'mhz'], ['pulsed', 'emission', 'with', 'a', 'specific', 'absorption', 'rate', '(', 'sar', ')', 'of', '4', 'w', '/', 'kg', 'and', 'continuous', 'emission', 'with', 'high', 'sar', '(', '32', 'w', '/', 'kg', ')'], ['bal', '##b', '/', 'c', 'mice'], ['forty', '-', 'eight'], ['wi', '##star', 'rats'], ['96'], ['900', 'mhz', ',', '1800', 'mhz', 'and', '245', '##0', 'mhz'], ['5', '.', '84', '×', '10', '-', '4', 'w', '/', 'kg', ',', '5', '.', '94', '×', '10', '-', '4', 'w', '/', 'kg', 'and', '6', '.', '4', '×', '10', '-', '4', 'w', '/', 'kg'], ['rats'], ['1800', 'mhz'], ['3', '.', '22', 'w', '/', 'kg'], ['wi', '##star', 'al', '##bino', 'rats'], ['24'], ['210', '##0', 'mhz'], ['mice'], ['whole', '-', 'body', 'exposed', '(', 'ne', '##x', '##p', '=', '8', ')', 'for', '2', 'hr', 'to', 'gs', '##m', '1800', 'mhz', 'mobile', 'phone', 'radiation', 'at', 'an', 'average', 'electric', 'field', 'intensity', 'range', 'of', '4', '.', '3', '-', '17', '.', '5', 'v', '/', 'm', 'or', 'sham', '-', 'exposed', '(', 'ns', '##e', '=', '8', ')'], ['1800', 'mhz'], ['c', '##57', '##bl', '/', '6', 'mice'], ['83', '##5', 'mhz'], ['4', '.', '0', 'w', '/', 'kg'], ['rats'], ['900', 'mhz'], ['bal', '##b', '/', 'c', 'mice'], ['twelve'], ['900', 'and', '1800', 'mhz'], ['rats'], ['1800', '##m', '##h', '##z'], ['rats'], ['900', 'and', '210', '##0', 'mhz'], ['sp', '##rag', '##ue', '-', 'da', '##wley', 'rats'], ['20'], ['900', 'mhz'], ['mice'], ['83', '##5', 'mhz'], ['4', '.', '0', 'w', '/', 'kg'], ['c', '##57', '##bl', '/', '6', 'mice'], ['83', '##5', 'mhz'], ['4', '.', '0', 'w', '/', 'kg'], ['chicken', 'eggs'], ['450', 'eggs', 'per', 'treatment', 'in', 'three', 'successive', 'rounds', 'per', 'treatment'], ['gs', '##m', '(', '1', '.', '8', 'ghz', ')', ',', 'dec', '##t', '(', '1', '.', '88', 'ghz', ')', ',', 'um', '##ts', '(', '2', '.', '1', 'ghz', ')', ',', 'and', 'w', '##lan', '(', '5', '.', '6', 'ghz', ')'], ['flies'], ['0', '.', '15', 'w', '/', 'kg'], ['rats'], ['14'], ['900', 'mhz'], ['0', '.', '03', '##6', '##9', 'w', '/', 'kg'], ['sp', '##rag', '##ue', '-', 'da', '##wley', 'rats'], ['900', 'mhz'], ['wi', '##star', 'rat'], ['bal', '##b', '/', 'c', 'mice'], ['three', 'equally', 'divided', 'groups', 'of', 'animals', '(', '6', 'animals', '/', 'group', ')', 'were', 'used'], ['the', 'first', 'group', 'was', 'exposed', 'to', 'a', 'typical', 'mobile', 'phone', ',', 'at', 'a', 'sar', 'level', 'range', 'of', '0', '.', '17', '-', '0', '.', '37', 'w', '/', 'kg', 'for', '3', 'h', 'daily', 'for', '8', 'months', ',', 'the', 'second', 'group', 'was', 'exposed', 'to', 'a', 'wireless', 'dec', '##t', 'base', '(', 'digital', 'enhanced', 'cord', '##less', 'telecommunications', '/', 'telephone', ')', 'at', 'a', 'sar', 'level', 'range', 'of', '0', '.', '01', '##2', '-', '0', '.', '02', '##8', 'w', '/', 'kg'], ['rats'], ['group', '1', 'is', 'composed', 'of', '3', '##g', '-', 'em', '##r', '-', 'exposed', 'rats', '(', 'n', '=', '9', ')', 'and', 'group', '2', 'is', 'the', 'control', 'group', '(', 'n', '=', '9', ')'], ['rats'], ['sp', '##rag', '##ue', '-', 'da', '##wley', 'rats'], ['mice'], ['1800', 'mhz'], ['1', '.', '1', 'w', '/', 'kg'], ['rats'], ['91', '##5', 'mhz'], ['0', '.', '4', 'mw', '/', 'g'], ['mice'], ['900', 'mhz'], ['rats'], ['9', '.', '4', 'ghz'], ['mice'], ['1', '.', '6', '-', 'ghz']], [['rats'], ['900', 'mhz'], ['rats'], ['900', 'mhz'], ['pulsed', 'emission', 'with', 'a', 'specific', 'absorption', 'rate', '(', 'sar', ')', 'of', '4', 'w', '/', 'kg', 'and', 'continuous', 'emission', 'with', 'high', 'sar', '(', '32', 'w', '/', 'kg', ')'], ['bal', '##b', '/', 'c', 'mice'], ['forty', '-', 'eight'], ['wi', '##star', 'rats'], ['96'], ['900', 'mhz', ',', '1800', 'mhz', 'and', '245', '##0', 'mhz'], ['5', '.', '84', '×', '10', '-', '4', 'w', '/', 'kg', ',', '5', '.', '94', '×', '10', '-', '4', 'w', '/', 'kg', 'and', '6', '.', '4', '×', '10', '-', '4', 'w', '/', 'kg'], ['rats'], ['1800', 'mhz'], ['3', '.', '22', 'w', '/', 'kg'], ['wi', '##star', 'al', '##bino', 'rats'], ['24'], ['210', '##0', 'mhz'], ['mice'], ['whole', '-', 'body', 'exposed', '(', 'ne', '##x', '##p', '=', '8', ')', 'for', '2', 'hr', 'to', 'gs', '##m', '1800', 'mhz', 'mobile', 'phone', 'radiation', 'at', 'an', 'average', 'electric', 'field', 'intensity', 'range', 'of', '4', '.', '3', '-', '17', '.', '5', 'v', '/', 'm', 'or', 'sham', '-', 'exposed', '(', 'ns', '##e', '=', '8', ')'], ['1800', 'mhz'], ['c', '##57', '##bl', '/', '6', 'mice'], ['83', '##5', 'mhz'], ['4', '.', '0', 'w', '/', 'kg'], ['rats'], ['900', 'mhz'], ['bal', '##b', '/', 'c', 'mice'], ['twelve'], ['900', 'and', '1800', 'mhz'], ['rats'], ['1800', '##m', '##h', '##z'], ['rats'], ['900', 'and', '210', '##0', 'mhz'], ['sp', '##rag', '##ue', '-', 'da', '##wley', 'rats'], ['20'], ['900', 'mhz'], ['mice'], ['83', '##5', 'mhz'], ['4', '.', '0', 'w', '/', 'kg'], ['c', '##57', '##bl', '/', '6', 'mice'], ['83', '##5', 'mhz'], ['4', '.', '0', 'w', '/', 'kg'], ['chicken', 'eggs'], ['450', 'eggs', 'per', 'treatment', 'in', 'three', 'successive', 'rounds', 'per', 'treatment'], ['gs', '##m', '(', '1', '.', '8', 'ghz', ')', ',', 'dec', '##t', '(', '1', '.', '88', 'ghz', ')', ',', 'um', '##ts', '(', '2', '.', '1', 'ghz', ')', ',', 'and', 'w', '##lan', '(', '5', '.', '6', 'ghz', ')'], ['flies'], ['0', '.', '15', 'w', '/', 'kg'], ['rats'], ['14'], ['900', 'mhz'], ['0', '.', '03', '##6', '##9', 'w', '/', 'kg'], ['sp', '##rag', '##ue', '-', 'da', '##wley', 'rats'], ['900', 'mhz'], ['wi', '##star', 'rat'], ['bal', '##b', '/', 'c', 'mice'], ['three', 'equally', 'divided', 'groups', 'of', 'animals', '(', '6', 'animals', '/', 'group', ')', 'were', 'used'], ['the', 'first', 'group', 'was', 'exposed', 'to', 'a', 'typical', 'mobile', 'phone', ',', 'at', 'a', 'sar', 'level', 'range', 'of', '0', '.', '17', '-', '0', '.', '37', 'w', '/', 'kg', 'for', '3', 'h', 'daily', 'for', '8', 'months', ',', 'the', 'second', 'group', 'was', 'exposed', 'to', 'a', 'wireless', 'dec', '##t', 'base', '(', 'digital', 'enhanced', 'cord', '##less', 'telecommunications', '/', 'telephone', ')', 'at', 'a', 'sar', 'level', 'range', 'of', '0', '.', '01', '##2', '-', '0', '.', '02', '##8', 'w', '/', 'kg'], ['rats'], ['group', '1', 'is', 'composed', 'of', '3', '##g', '-', 'em', '##r', '-', 'exposed', 'rats', '(', 'n', '=', '9', ')', 'and', 'group', '2', 'is', 'the', 'control', 'group', '(', 'n', '=', '9', ')'], ['rats'], ['sp', '##rag', '##ue', '-', 'da', '##wley', 'rats'], ['mice'], ['1800', 'mhz'], ['1', '.', '1', 'w', '/', 'kg'], ['rats'], ['91', '##5', 'mhz'], ['0', '.', '4', 'mw', '/', 'g'], ['mice'], ['900', 'mhz'], ['rats'], ['9', '.', '4', 'ghz'], ['mice'], ['1', '.', '6', '-', 'ghz']], [['rats'], ['900', 'mhz'], ['rats'], ['900', 'mhz'], ['pulsed', 'emission', 'with', 'a', 'specific', 'absorption', 'rate', '(', 'sar', ')', 'of', '4', 'w', '/', 'kg', 'and', 'continuous', 'emission', 'with', 'high', 'sar', '(', '32', 'w', '/', 'kg', ')'], ['bal', '##b', '/', 'c', 'mice'], ['forty', '-', 'eight'], ['wi', '##star', 'rats'], ['96'], ['900', 'mhz', ',', '1800', 'mhz', 'and', '245', '##0', 'mhz'], ['5', '.', '84', '×', '10', '-', '4', 'w', '/', 'kg', ',', '5', '.', '94', '×', '10', '-', '4', 'w', '/', 'kg', 'and', '6', '.', '4', '×', '10', '-', '4', 'w', '/', 'kg'], ['rats'], ['1800', 'mhz'], ['3', '.', '22', 'w', '/', 'kg'], ['wi', '##star', 'al', '##bino', 'rats'], ['24'], ['210', '##0', 'mhz'], ['mice'], ['whole', '-', 'body', 'exposed', '(', 'ne', '##x', '##p', '=', '8', ')', 'for', '2', 'hr', 'to', 'gs', '##m', '1800', 'mhz', 'mobile', 'phone', 'radiation', 'at', 'an', 'average', 'electric', 'field', 'intensity', 'range', 'of', '4', '.', '3', '-', '17', '.', '5', 'v', '/', 'm', 'or', 'sham', '-', 'exposed', '(', 'ns', '##e', '=', '8', ')'], ['1800', 'mhz'], ['c', '##57', '##bl', '/', '6', 'mice'], ['83', '##5', 'mhz'], ['4', '.', '0', 'w', '/', 'kg'], ['rats'], ['900', 'mhz'], ['bal', '##b', '/', 'c', 'mice'], ['twelve'], ['900', 'and', '1800', 'mhz'], ['rats'], ['1800', '##m', '##h', '##z'], ['rats'], ['900', 'and', '210', '##0', 'mhz'], ['sp', '##rag', '##ue', '-', 'da', '##wley', 'rats'], ['20'], ['900', 'mhz'], ['mice'], ['83', '##5', 'mhz'], ['4', '.', '0', 'w', '/', 'kg'], ['c', '##57', '##bl', '/', '6', 'mice'], ['83', '##5', 'mhz'], ['4', '.', '0', 'w', '/', 'kg'], ['chicken', 'eggs'], ['450', 'eggs', 'per', 'treatment', 'in', 'three', 'successive', 'rounds', 'per', 'treatment'], ['gs', '##m', '(', '1', '.', '8', 'ghz', ')', ',', 'dec', '##t', '(', '1', '.', '88', 'ghz', ')', ',', 'um', '##ts', '(', '2', '.', '1', 'ghz', ')', ',', 'and', 'w', '##lan', '(', '5', '.', '6', 'ghz', ')'], ['flies'], ['0', '.', '15', 'w', '/', 'kg'], ['rats'], ['14'], ['900', 'mhz'], ['0', '.', '03', '##6', '##9', 'w', '/', 'kg'], ['sp', '##rag', '##ue', '-', 'da', '##wley', 'rats'], ['900', 'mhz'], ['wi', '##star', 'rat'], ['bal', '##b', '/', 'c', 'mice'], ['three', 'equally', 'divided', 'groups', 'of', 'animals', '(', '6', 'animals', '/', 'group', ')', 'were', 'used'], ['the', 'first', 'group', 'was', 'exposed', 'to', 'a', 'typical', 'mobile', 'phone', ',', 'at', 'a', 'sar', 'level', 'range', 'of', '0', '.', '17', '-', '0', '.', '37', 'w', '/', 'kg', 'for', '3', 'h', 'daily', 'for', '8', 'months', ',', 'the', 'second', 'group', 'was', 'exposed', 'to', 'a', 'wireless', 'dec', '##t', 'base', '(', 'digital', 'enhanced', 'cord', '##less', 'telecommunications', '/', 'telephone', ')', 'at', 'a', 'sar', 'level', 'range', 'of', '0', '.', '01', '##2', '-', '0', '.', '02', '##8', 'w', '/', 'kg'], ['rats'], ['group', '1', 'is', 'composed', 'of', '3', '##g', '-', 'em', '##r', '-', 'exposed', 'rats', '(', 'n', '=', '9', ')', 'and', 'group', '2', 'is', 'the', 'control', 'group', '(', 'n', '=', '9', ')'], ['rats'], ['sp', '##rag', '##ue', '-', 'da', '##wley', 'rats'], ['mice'], ['1800', 'mhz'], ['1', '.', '1', 'w', '/', 'kg'], ['rats'], ['91', '##5', 'mhz'], ['0', '.', '4', 'mw', '/', 'g'], ['mice'], ['900', 'mhz'], ['rats'], ['9', '.', '4', 'ghz'], ['mice'], ['1', '.', '6', '-', 'ghz']], [['rats'], ['900', 'mhz'], ['rats'], ['900', 'mhz'], ['pulsed', 'emission', 'with', 'a', 'specific', 'absorption', 'rate', '(', 'sar', ')', 'of', '4', 'w', '/', 'kg', 'and', 'continuous', 'emission', 'with', 'high', 'sar', '(', '32', 'w', '/', 'kg', ')'], ['bal', '##b', '/', 'c', 'mice'], ['forty', '-', 'eight'], ['wi', '##star', 'rats'], ['96'], ['900', 'mhz', ',', '1800', 'mhz', 'and', '245', '##0', 'mhz'], ['5', '.', '84', '×', '10', '-', '4', 'w', '/', 'kg', ',', '5', '.', '94', '×', '10', '-', '4', 'w', '/', 'kg', 'and', '6', '.', '4', '×', '10', '-', '4', 'w', '/', 'kg'], ['rats'], ['1800', 'mhz'], ['3', '.', '22', 'w', '/', 'kg'], ['wi', '##star', 'al', '##bino', 'rats'], ['24'], ['210', '##0', 'mhz'], ['mice'], ['whole', '-', 'body', 'exposed', '(', 'ne', '##x', '##p', '=', '8', ')', 'for', '2', 'hr', 'to', 'gs', '##m', '1800', 'mhz', 'mobile', 'phone', 'radiation', 'at', 'an', 'average', 'electric', 'field', 'intensity', 'range', 'of', '4', '.', '3', '-', '17', '.', '5', 'v', '/', 'm', 'or', 'sham', '-', 'exposed', '(', 'ns', '##e', '=', '8', ')'], ['1800', 'mhz'], ['c', '##57', '##bl', '/', '6', 'mice'], ['83', '##5', 'mhz'], ['4', '.', '0', 'w', '/', 'kg'], ['rats'], ['900', 'mhz'], ['bal', '##b', '/', 'c', 'mice'], ['twelve'], ['900', 'and', '1800', 'mhz'], ['rats'], ['1800', '##m', '##h', '##z'], ['rats'], ['900', 'and', '210', '##0', 'mhz'], ['sp', '##rag', '##ue', '-', 'da', '##wley', 'rats'], ['20'], ['900', 'mhz'], ['mice'], ['83', '##5', 'mhz'], ['4', '.', '0', 'w', '/', 'kg'], ['c', '##57', '##bl', '/', '6', 'mice'], ['83', '##5', 'mhz'], ['4', '.', '0', 'w', '/', 'kg'], ['chicken', 'eggs'], ['450', 'eggs', 'per', 'treatment', 'in', 'three', 'successive', 'rounds', 'per', 'treatment'], ['gs', '##m', '(', '1', '.', '8', 'ghz', ')', ',', 'dec', '##t', '(', '1', '.', '88', 'ghz', ')', ',', 'um', '##ts', '(', '2', '.', '1', 'ghz', ')', ',', 'and', 'w', '##lan', '(', '5', '.', '6', 'ghz', ')'], ['flies'], ['0', '.', '15', 'w', '/', 'kg'], ['rats'], ['14'], ['900', 'mhz'], ['0', '.', '03', '##6', '##9', 'w', '/', 'kg'], ['sp', '##rag', '##ue', '-', 'da', '##wley', 'rats'], ['900', 'mhz'], ['wi', '##star', 'rat'], ['bal', '##b', '/', 'c', 'mice'], ['three', 'equally', 'divided', 'groups', 'of', 'animals', '(', '6', 'animals', '/', 'group', ')', 'were', 'used'], ['the', 'first', 'group', 'was', 'exposed', 'to', 'a', 'typical', 'mobile', 'phone', ',', 'at', 'a', 'sar', 'level', 'range', 'of', '0', '.', '17', '-', '0', '.', '37', 'w', '/', 'kg', 'for', '3', 'h', 'daily', 'for', '8', 'months', ',', 'the', 'second', 'group', 'was', 'exposed', 'to', 'a', 'wireless', 'dec', '##t', 'base', '(', 'digital', 'enhanced', 'cord', '##less', 'telecommunications', '/', 'telephone', ')', 'at', 'a', 'sar', 'level', 'range', 'of', '0', '.', '01', '##2', '-', '0', '.', '02', '##8', 'w', '/', 'kg'], ['rats'], ['group', '1', 'is', 'composed', 'of', '3', '##g', '-', 'em', '##r', '-', 'exposed', 'rats', '(', 'n', '=', '9', ')', 'and', 'group', '2', 'is', 'the', 'control', 'group', '(', 'n', '=', '9', ')'], ['rats'], ['sp', '##rag', '##ue', '-', 'da', '##wley', 'rats'], ['mice'], ['1800', 'mhz'], ['1', '.', '1', 'w', '/', 'kg'], ['rats'], ['91', '##5', 'mhz'], ['0', '.', '4', 'mw', '/', 'g'], ['mice'], ['900', 'mhz'], ['rats'], ['9', '.', '4', 'ghz'], ['mice'], ['1', '.', '6', '-', 'ghz']]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting tokenized answers into sentences\n",
        "# And putting all together\n",
        "dev_answer_lists = []\n",
        "for dev_answers in dev_tokens:\n",
        "  dev_answer_list = []\n",
        "  for dev_answer in dev_answers:\n",
        "    dev_answer_string = \" \".join(dev_answer)\n",
        "    dev_answer_list.append(dev_answer_string)\n",
        "  dev_answer_lists.append(dev_answer_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Qx4ukZO6wpHy",
        "outputId": "2fcf48e5-c401-4948-8307-72991e5540ca"
      },
      "execution_count": 428,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dev_answer_lists[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "we5TK3LjxE87",
        "outputId": "02c63f3f-4f7c-4e63-d4a8-106086ad39fe"
      },
      "execution_count": 429,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['rats', '900 mhz', 'rats', '900 mhz', 'pulsed emission with a specific absorption rate ( sar ) of 4 w / kg and continuous emission with high sar ( 32 w / kg )', 'bal ##b / c mice', 'forty - eight', 'wi ##star rats', '96', '900 mhz , 1800 mhz and 245 ##0 mhz', '5 . 84 × 10 - 4 w / kg , 5 . 94 × 10 - 4 w / kg and 6 . 4 × 10 - 4 w / kg', 'rats', '1800 mhz', '3 . 22 w / kg', 'wi ##star al ##bino rats', '24', '210 ##0 mhz', 'mice', 'whole - body exposed ( ne ##x ##p = 8 ) for 2 hr to gs ##m 1800 mhz mobile phone radiation at an average electric field intensity range of 4 . 3 - 17 . 5 v / m or sham - exposed ( ns ##e = 8 )', '1800 mhz', 'c ##57 ##bl / 6 mice', '83 ##5 mhz', '4 . 0 w / kg', 'rats', '900 mhz', 'bal ##b / c mice', 'twelve', '900 and 1800 mhz', 'rats', '1800 ##m ##h ##z', 'rats', '900 and 210 ##0 mhz', 'sp ##rag ##ue - da ##wley rats', '20', '900 mhz', 'mice', '83 ##5 mhz', '4 . 0 w / kg', 'c ##57 ##bl / 6 mice', '83 ##5 mhz', '4 . 0 w / kg', 'chicken eggs', '450 eggs per treatment in three successive rounds per treatment', 'gs ##m ( 1 . 8 ghz ) , dec ##t ( 1 . 88 ghz ) , um ##ts ( 2 . 1 ghz ) , and w ##lan ( 5 . 6 ghz )', 'flies', '0 . 15 w / kg', 'rats', '14', '900 mhz', '0 . 03 ##6 ##9 w / kg', 'sp ##rag ##ue - da ##wley rats', '900 mhz', 'wi ##star rat', 'bal ##b / c mice', 'three equally divided groups of animals ( 6 animals / group ) were used', 'the first group was exposed to a typical mobile phone , at a sar level range of 0 . 17 - 0 . 37 w / kg for 3 h daily for 8 months , the second group was exposed to a wireless dec ##t base ( digital enhanced cord ##less telecommunications / telephone ) at a sar level range of 0 . 01 ##2 - 0 . 02 ##8 w / kg', 'rats', 'group 1 is composed of 3 ##g - em ##r - exposed rats ( n = 9 ) and group 2 is the control group ( n = 9 )', 'rats', 'sp ##rag ##ue - da ##wley rats', 'mice', '1800 mhz', '1 . 1 w / kg', 'rats', '91 ##5 mhz', '0 . 4 mw / g', 'mice', '900 mhz', 'rats', '9 . 4 ghz', 'mice', '1 . 6 - ghz'], ['rats', '900 mhz', 'rats', '900 mhz', 'pulsed emission with a specific absorption rate ( sar ) of 4 w / kg and continuous emission with high sar ( 32 w / kg )', 'bal ##b / c mice', 'forty - eight', 'wi ##star rats', '96', '900 mhz , 1800 mhz and 245 ##0 mhz', '5 . 84 × 10 - 4 w / kg , 5 . 94 × 10 - 4 w / kg and 6 . 4 × 10 - 4 w / kg', 'rats', '1800 mhz', '3 . 22 w / kg', 'wi ##star al ##bino rats', '24', '210 ##0 mhz', 'mice', 'whole - body exposed ( ne ##x ##p = 8 ) for 2 hr to gs ##m 1800 mhz mobile phone radiation at an average electric field intensity range of 4 . 3 - 17 . 5 v / m or sham - exposed ( ns ##e = 8 )', '1800 mhz', 'c ##57 ##bl / 6 mice', '83 ##5 mhz', '4 . 0 w / kg', 'rats', '900 mhz', 'bal ##b / c mice', 'twelve', '900 and 1800 mhz', 'rats', '1800 ##m ##h ##z', 'rats', '900 and 210 ##0 mhz', 'sp ##rag ##ue - da ##wley rats', '20', '900 mhz', 'mice', '83 ##5 mhz', '4 . 0 w / kg', 'c ##57 ##bl / 6 mice', '83 ##5 mhz', '4 . 0 w / kg', 'chicken eggs', '450 eggs per treatment in three successive rounds per treatment', 'gs ##m ( 1 . 8 ghz ) , dec ##t ( 1 . 88 ghz ) , um ##ts ( 2 . 1 ghz ) , and w ##lan ( 5 . 6 ghz )', 'flies', '0 . 15 w / kg', 'rats', '14', '900 mhz', '0 . 03 ##6 ##9 w / kg', 'sp ##rag ##ue - da ##wley rats', '900 mhz', 'wi ##star rat', 'bal ##b / c mice', 'three equally divided groups of animals ( 6 animals / group ) were used', 'the first group was exposed to a typical mobile phone , at a sar level range of 0 . 17 - 0 . 37 w / kg for 3 h daily for 8 months , the second group was exposed to a wireless dec ##t base ( digital enhanced cord ##less telecommunications / telephone ) at a sar level range of 0 . 01 ##2 - 0 . 02 ##8 w / kg', 'rats', 'group 1 is composed of 3 ##g - em ##r - exposed rats ( n = 9 ) and group 2 is the control group ( n = 9 )', 'rats', 'sp ##rag ##ue - da ##wley rats', 'mice', '1800 mhz', '1 . 1 w / kg', 'rats', '91 ##5 mhz', '0 . 4 mw / g', 'mice', '900 mhz', 'rats', '9 . 4 ghz', 'mice', '1 . 6 - ghz'], ['rats', '900 mhz', 'rats', '900 mhz', 'pulsed emission with a specific absorption rate ( sar ) of 4 w / kg and continuous emission with high sar ( 32 w / kg )', 'bal ##b / c mice', 'forty - eight', 'wi ##star rats', '96', '900 mhz , 1800 mhz and 245 ##0 mhz', '5 . 84 × 10 - 4 w / kg , 5 . 94 × 10 - 4 w / kg and 6 . 4 × 10 - 4 w / kg', 'rats', '1800 mhz', '3 . 22 w / kg', 'wi ##star al ##bino rats', '24', '210 ##0 mhz', 'mice', 'whole - body exposed ( ne ##x ##p = 8 ) for 2 hr to gs ##m 1800 mhz mobile phone radiation at an average electric field intensity range of 4 . 3 - 17 . 5 v / m or sham - exposed ( ns ##e = 8 )', '1800 mhz', 'c ##57 ##bl / 6 mice', '83 ##5 mhz', '4 . 0 w / kg', 'rats', '900 mhz', 'bal ##b / c mice', 'twelve', '900 and 1800 mhz', 'rats', '1800 ##m ##h ##z', 'rats', '900 and 210 ##0 mhz', 'sp ##rag ##ue - da ##wley rats', '20', '900 mhz', 'mice', '83 ##5 mhz', '4 . 0 w / kg', 'c ##57 ##bl / 6 mice', '83 ##5 mhz', '4 . 0 w / kg', 'chicken eggs', '450 eggs per treatment in three successive rounds per treatment', 'gs ##m ( 1 . 8 ghz ) , dec ##t ( 1 . 88 ghz ) , um ##ts ( 2 . 1 ghz ) , and w ##lan ( 5 . 6 ghz )', 'flies', '0 . 15 w / kg', 'rats', '14', '900 mhz', '0 . 03 ##6 ##9 w / kg', 'sp ##rag ##ue - da ##wley rats', '900 mhz', 'wi ##star rat', 'bal ##b / c mice', 'three equally divided groups of animals ( 6 animals / group ) were used', 'the first group was exposed to a typical mobile phone , at a sar level range of 0 . 17 - 0 . 37 w / kg for 3 h daily for 8 months , the second group was exposed to a wireless dec ##t base ( digital enhanced cord ##less telecommunications / telephone ) at a sar level range of 0 . 01 ##2 - 0 . 02 ##8 w / kg', 'rats', 'group 1 is composed of 3 ##g - em ##r - exposed rats ( n = 9 ) and group 2 is the control group ( n = 9 )', 'rats', 'sp ##rag ##ue - da ##wley rats', 'mice', '1800 mhz', '1 . 1 w / kg', 'rats', '91 ##5 mhz', '0 . 4 mw / g', 'mice', '900 mhz', 'rats', '9 . 4 ghz', 'mice', '1 . 6 - ghz'], ['rats', '900 mhz', 'rats', '900 mhz', 'pulsed emission with a specific absorption rate ( sar ) of 4 w / kg and continuous emission with high sar ( 32 w / kg )', 'bal ##b / c mice', 'forty - eight', 'wi ##star rats', '96', '900 mhz , 1800 mhz and 245 ##0 mhz', '5 . 84 × 10 - 4 w / kg , 5 . 94 × 10 - 4 w / kg and 6 . 4 × 10 - 4 w / kg', 'rats', '1800 mhz', '3 . 22 w / kg', 'wi ##star al ##bino rats', '24', '210 ##0 mhz', 'mice', 'whole - body exposed ( ne ##x ##p = 8 ) for 2 hr to gs ##m 1800 mhz mobile phone radiation at an average electric field intensity range of 4 . 3 - 17 . 5 v / m or sham - exposed ( ns ##e = 8 )', '1800 mhz', 'c ##57 ##bl / 6 mice', '83 ##5 mhz', '4 . 0 w / kg', 'rats', '900 mhz', 'bal ##b / c mice', 'twelve', '900 and 1800 mhz', 'rats', '1800 ##m ##h ##z', 'rats', '900 and 210 ##0 mhz', 'sp ##rag ##ue - da ##wley rats', '20', '900 mhz', 'mice', '83 ##5 mhz', '4 . 0 w / kg', 'c ##57 ##bl / 6 mice', '83 ##5 mhz', '4 . 0 w / kg', 'chicken eggs', '450 eggs per treatment in three successive rounds per treatment', 'gs ##m ( 1 . 8 ghz ) , dec ##t ( 1 . 88 ghz ) , um ##ts ( 2 . 1 ghz ) , and w ##lan ( 5 . 6 ghz )', 'flies', '0 . 15 w / kg', 'rats', '14', '900 mhz', '0 . 03 ##6 ##9 w / kg', 'sp ##rag ##ue - da ##wley rats', '900 mhz', 'wi ##star rat', 'bal ##b / c mice', 'three equally divided groups of animals ( 6 animals / group ) were used', 'the first group was exposed to a typical mobile phone , at a sar level range of 0 . 17 - 0 . 37 w / kg for 3 h daily for 8 months , the second group was exposed to a wireless dec ##t base ( digital enhanced cord ##less telecommunications / telephone ) at a sar level range of 0 . 01 ##2 - 0 . 02 ##8 w / kg', 'rats', 'group 1 is composed of 3 ##g - em ##r - exposed rats ( n = 9 ) and group 2 is the control group ( n = 9 )', 'rats', 'sp ##rag ##ue - da ##wley rats', 'mice', '1800 mhz', '1 . 1 w / kg', 'rats', '91 ##5 mhz', '0 . 4 mw / g', 'mice', '900 mhz', 'rats', '9 . 4 ghz', 'mice', '1 . 6 - ghz'], ['rats', '900 mhz', 'rats', '900 mhz', 'pulsed emission with a specific absorption rate ( sar ) of 4 w / kg and continuous emission with high sar ( 32 w / kg )', 'bal ##b / c mice', 'forty - eight', 'wi ##star rats', '96', '900 mhz , 1800 mhz and 245 ##0 mhz', '5 . 84 × 10 - 4 w / kg , 5 . 94 × 10 - 4 w / kg and 6 . 4 × 10 - 4 w / kg', 'rats', '1800 mhz', '3 . 22 w / kg', 'wi ##star al ##bino rats', '24', '210 ##0 mhz', 'mice', 'whole - body exposed ( ne ##x ##p = 8 ) for 2 hr to gs ##m 1800 mhz mobile phone radiation at an average electric field intensity range of 4 . 3 - 17 . 5 v / m or sham - exposed ( ns ##e = 8 )', '1800 mhz', 'c ##57 ##bl / 6 mice', '83 ##5 mhz', '4 . 0 w / kg', 'rats', '900 mhz', 'bal ##b / c mice', 'twelve', '900 and 1800 mhz', 'rats', '1800 ##m ##h ##z', 'rats', '900 and 210 ##0 mhz', 'sp ##rag ##ue - da ##wley rats', '20', '900 mhz', 'mice', '83 ##5 mhz', '4 . 0 w / kg', 'c ##57 ##bl / 6 mice', '83 ##5 mhz', '4 . 0 w / kg', 'chicken eggs', '450 eggs per treatment in three successive rounds per treatment', 'gs ##m ( 1 . 8 ghz ) , dec ##t ( 1 . 88 ghz ) , um ##ts ( 2 . 1 ghz ) , and w ##lan ( 5 . 6 ghz )', 'flies', '0 . 15 w / kg', 'rats', '14', '900 mhz', '0 . 03 ##6 ##9 w / kg', 'sp ##rag ##ue - da ##wley rats', '900 mhz', 'wi ##star rat', 'bal ##b / c mice', 'three equally divided groups of animals ( 6 animals / group ) were used', 'the first group was exposed to a typical mobile phone , at a sar level range of 0 . 17 - 0 . 37 w / kg for 3 h daily for 8 months , the second group was exposed to a wireless dec ##t base ( digital enhanced cord ##less telecommunications / telephone ) at a sar level range of 0 . 01 ##2 - 0 . 02 ##8 w / kg', 'rats', 'group 1 is composed of 3 ##g - em ##r - exposed rats ( n = 9 ) and group 2 is the control group ( n = 9 )', 'rats', 'sp ##rag ##ue - da ##wley rats', 'mice', '1800 mhz', '1 . 1 w / kg', 'rats', '91 ##5 mhz', '0 . 4 mw / g', 'mice', '900 mhz', 'rats', '9 . 4 ghz', 'mice', '1 . 6 - ghz']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Untokenizing (including deleting \" ##\" etc)\n",
        "dev_strings_end = []\n",
        "for dev_strings in dev_answer_lists:\n",
        "  dev_strings_processed = []\n",
        "  for dev_string in dev_strings:\n",
        "    dev_string = dev_string.replace(\" ##\", \"\")\n",
        "    dev_strings_processed.append(dev_string)\n",
        "  dev_strings_end.append(dev_strings_processed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "YzeFT3AxxHyb",
        "outputId": "b6b9946f-d9da-4ef6-cf48-c22b5da0a639"
      },
      "execution_count": 430,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev_answers = dev_strings_end"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "uMtFl0B1xs50",
        "outputId": "ec94a3b9-3476-450c-ff16-cf1469167779"
      },
      "execution_count": 431,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dev_answers[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nli6rzxaxvDL",
        "outputId": "54400d9f-09db-4083-d084-c81d5774ffcd"
      },
      "execution_count": 432,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['rats', '900 mhz', 'rats', '900 mhz', 'pulsed emission with a specific absorption rate ( sar ) of 4 w / kg and continuous emission with high sar ( 32 w / kg )', 'balb / c mice', 'forty - eight', 'wistar rats', '96', '900 mhz , 1800 mhz and 2450 mhz', '5 . 84 × 10 - 4 w / kg , 5 . 94 × 10 - 4 w / kg and 6 . 4 × 10 - 4 w / kg', 'rats', '1800 mhz', '3 . 22 w / kg', 'wistar albino rats', '24', '2100 mhz', 'mice', 'whole - body exposed ( nexp = 8 ) for 2 hr to gsm 1800 mhz mobile phone radiation at an average electric field intensity range of 4 . 3 - 17 . 5 v / m or sham - exposed ( nse = 8 )', '1800 mhz', 'c57bl / 6 mice', '835 mhz', '4 . 0 w / kg', 'rats', '900 mhz', 'balb / c mice', 'twelve', '900 and 1800 mhz', 'rats', '1800mhz', 'rats', '900 and 2100 mhz', 'sprague - dawley rats', '20', '900 mhz', 'mice', '835 mhz', '4 . 0 w / kg', 'c57bl / 6 mice', '835 mhz', '4 . 0 w / kg', 'chicken eggs', '450 eggs per treatment in three successive rounds per treatment', 'gsm ( 1 . 8 ghz ) , dect ( 1 . 88 ghz ) , umts ( 2 . 1 ghz ) , and wlan ( 5 . 6 ghz )', 'flies', '0 . 15 w / kg', 'rats', '14', '900 mhz', '0 . 0369 w / kg', 'sprague - dawley rats', '900 mhz', 'wistar rat', 'balb / c mice', 'three equally divided groups of animals ( 6 animals / group ) were used', 'the first group was exposed to a typical mobile phone , at a sar level range of 0 . 17 - 0 . 37 w / kg for 3 h daily for 8 months , the second group was exposed to a wireless dect base ( digital enhanced cordless telecommunications / telephone ) at a sar level range of 0 . 012 - 0 . 028 w / kg', 'rats', 'group 1 is composed of 3g - emr - exposed rats ( n = 9 ) and group 2 is the control group ( n = 9 )', 'rats', 'sprague - dawley rats', 'mice', '1800 mhz', '1 . 1 w / kg', 'rats', '915 mhz', '0 . 4 mw / g', 'mice', '900 mhz', 'rats', '9 . 4 ghz', 'mice', '1 . 6 - ghz'], ['rats', '900 mhz', 'rats', '900 mhz', 'pulsed emission with a specific absorption rate ( sar ) of 4 w / kg and continuous emission with high sar ( 32 w / kg )', 'balb / c mice', 'forty - eight', 'wistar rats', '96', '900 mhz , 1800 mhz and 2450 mhz', '5 . 84 × 10 - 4 w / kg , 5 . 94 × 10 - 4 w / kg and 6 . 4 × 10 - 4 w / kg', 'rats', '1800 mhz', '3 . 22 w / kg', 'wistar albino rats', '24', '2100 mhz', 'mice', 'whole - body exposed ( nexp = 8 ) for 2 hr to gsm 1800 mhz mobile phone radiation at an average electric field intensity range of 4 . 3 - 17 . 5 v / m or sham - exposed ( nse = 8 )', '1800 mhz', 'c57bl / 6 mice', '835 mhz', '4 . 0 w / kg', 'rats', '900 mhz', 'balb / c mice', 'twelve', '900 and 1800 mhz', 'rats', '1800mhz', 'rats', '900 and 2100 mhz', 'sprague - dawley rats', '20', '900 mhz', 'mice', '835 mhz', '4 . 0 w / kg', 'c57bl / 6 mice', '835 mhz', '4 . 0 w / kg', 'chicken eggs', '450 eggs per treatment in three successive rounds per treatment', 'gsm ( 1 . 8 ghz ) , dect ( 1 . 88 ghz ) , umts ( 2 . 1 ghz ) , and wlan ( 5 . 6 ghz )', 'flies', '0 . 15 w / kg', 'rats', '14', '900 mhz', '0 . 0369 w / kg', 'sprague - dawley rats', '900 mhz', 'wistar rat', 'balb / c mice', 'three equally divided groups of animals ( 6 animals / group ) were used', 'the first group was exposed to a typical mobile phone , at a sar level range of 0 . 17 - 0 . 37 w / kg for 3 h daily for 8 months , the second group was exposed to a wireless dect base ( digital enhanced cordless telecommunications / telephone ) at a sar level range of 0 . 012 - 0 . 028 w / kg', 'rats', 'group 1 is composed of 3g - emr - exposed rats ( n = 9 ) and group 2 is the control group ( n = 9 )', 'rats', 'sprague - dawley rats', 'mice', '1800 mhz', '1 . 1 w / kg', 'rats', '915 mhz', '0 . 4 mw / g', 'mice', '900 mhz', 'rats', '9 . 4 ghz', 'mice', '1 . 6 - ghz'], ['rats', '900 mhz', 'rats', '900 mhz', 'pulsed emission with a specific absorption rate ( sar ) of 4 w / kg and continuous emission with high sar ( 32 w / kg )', 'balb / c mice', 'forty - eight', 'wistar rats', '96', '900 mhz , 1800 mhz and 2450 mhz', '5 . 84 × 10 - 4 w / kg , 5 . 94 × 10 - 4 w / kg and 6 . 4 × 10 - 4 w / kg', 'rats', '1800 mhz', '3 . 22 w / kg', 'wistar albino rats', '24', '2100 mhz', 'mice', 'whole - body exposed ( nexp = 8 ) for 2 hr to gsm 1800 mhz mobile phone radiation at an average electric field intensity range of 4 . 3 - 17 . 5 v / m or sham - exposed ( nse = 8 )', '1800 mhz', 'c57bl / 6 mice', '835 mhz', '4 . 0 w / kg', 'rats', '900 mhz', 'balb / c mice', 'twelve', '900 and 1800 mhz', 'rats', '1800mhz', 'rats', '900 and 2100 mhz', 'sprague - dawley rats', '20', '900 mhz', 'mice', '835 mhz', '4 . 0 w / kg', 'c57bl / 6 mice', '835 mhz', '4 . 0 w / kg', 'chicken eggs', '450 eggs per treatment in three successive rounds per treatment', 'gsm ( 1 . 8 ghz ) , dect ( 1 . 88 ghz ) , umts ( 2 . 1 ghz ) , and wlan ( 5 . 6 ghz )', 'flies', '0 . 15 w / kg', 'rats', '14', '900 mhz', '0 . 0369 w / kg', 'sprague - dawley rats', '900 mhz', 'wistar rat', 'balb / c mice', 'three equally divided groups of animals ( 6 animals / group ) were used', 'the first group was exposed to a typical mobile phone , at a sar level range of 0 . 17 - 0 . 37 w / kg for 3 h daily for 8 months , the second group was exposed to a wireless dect base ( digital enhanced cordless telecommunications / telephone ) at a sar level range of 0 . 012 - 0 . 028 w / kg', 'rats', 'group 1 is composed of 3g - emr - exposed rats ( n = 9 ) and group 2 is the control group ( n = 9 )', 'rats', 'sprague - dawley rats', 'mice', '1800 mhz', '1 . 1 w / kg', 'rats', '915 mhz', '0 . 4 mw / g', 'mice', '900 mhz', 'rats', '9 . 4 ghz', 'mice', '1 . 6 - ghz'], ['rats', '900 mhz', 'rats', '900 mhz', 'pulsed emission with a specific absorption rate ( sar ) of 4 w / kg and continuous emission with high sar ( 32 w / kg )', 'balb / c mice', 'forty - eight', 'wistar rats', '96', '900 mhz , 1800 mhz and 2450 mhz', '5 . 84 × 10 - 4 w / kg , 5 . 94 × 10 - 4 w / kg and 6 . 4 × 10 - 4 w / kg', 'rats', '1800 mhz', '3 . 22 w / kg', 'wistar albino rats', '24', '2100 mhz', 'mice', 'whole - body exposed ( nexp = 8 ) for 2 hr to gsm 1800 mhz mobile phone radiation at an average electric field intensity range of 4 . 3 - 17 . 5 v / m or sham - exposed ( nse = 8 )', '1800 mhz', 'c57bl / 6 mice', '835 mhz', '4 . 0 w / kg', 'rats', '900 mhz', 'balb / c mice', 'twelve', '900 and 1800 mhz', 'rats', '1800mhz', 'rats', '900 and 2100 mhz', 'sprague - dawley rats', '20', '900 mhz', 'mice', '835 mhz', '4 . 0 w / kg', 'c57bl / 6 mice', '835 mhz', '4 . 0 w / kg', 'chicken eggs', '450 eggs per treatment in three successive rounds per treatment', 'gsm ( 1 . 8 ghz ) , dect ( 1 . 88 ghz ) , umts ( 2 . 1 ghz ) , and wlan ( 5 . 6 ghz )', 'flies', '0 . 15 w / kg', 'rats', '14', '900 mhz', '0 . 0369 w / kg', 'sprague - dawley rats', '900 mhz', 'wistar rat', 'balb / c mice', 'three equally divided groups of animals ( 6 animals / group ) were used', 'the first group was exposed to a typical mobile phone , at a sar level range of 0 . 17 - 0 . 37 w / kg for 3 h daily for 8 months , the second group was exposed to a wireless dect base ( digital enhanced cordless telecommunications / telephone ) at a sar level range of 0 . 012 - 0 . 028 w / kg', 'rats', 'group 1 is composed of 3g - emr - exposed rats ( n = 9 ) and group 2 is the control group ( n = 9 )', 'rats', 'sprague - dawley rats', 'mice', '1800 mhz', '1 . 1 w / kg', 'rats', '915 mhz', '0 . 4 mw / g', 'mice', '900 mhz', 'rats', '9 . 4 ghz', 'mice', '1 . 6 - ghz'], ['rats', '900 mhz', 'rats', '900 mhz', 'pulsed emission with a specific absorption rate ( sar ) of 4 w / kg and continuous emission with high sar ( 32 w / kg )', 'balb / c mice', 'forty - eight', 'wistar rats', '96', '900 mhz , 1800 mhz and 2450 mhz', '5 . 84 × 10 - 4 w / kg , 5 . 94 × 10 - 4 w / kg and 6 . 4 × 10 - 4 w / kg', 'rats', '1800 mhz', '3 . 22 w / kg', 'wistar albino rats', '24', '2100 mhz', 'mice', 'whole - body exposed ( nexp = 8 ) for 2 hr to gsm 1800 mhz mobile phone radiation at an average electric field intensity range of 4 . 3 - 17 . 5 v / m or sham - exposed ( nse = 8 )', '1800 mhz', 'c57bl / 6 mice', '835 mhz', '4 . 0 w / kg', 'rats', '900 mhz', 'balb / c mice', 'twelve', '900 and 1800 mhz', 'rats', '1800mhz', 'rats', '900 and 2100 mhz', 'sprague - dawley rats', '20', '900 mhz', 'mice', '835 mhz', '4 . 0 w / kg', 'c57bl / 6 mice', '835 mhz', '4 . 0 w / kg', 'chicken eggs', '450 eggs per treatment in three successive rounds per treatment', 'gsm ( 1 . 8 ghz ) , dect ( 1 . 88 ghz ) , umts ( 2 . 1 ghz ) , and wlan ( 5 . 6 ghz )', 'flies', '0 . 15 w / kg', 'rats', '14', '900 mhz', '0 . 0369 w / kg', 'sprague - dawley rats', '900 mhz', 'wistar rat', 'balb / c mice', 'three equally divided groups of animals ( 6 animals / group ) were used', 'the first group was exposed to a typical mobile phone , at a sar level range of 0 . 17 - 0 . 37 w / kg for 3 h daily for 8 months , the second group was exposed to a wireless dect base ( digital enhanced cordless telecommunications / telephone ) at a sar level range of 0 . 012 - 0 . 028 w / kg', 'rats', 'group 1 is composed of 3g - emr - exposed rats ( n = 9 ) and group 2 is the control group ( n = 9 )', 'rats', 'sprague - dawley rats', 'mice', '1800 mhz', '1 . 1 w / kg', 'rats', '915 mhz', '0 . 4 mw / g', 'mice', '900 mhz', 'rats', '9 . 4 ghz', 'mice', '1 . 6 - ghz']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Baseline Evaluation - F1 & EM"
      ],
      "metadata": {
        "id": "dnAhc8NNaEkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# F1 & EM function defined by myself\n",
        "from collections import Counter\n",
        "import string\n",
        "\n",
        "# In normalize step, doing some works such as converting words into lower case,\n",
        "# and deleting punctuations, clearing unnecessary spaces etc\n",
        "def normalize_answer(s):\n",
        "  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "  def remove_articles(text):\n",
        "    return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "  \n",
        "  def white_space_fix(text):\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "  def remove_punc(text):\n",
        "    exclude = set(string.punctuation)\n",
        "    return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "  def lower(text):\n",
        "    return text.lower()\n",
        "  \n",
        "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "  prediction_tokens = normalize_answer(prediction).split()\n",
        "  ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "  common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "  num_same = sum(common.values())\n",
        "  if num_same == 0:\n",
        "    return 0\n",
        "  precision = 1.0 * num_same / len(prediction_tokens)\n",
        "  recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "  f1 = (2 * precision * recall) / (precision + recall)\n",
        "  return f1\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "  return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "  scores_for_ground_truths = []\n",
        "  for ground_truth in ground_truths:\n",
        "    score = metric_fn(prediction, ground_truth)\n",
        "    scores_for_ground_truths.append(score)\n",
        "  return max(scores_for_ground_truths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "pjRGndV4xxAT",
        "outputId": "aaa6e254-c3a7-4a2b-854d-abb576269a0a"
      },
      "execution_count": 433,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating F1 Score\n",
        "# Theoretical Max : 88%\n",
        "f1_sum = 0\n",
        "\n",
        "for i in range(len(untokenized)):\n",
        "  f1 = metric_max_over_ground_truths(f1_score, untokenized[i], dev_answers[i])\n",
        "  f1_sum += f1\n",
        "print(\"f1 score: \", f1_sum/length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "Sr3nPoMubM1t",
        "outputId": "443962f7-cd20-47a4-c45e-a9954766992d"
      },
      "execution_count": 434,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1 score:  0.8190902464122276\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating EM score\n",
        "# Theoretical Max : 80%\n",
        "EM_sum = 0\n",
        "\n",
        "for i in range(len(untokenized)):\n",
        "  EM = metric_max_over_ground_truths(exact_match_score, untokenized[i], dev_answers[i])\n",
        "  EM_sum += EM\n",
        "print(\"EM score: \", EM_sum/length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "K9RKBP05eOZu",
        "outputId": "08c5769c-46c0-4714-fb14-4cfe9490152a"
      },
      "execution_count": 435,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EM score:  0.6323529411764706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Fine-tuning Training (Re-training)"
      ],
      "metadata": {
        "id": "iSFE_SznKUgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Our dataset loading in Pandas format\n",
        "import pandas as pd\n",
        "fine_train = pd.read_excel('Train_QA_mod.xlsx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "BsEKigwILBpT",
        "outputId": "c197fa67-635d-4a33-abc7-259db00f70b5"
      },
      "execution_count": 437,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fine_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 855
        },
        "id": "SFLuQo3hLDXL",
        "outputId": "2940c786-b96b-4db2-f1dd-b46561413aff"
      },
      "execution_count": 438,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a71eac63-dc65-48e9-b9a8-bfc7252aed5d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Index</th>\n",
              "      <th>Paper</th>\n",
              "      <th>Category</th>\n",
              "      <th>Question</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Experimental Studies of Brain Tumor Developmen...</td>\n",
              "      <td>Research Subject</td>\n",
              "      <td>What animal has been used?</td>\n",
              "      <td>It has been suggested that electromagnetic fie...</td>\n",
              "      <td>Fischer 344 rats</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Experimental Studies of Brain Tumor Developmen...</td>\n",
              "      <td>Number of Research Subject</td>\n",
              "      <td>How many animals were used?</td>\n",
              "      <td>It has been suggested that electromagnetic fie...</td>\n",
              "      <td>37 experimental rats and 37 matched controls</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Experimental Studies of Brain Tumor Developmen...</td>\n",
              "      <td>Radio Frequency</td>\n",
              "      <td>What is the signal frequency?</td>\n",
              "      <td>It has been suggested that electromagnetic fie...</td>\n",
              "      <td>915 MHz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>Experimental Studies of Brain Tumor Developmen...</td>\n",
              "      <td>Other Units of Exposure Level</td>\n",
              "      <td>How much W/kg was used?</td>\n",
              "      <td>It has been suggested that electromagnetic fie...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>Chronic Exposure of Cancer-Prone Mice to Low-L...</td>\n",
              "      <td>Research Subject</td>\n",
              "      <td>What animal has been used?</td>\n",
              "      <td>The purpose of this study was to determine whe...</td>\n",
              "      <td>C3H/ HeJ mice</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>663</th>\n",
              "      <td>368</td>\n",
              "      <td>Melatonin modulates 900 Mhz microwave-induced ...</td>\n",
              "      <td>Other Units of Exposure Level</td>\n",
              "      <td>How much W/kg was used?</td>\n",
              "      <td>Microwaves (MW) from cellular phones may affec...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>664</th>\n",
              "      <td>370</td>\n",
              "      <td>Does 900 MHz GSM Mobile Phone Exposure Affect ...</td>\n",
              "      <td>Research Subject</td>\n",
              "      <td>What animal has been used?</td>\n",
              "      <td>This study investigated the effects of cell ph...</td>\n",
              "      <td>Sprague-Dawley rats</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>665</th>\n",
              "      <td>370</td>\n",
              "      <td>Does 900 MHz GSM Mobile Phone Exposure Affect ...</td>\n",
              "      <td>Number of Research Subject</td>\n",
              "      <td>How many animals were used?</td>\n",
              "      <td>This study investigated the effects of cell ph...</td>\n",
              "      <td>Sixteen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>666</th>\n",
              "      <td>370</td>\n",
              "      <td>Does 900 MHz GSM Mobile Phone Exposure Affect ...</td>\n",
              "      <td>Radio Frequency</td>\n",
              "      <td>What is the signal frequency?</td>\n",
              "      <td>This study investigated the effects of cell ph...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>667</th>\n",
              "      <td>370</td>\n",
              "      <td>Does 900 MHz GSM Mobile Phone Exposure Affect ...</td>\n",
              "      <td>Other Units of Exposure Level</td>\n",
              "      <td>How much W/kg was used?</td>\n",
              "      <td>This study investigated the effects of cell ph...</td>\n",
              "      <td>0.52 W/kg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>668 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a71eac63-dc65-48e9-b9a8-bfc7252aed5d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a71eac63-dc65-48e9-b9a8-bfc7252aed5d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a71eac63-dc65-48e9-b9a8-bfc7252aed5d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     Index  ...                                        Answer\n",
              "0        1  ...                              Fischer 344 rats\n",
              "1        1  ...  37 experimental rats and 37 matched controls\n",
              "2        1  ...                                       915 MHz\n",
              "3        1  ...                                           NaN\n",
              "4        3  ...                                 C3H/ HeJ mice\n",
              "..     ...  ...                                           ...\n",
              "663    368  ...                                           NaN\n",
              "664    370  ...                           Sprague-Dawley rats\n",
              "665    370  ...                                       Sixteen\n",
              "666    370  ...                                           NaN\n",
              "667    370  ...                                     0.52 W/kg\n",
              "\n",
              "[668 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 438
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Hyperparameters set up\n",
        "\n",
        "# # Input sequence maximum length setup (Question + Context)\n",
        "# SEQ_LEN = 384\n",
        "\n",
        "# # Batch size (training size at one time)\n",
        "# # Refer to this site https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "# BATCH_SIZE = 32\n",
        "\n",
        "# EPOCH = 4\n",
        "\n",
        "# # Learning rate is set in very small number like 1.5e-5.\n",
        "# # In fine tuning session, Learning rate must be very small \n",
        "# # (Optimizer RAdam starts from LR 0 to 1.5e-5)\n",
        "# LR = 1.5e-5\n",
        "\n",
        "# # Model location designation\n",
        "# pretrained_path = \"bert\"\n",
        "# # Pretrained hyperparameters like weights\n",
        "# checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
        "# # Word corpus file location designation\n",
        "# vocab_path =  os.path.join(pretrained_path, 'vocab.txt')\n",
        "\n",
        "# # Configuration file designation\n",
        "# config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
        "\n",
        "# # Column name of context\n",
        "# FINE_DATA_COLUMN = \"Abstract\"\n",
        "# # Column name of question\n",
        "# FINE_QUESTION_COLUMN = \"Question\"\n",
        "# # Column name of answer ('text')\n",
        "# FINE_TEXT = \"Answer\""
      ],
      "metadata": {
        "id": "H5VvLeZLSxVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters set up\n",
        "\n",
        "# Input sequence maximum length setup (Question + Context)\n",
        "SEQ_LEN = 384\n",
        "\n",
        "# Batch size (training size at one time)\n",
        "# Refer to this site https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "EPOCH = 4\n",
        "\n",
        "# Learning rate is set in very small number like 1.5e-5.\n",
        "# In fine tuning session, Learning rate must be very small \n",
        "# (Optimizer RAdam starts from LR 0 to 1.5e-5)\n",
        "LR = 1.5e-5\n",
        "\n",
        "# Model location designation\n",
        "pretrained_path = \"bert\"\n",
        "# Pretrained hyperparameters like weights\n",
        "checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
        "# Word corpus file location designation\n",
        "vocab_path =  os.path.join(pretrained_path, 'vocab.txt')\n",
        "\n",
        "# Configuration file designation\n",
        "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
        "\n",
        "# Column name of context\n",
        "FINE_DATA_COLUMN = \"Abstract\"\n",
        "# Column name of question\n",
        "FINE_QUESTION_COLUMN = \"Question\"\n",
        "# Column name of answer ('text')\n",
        "FINE_TEXT = \"Answer\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "v9WwWG6vLDUu",
        "outputId": "3b2ac537-b3b6-42f3-a534-3edf9d0e4763"
      },
      "execution_count": 439,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary Construction for indexing each word\n",
        "token_dict = {}\n",
        "with codecs.open(vocab_path, 'r', 'utf8') as reader:\n",
        "  for line in reader:\n",
        "    token = line.strip()\n",
        "    token_dict[token] = len(token_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "0rZzyDqpLDSO",
        "outputId": "b56d92db-fefb-491e-b264-4f8434e72917"
      },
      "execution_count": 440,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0y9FIdB1LDPl",
        "outputId": "27a218b9-4f13-4c71-a8e8-cf1a86a3446d"
      },
      "execution_count": 441,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'[PAD]': 0,\n",
              " '[unused0]': 1,\n",
              " '[unused1]': 2,\n",
              " '[unused2]': 3,\n",
              " '[unused3]': 4,\n",
              " '[unused4]': 5,\n",
              " '[unused5]': 6,\n",
              " '[unused6]': 7,\n",
              " '[unused7]': 8,\n",
              " '[unused8]': 9,\n",
              " '[unused9]': 10,\n",
              " '[unused10]': 11,\n",
              " '[unused11]': 12,\n",
              " '[unused12]': 13,\n",
              " '[unused13]': 14,\n",
              " '[unused14]': 15,\n",
              " '[unused15]': 16,\n",
              " '[unused16]': 17,\n",
              " '[unused17]': 18,\n",
              " '[unused18]': 19,\n",
              " '[unused19]': 20,\n",
              " '[unused20]': 21,\n",
              " '[unused21]': 22,\n",
              " '[unused22]': 23,\n",
              " '[unused23]': 24,\n",
              " '[unused24]': 25,\n",
              " '[unused25]': 26,\n",
              " '[unused26]': 27,\n",
              " '[unused27]': 28,\n",
              " '[unused28]': 29,\n",
              " '[unused29]': 30,\n",
              " '[unused30]': 31,\n",
              " '[unused31]': 32,\n",
              " '[unused32]': 33,\n",
              " '[unused33]': 34,\n",
              " '[unused34]': 35,\n",
              " '[unused35]': 36,\n",
              " '[unused36]': 37,\n",
              " '[unused37]': 38,\n",
              " '[unused38]': 39,\n",
              " '[unused39]': 40,\n",
              " '[unused40]': 41,\n",
              " '[unused41]': 42,\n",
              " '[unused42]': 43,\n",
              " '[unused43]': 44,\n",
              " '[unused44]': 45,\n",
              " '[unused45]': 46,\n",
              " '[unused46]': 47,\n",
              " '[unused47]': 48,\n",
              " '[unused48]': 49,\n",
              " '[unused49]': 50,\n",
              " '[unused50]': 51,\n",
              " '[unused51]': 52,\n",
              " '[unused52]': 53,\n",
              " '[unused53]': 54,\n",
              " '[unused54]': 55,\n",
              " '[unused55]': 56,\n",
              " '[unused56]': 57,\n",
              " '[unused57]': 58,\n",
              " '[unused58]': 59,\n",
              " '[unused59]': 60,\n",
              " '[unused60]': 61,\n",
              " '[unused61]': 62,\n",
              " '[unused62]': 63,\n",
              " '[unused63]': 64,\n",
              " '[unused64]': 65,\n",
              " '[unused65]': 66,\n",
              " '[unused66]': 67,\n",
              " '[unused67]': 68,\n",
              " '[unused68]': 69,\n",
              " '[unused69]': 70,\n",
              " '[unused70]': 71,\n",
              " '[unused71]': 72,\n",
              " '[unused72]': 73,\n",
              " '[unused73]': 74,\n",
              " '[unused74]': 75,\n",
              " '[unused75]': 76,\n",
              " '[unused76]': 77,\n",
              " '[unused77]': 78,\n",
              " '[unused78]': 79,\n",
              " '[unused79]': 80,\n",
              " '[unused80]': 81,\n",
              " '[unused81]': 82,\n",
              " '[unused82]': 83,\n",
              " '[unused83]': 84,\n",
              " '[unused84]': 85,\n",
              " '[unused85]': 86,\n",
              " '[unused86]': 87,\n",
              " '[unused87]': 88,\n",
              " '[unused88]': 89,\n",
              " '[unused89]': 90,\n",
              " '[unused90]': 91,\n",
              " '[unused91]': 92,\n",
              " '[unused92]': 93,\n",
              " '[unused93]': 94,\n",
              " '[unused94]': 95,\n",
              " '[unused95]': 96,\n",
              " '[unused96]': 97,\n",
              " '[unused97]': 98,\n",
              " '[unused98]': 99,\n",
              " '[UNK]': 100,\n",
              " '[CLS]': 101,\n",
              " '[SEP]': 102,\n",
              " '[MASK]': 103,\n",
              " '[unused99]': 104,\n",
              " '[unused100]': 105,\n",
              " '[unused101]': 106,\n",
              " '[unused102]': 107,\n",
              " '[unused103]': 108,\n",
              " '[unused104]': 109,\n",
              " '[unused105]': 110,\n",
              " '[unused106]': 111,\n",
              " '[unused107]': 112,\n",
              " '[unused108]': 113,\n",
              " '[unused109]': 114,\n",
              " '[unused110]': 115,\n",
              " '[unused111]': 116,\n",
              " '[unused112]': 117,\n",
              " '[unused113]': 118,\n",
              " '[unused114]': 119,\n",
              " '[unused115]': 120,\n",
              " '[unused116]': 121,\n",
              " '[unused117]': 122,\n",
              " '[unused118]': 123,\n",
              " '[unused119]': 124,\n",
              " '[unused120]': 125,\n",
              " '[unused121]': 126,\n",
              " '[unused122]': 127,\n",
              " '[unused123]': 128,\n",
              " '[unused124]': 129,\n",
              " '[unused125]': 130,\n",
              " '[unused126]': 131,\n",
              " '[unused127]': 132,\n",
              " '[unused128]': 133,\n",
              " '[unused129]': 134,\n",
              " '[unused130]': 135,\n",
              " '[unused131]': 136,\n",
              " '[unused132]': 137,\n",
              " '[unused133]': 138,\n",
              " '[unused134]': 139,\n",
              " '[unused135]': 140,\n",
              " '[unused136]': 141,\n",
              " '[unused137]': 142,\n",
              " '[unused138]': 143,\n",
              " '[unused139]': 144,\n",
              " '[unused140]': 145,\n",
              " '[unused141]': 146,\n",
              " '[unused142]': 147,\n",
              " '[unused143]': 148,\n",
              " '[unused144]': 149,\n",
              " '[unused145]': 150,\n",
              " '[unused146]': 151,\n",
              " '[unused147]': 152,\n",
              " '[unused148]': 153,\n",
              " '[unused149]': 154,\n",
              " '[unused150]': 155,\n",
              " '[unused151]': 156,\n",
              " '[unused152]': 157,\n",
              " '[unused153]': 158,\n",
              " '[unused154]': 159,\n",
              " '[unused155]': 160,\n",
              " '[unused156]': 161,\n",
              " '[unused157]': 162,\n",
              " '[unused158]': 163,\n",
              " '[unused159]': 164,\n",
              " '[unused160]': 165,\n",
              " '[unused161]': 166,\n",
              " '[unused162]': 167,\n",
              " '[unused163]': 168,\n",
              " '[unused164]': 169,\n",
              " '[unused165]': 170,\n",
              " '[unused166]': 171,\n",
              " '[unused167]': 172,\n",
              " '[unused168]': 173,\n",
              " '[unused169]': 174,\n",
              " '[unused170]': 175,\n",
              " '[unused171]': 176,\n",
              " '[unused172]': 177,\n",
              " '[unused173]': 178,\n",
              " '[unused174]': 179,\n",
              " '[unused175]': 180,\n",
              " '[unused176]': 181,\n",
              " '[unused177]': 182,\n",
              " '[unused178]': 183,\n",
              " '[unused179]': 184,\n",
              " '[unused180]': 185,\n",
              " '[unused181]': 186,\n",
              " '[unused182]': 187,\n",
              " '[unused183]': 188,\n",
              " '[unused184]': 189,\n",
              " '[unused185]': 190,\n",
              " '[unused186]': 191,\n",
              " '[unused187]': 192,\n",
              " '[unused188]': 193,\n",
              " '[unused189]': 194,\n",
              " '[unused190]': 195,\n",
              " '[unused191]': 196,\n",
              " '[unused192]': 197,\n",
              " '[unused193]': 198,\n",
              " '[unused194]': 199,\n",
              " '[unused195]': 200,\n",
              " '[unused196]': 201,\n",
              " '[unused197]': 202,\n",
              " '[unused198]': 203,\n",
              " '[unused199]': 204,\n",
              " '[unused200]': 205,\n",
              " '[unused201]': 206,\n",
              " '[unused202]': 207,\n",
              " '[unused203]': 208,\n",
              " '[unused204]': 209,\n",
              " '[unused205]': 210,\n",
              " '[unused206]': 211,\n",
              " '[unused207]': 212,\n",
              " '[unused208]': 213,\n",
              " '[unused209]': 214,\n",
              " '[unused210]': 215,\n",
              " '[unused211]': 216,\n",
              " '[unused212]': 217,\n",
              " '[unused213]': 218,\n",
              " '[unused214]': 219,\n",
              " '[unused215]': 220,\n",
              " '[unused216]': 221,\n",
              " '[unused217]': 222,\n",
              " '[unused218]': 223,\n",
              " '[unused219]': 224,\n",
              " '[unused220]': 225,\n",
              " '[unused221]': 226,\n",
              " '[unused222]': 227,\n",
              " '[unused223]': 228,\n",
              " '[unused224]': 229,\n",
              " '[unused225]': 230,\n",
              " '[unused226]': 231,\n",
              " '[unused227]': 232,\n",
              " '[unused228]': 233,\n",
              " '[unused229]': 234,\n",
              " '[unused230]': 235,\n",
              " '[unused231]': 236,\n",
              " '[unused232]': 237,\n",
              " '[unused233]': 238,\n",
              " '[unused234]': 239,\n",
              " '[unused235]': 240,\n",
              " '[unused236]': 241,\n",
              " '[unused237]': 242,\n",
              " '[unused238]': 243,\n",
              " '[unused239]': 244,\n",
              " '[unused240]': 245,\n",
              " '[unused241]': 246,\n",
              " '[unused242]': 247,\n",
              " '[unused243]': 248,\n",
              " '[unused244]': 249,\n",
              " '[unused245]': 250,\n",
              " '[unused246]': 251,\n",
              " '[unused247]': 252,\n",
              " '[unused248]': 253,\n",
              " '[unused249]': 254,\n",
              " '[unused250]': 255,\n",
              " '[unused251]': 256,\n",
              " '[unused252]': 257,\n",
              " '[unused253]': 258,\n",
              " '[unused254]': 259,\n",
              " '[unused255]': 260,\n",
              " '[unused256]': 261,\n",
              " '[unused257]': 262,\n",
              " '[unused258]': 263,\n",
              " '[unused259]': 264,\n",
              " '[unused260]': 265,\n",
              " '[unused261]': 266,\n",
              " '[unused262]': 267,\n",
              " '[unused263]': 268,\n",
              " '[unused264]': 269,\n",
              " '[unused265]': 270,\n",
              " '[unused266]': 271,\n",
              " '[unused267]': 272,\n",
              " '[unused268]': 273,\n",
              " '[unused269]': 274,\n",
              " '[unused270]': 275,\n",
              " '[unused271]': 276,\n",
              " '[unused272]': 277,\n",
              " '[unused273]': 278,\n",
              " '[unused274]': 279,\n",
              " '[unused275]': 280,\n",
              " '[unused276]': 281,\n",
              " '[unused277]': 282,\n",
              " '[unused278]': 283,\n",
              " '[unused279]': 284,\n",
              " '[unused280]': 285,\n",
              " '[unused281]': 286,\n",
              " '[unused282]': 287,\n",
              " '[unused283]': 288,\n",
              " '[unused284]': 289,\n",
              " '[unused285]': 290,\n",
              " '[unused286]': 291,\n",
              " '[unused287]': 292,\n",
              " '[unused288]': 293,\n",
              " '[unused289]': 294,\n",
              " '[unused290]': 295,\n",
              " '[unused291]': 296,\n",
              " '[unused292]': 297,\n",
              " '[unused293]': 298,\n",
              " '[unused294]': 299,\n",
              " '[unused295]': 300,\n",
              " '[unused296]': 301,\n",
              " '[unused297]': 302,\n",
              " '[unused298]': 303,\n",
              " '[unused299]': 304,\n",
              " '[unused300]': 305,\n",
              " '[unused301]': 306,\n",
              " '[unused302]': 307,\n",
              " '[unused303]': 308,\n",
              " '[unused304]': 309,\n",
              " '[unused305]': 310,\n",
              " '[unused306]': 311,\n",
              " '[unused307]': 312,\n",
              " '[unused308]': 313,\n",
              " '[unused309]': 314,\n",
              " '[unused310]': 315,\n",
              " '[unused311]': 316,\n",
              " '[unused312]': 317,\n",
              " '[unused313]': 318,\n",
              " '[unused314]': 319,\n",
              " '[unused315]': 320,\n",
              " '[unused316]': 321,\n",
              " '[unused317]': 322,\n",
              " '[unused318]': 323,\n",
              " '[unused319]': 324,\n",
              " '[unused320]': 325,\n",
              " '[unused321]': 326,\n",
              " '[unused322]': 327,\n",
              " '[unused323]': 328,\n",
              " '[unused324]': 329,\n",
              " '[unused325]': 330,\n",
              " '[unused326]': 331,\n",
              " '[unused327]': 332,\n",
              " '[unused328]': 333,\n",
              " '[unused329]': 334,\n",
              " '[unused330]': 335,\n",
              " '[unused331]': 336,\n",
              " '[unused332]': 337,\n",
              " '[unused333]': 338,\n",
              " '[unused334]': 339,\n",
              " '[unused335]': 340,\n",
              " '[unused336]': 341,\n",
              " '[unused337]': 342,\n",
              " '[unused338]': 343,\n",
              " '[unused339]': 344,\n",
              " '[unused340]': 345,\n",
              " '[unused341]': 346,\n",
              " '[unused342]': 347,\n",
              " '[unused343]': 348,\n",
              " '[unused344]': 349,\n",
              " '[unused345]': 350,\n",
              " '[unused346]': 351,\n",
              " '[unused347]': 352,\n",
              " '[unused348]': 353,\n",
              " '[unused349]': 354,\n",
              " '[unused350]': 355,\n",
              " '[unused351]': 356,\n",
              " '[unused352]': 357,\n",
              " '[unused353]': 358,\n",
              " '[unused354]': 359,\n",
              " '[unused355]': 360,\n",
              " '[unused356]': 361,\n",
              " '[unused357]': 362,\n",
              " '[unused358]': 363,\n",
              " '[unused359]': 364,\n",
              " '[unused360]': 365,\n",
              " '[unused361]': 366,\n",
              " '[unused362]': 367,\n",
              " '[unused363]': 368,\n",
              " '[unused364]': 369,\n",
              " '[unused365]': 370,\n",
              " '[unused366]': 371,\n",
              " '[unused367]': 372,\n",
              " '[unused368]': 373,\n",
              " '[unused369]': 374,\n",
              " '[unused370]': 375,\n",
              " '[unused371]': 376,\n",
              " '[unused372]': 377,\n",
              " '[unused373]': 378,\n",
              " '[unused374]': 379,\n",
              " '[unused375]': 380,\n",
              " '[unused376]': 381,\n",
              " '[unused377]': 382,\n",
              " '[unused378]': 383,\n",
              " '[unused379]': 384,\n",
              " '[unused380]': 385,\n",
              " '[unused381]': 386,\n",
              " '[unused382]': 387,\n",
              " '[unused383]': 388,\n",
              " '[unused384]': 389,\n",
              " '[unused385]': 390,\n",
              " '[unused386]': 391,\n",
              " '[unused387]': 392,\n",
              " '[unused388]': 393,\n",
              " '[unused389]': 394,\n",
              " '[unused390]': 395,\n",
              " '[unused391]': 396,\n",
              " '[unused392]': 397,\n",
              " '[unused393]': 398,\n",
              " '[unused394]': 399,\n",
              " '[unused395]': 400,\n",
              " '[unused396]': 401,\n",
              " '[unused397]': 402,\n",
              " '[unused398]': 403,\n",
              " '[unused399]': 404,\n",
              " '[unused400]': 405,\n",
              " '[unused401]': 406,\n",
              " '[unused402]': 407,\n",
              " '[unused403]': 408,\n",
              " '[unused404]': 409,\n",
              " '[unused405]': 410,\n",
              " '[unused406]': 411,\n",
              " '[unused407]': 412,\n",
              " '[unused408]': 413,\n",
              " '[unused409]': 414,\n",
              " '[unused410]': 415,\n",
              " '[unused411]': 416,\n",
              " '[unused412]': 417,\n",
              " '[unused413]': 418,\n",
              " '[unused414]': 419,\n",
              " '[unused415]': 420,\n",
              " '[unused416]': 421,\n",
              " '[unused417]': 422,\n",
              " '[unused418]': 423,\n",
              " '[unused419]': 424,\n",
              " '[unused420]': 425,\n",
              " '[unused421]': 426,\n",
              " '[unused422]': 427,\n",
              " '[unused423]': 428,\n",
              " '[unused424]': 429,\n",
              " '[unused425]': 430,\n",
              " '[unused426]': 431,\n",
              " '[unused427]': 432,\n",
              " '[unused428]': 433,\n",
              " '[unused429]': 434,\n",
              " '[unused430]': 435,\n",
              " '[unused431]': 436,\n",
              " '[unused432]': 437,\n",
              " '[unused433]': 438,\n",
              " '[unused434]': 439,\n",
              " '[unused435]': 440,\n",
              " '[unused436]': 441,\n",
              " '[unused437]': 442,\n",
              " '[unused438]': 443,\n",
              " '[unused439]': 444,\n",
              " '[unused440]': 445,\n",
              " '[unused441]': 446,\n",
              " '[unused442]': 447,\n",
              " '[unused443]': 448,\n",
              " '[unused444]': 449,\n",
              " '[unused445]': 450,\n",
              " '[unused446]': 451,\n",
              " '[unused447]': 452,\n",
              " '[unused448]': 453,\n",
              " '[unused449]': 454,\n",
              " '[unused450]': 455,\n",
              " '[unused451]': 456,\n",
              " '[unused452]': 457,\n",
              " '[unused453]': 458,\n",
              " '[unused454]': 459,\n",
              " '[unused455]': 460,\n",
              " '[unused456]': 461,\n",
              " '[unused457]': 462,\n",
              " '[unused458]': 463,\n",
              " '[unused459]': 464,\n",
              " '[unused460]': 465,\n",
              " '[unused461]': 466,\n",
              " '[unused462]': 467,\n",
              " '[unused463]': 468,\n",
              " '[unused464]': 469,\n",
              " '[unused465]': 470,\n",
              " '[unused466]': 471,\n",
              " '[unused467]': 472,\n",
              " '[unused468]': 473,\n",
              " '[unused469]': 474,\n",
              " '[unused470]': 475,\n",
              " '[unused471]': 476,\n",
              " '[unused472]': 477,\n",
              " '[unused473]': 478,\n",
              " '[unused474]': 479,\n",
              " '[unused475]': 480,\n",
              " '[unused476]': 481,\n",
              " '[unused477]': 482,\n",
              " '[unused478]': 483,\n",
              " '[unused479]': 484,\n",
              " '[unused480]': 485,\n",
              " '[unused481]': 486,\n",
              " '[unused482]': 487,\n",
              " '[unused483]': 488,\n",
              " '[unused484]': 489,\n",
              " '[unused485]': 490,\n",
              " '[unused486]': 491,\n",
              " '[unused487]': 492,\n",
              " '[unused488]': 493,\n",
              " '[unused489]': 494,\n",
              " '[unused490]': 495,\n",
              " '[unused491]': 496,\n",
              " '[unused492]': 497,\n",
              " '[unused493]': 498,\n",
              " '[unused494]': 499,\n",
              " '[unused495]': 500,\n",
              " '[unused496]': 501,\n",
              " '[unused497]': 502,\n",
              " '[unused498]': 503,\n",
              " '[unused499]': 504,\n",
              " '[unused500]': 505,\n",
              " '[unused501]': 506,\n",
              " '[unused502]': 507,\n",
              " '[unused503]': 508,\n",
              " '[unused504]': 509,\n",
              " '[unused505]': 510,\n",
              " '[unused506]': 511,\n",
              " '[unused507]': 512,\n",
              " '[unused508]': 513,\n",
              " '[unused509]': 514,\n",
              " '[unused510]': 515,\n",
              " '[unused511]': 516,\n",
              " '[unused512]': 517,\n",
              " '[unused513]': 518,\n",
              " '[unused514]': 519,\n",
              " '[unused515]': 520,\n",
              " '[unused516]': 521,\n",
              " '[unused517]': 522,\n",
              " '[unused518]': 523,\n",
              " '[unused519]': 524,\n",
              " '[unused520]': 525,\n",
              " '[unused521]': 526,\n",
              " '[unused522]': 527,\n",
              " '[unused523]': 528,\n",
              " '[unused524]': 529,\n",
              " '[unused525]': 530,\n",
              " '[unused526]': 531,\n",
              " '[unused527]': 532,\n",
              " '[unused528]': 533,\n",
              " '[unused529]': 534,\n",
              " '[unused530]': 535,\n",
              " '[unused531]': 536,\n",
              " '[unused532]': 537,\n",
              " '[unused533]': 538,\n",
              " '[unused534]': 539,\n",
              " '[unused535]': 540,\n",
              " '[unused536]': 541,\n",
              " '[unused537]': 542,\n",
              " '[unused538]': 543,\n",
              " '[unused539]': 544,\n",
              " '[unused540]': 545,\n",
              " '[unused541]': 546,\n",
              " '[unused542]': 547,\n",
              " '[unused543]': 548,\n",
              " '[unused544]': 549,\n",
              " '[unused545]': 550,\n",
              " '[unused546]': 551,\n",
              " '[unused547]': 552,\n",
              " '[unused548]': 553,\n",
              " '[unused549]': 554,\n",
              " '[unused550]': 555,\n",
              " '[unused551]': 556,\n",
              " '[unused552]': 557,\n",
              " '[unused553]': 558,\n",
              " '[unused554]': 559,\n",
              " '[unused555]': 560,\n",
              " '[unused556]': 561,\n",
              " '[unused557]': 562,\n",
              " '[unused558]': 563,\n",
              " '[unused559]': 564,\n",
              " '[unused560]': 565,\n",
              " '[unused561]': 566,\n",
              " '[unused562]': 567,\n",
              " '[unused563]': 568,\n",
              " '[unused564]': 569,\n",
              " '[unused565]': 570,\n",
              " '[unused566]': 571,\n",
              " '[unused567]': 572,\n",
              " '[unused568]': 573,\n",
              " '[unused569]': 574,\n",
              " '[unused570]': 575,\n",
              " '[unused571]': 576,\n",
              " '[unused572]': 577,\n",
              " '[unused573]': 578,\n",
              " '[unused574]': 579,\n",
              " '[unused575]': 580,\n",
              " '[unused576]': 581,\n",
              " '[unused577]': 582,\n",
              " '[unused578]': 583,\n",
              " '[unused579]': 584,\n",
              " '[unused580]': 585,\n",
              " '[unused581]': 586,\n",
              " '[unused582]': 587,\n",
              " '[unused583]': 588,\n",
              " '[unused584]': 589,\n",
              " '[unused585]': 590,\n",
              " '[unused586]': 591,\n",
              " '[unused587]': 592,\n",
              " '[unused588]': 593,\n",
              " '[unused589]': 594,\n",
              " '[unused590]': 595,\n",
              " '[unused591]': 596,\n",
              " '[unused592]': 597,\n",
              " '[unused593]': 598,\n",
              " '[unused594]': 599,\n",
              " '[unused595]': 600,\n",
              " '[unused596]': 601,\n",
              " '[unused597]': 602,\n",
              " '[unused598]': 603,\n",
              " '[unused599]': 604,\n",
              " '[unused600]': 605,\n",
              " '[unused601]': 606,\n",
              " '[unused602]': 607,\n",
              " '[unused603]': 608,\n",
              " '[unused604]': 609,\n",
              " '[unused605]': 610,\n",
              " '[unused606]': 611,\n",
              " '[unused607]': 612,\n",
              " '[unused608]': 613,\n",
              " '[unused609]': 614,\n",
              " '[unused610]': 615,\n",
              " '[unused611]': 616,\n",
              " '[unused612]': 617,\n",
              " '[unused613]': 618,\n",
              " '[unused614]': 619,\n",
              " '[unused615]': 620,\n",
              " '[unused616]': 621,\n",
              " '[unused617]': 622,\n",
              " '[unused618]': 623,\n",
              " '[unused619]': 624,\n",
              " '[unused620]': 625,\n",
              " '[unused621]': 626,\n",
              " '[unused622]': 627,\n",
              " '[unused623]': 628,\n",
              " '[unused624]': 629,\n",
              " '[unused625]': 630,\n",
              " '[unused626]': 631,\n",
              " '[unused627]': 632,\n",
              " '[unused628]': 633,\n",
              " '[unused629]': 634,\n",
              " '[unused630]': 635,\n",
              " '[unused631]': 636,\n",
              " '[unused632]': 637,\n",
              " '[unused633]': 638,\n",
              " '[unused634]': 639,\n",
              " '[unused635]': 640,\n",
              " '[unused636]': 641,\n",
              " '[unused637]': 642,\n",
              " '[unused638]': 643,\n",
              " '[unused639]': 644,\n",
              " '[unused640]': 645,\n",
              " '[unused641]': 646,\n",
              " '[unused642]': 647,\n",
              " '[unused643]': 648,\n",
              " '[unused644]': 649,\n",
              " '[unused645]': 650,\n",
              " '[unused646]': 651,\n",
              " '[unused647]': 652,\n",
              " '[unused648]': 653,\n",
              " '[unused649]': 654,\n",
              " '[unused650]': 655,\n",
              " '[unused651]': 656,\n",
              " '[unused652]': 657,\n",
              " '[unused653]': 658,\n",
              " '[unused654]': 659,\n",
              " '[unused655]': 660,\n",
              " '[unused656]': 661,\n",
              " '[unused657]': 662,\n",
              " '[unused658]': 663,\n",
              " '[unused659]': 664,\n",
              " '[unused660]': 665,\n",
              " '[unused661]': 666,\n",
              " '[unused662]': 667,\n",
              " '[unused663]': 668,\n",
              " '[unused664]': 669,\n",
              " '[unused665]': 670,\n",
              " '[unused666]': 671,\n",
              " '[unused667]': 672,\n",
              " '[unused668]': 673,\n",
              " '[unused669]': 674,\n",
              " '[unused670]': 675,\n",
              " '[unused671]': 676,\n",
              " '[unused672]': 677,\n",
              " '[unused673]': 678,\n",
              " '[unused674]': 679,\n",
              " '[unused675]': 680,\n",
              " '[unused676]': 681,\n",
              " '[unused677]': 682,\n",
              " '[unused678]': 683,\n",
              " '[unused679]': 684,\n",
              " '[unused680]': 685,\n",
              " '[unused681]': 686,\n",
              " '[unused682]': 687,\n",
              " '[unused683]': 688,\n",
              " '[unused684]': 689,\n",
              " '[unused685]': 690,\n",
              " '[unused686]': 691,\n",
              " '[unused687]': 692,\n",
              " '[unused688]': 693,\n",
              " '[unused689]': 694,\n",
              " '[unused690]': 695,\n",
              " '[unused691]': 696,\n",
              " '[unused692]': 697,\n",
              " '[unused693]': 698,\n",
              " '[unused694]': 699,\n",
              " '[unused695]': 700,\n",
              " '[unused696]': 701,\n",
              " '[unused697]': 702,\n",
              " '[unused698]': 703,\n",
              " '[unused699]': 704,\n",
              " '[unused700]': 705,\n",
              " '[unused701]': 706,\n",
              " '[unused702]': 707,\n",
              " '[unused703]': 708,\n",
              " '[unused704]': 709,\n",
              " '[unused705]': 710,\n",
              " '[unused706]': 711,\n",
              " '[unused707]': 712,\n",
              " '[unused708]': 713,\n",
              " '[unused709]': 714,\n",
              " '[unused710]': 715,\n",
              " '[unused711]': 716,\n",
              " '[unused712]': 717,\n",
              " '[unused713]': 718,\n",
              " '[unused714]': 719,\n",
              " '[unused715]': 720,\n",
              " '[unused716]': 721,\n",
              " '[unused717]': 722,\n",
              " '[unused718]': 723,\n",
              " '[unused719]': 724,\n",
              " '[unused720]': 725,\n",
              " '[unused721]': 726,\n",
              " '[unused722]': 727,\n",
              " '[unused723]': 728,\n",
              " '[unused724]': 729,\n",
              " '[unused725]': 730,\n",
              " '[unused726]': 731,\n",
              " '[unused727]': 732,\n",
              " '[unused728]': 733,\n",
              " '[unused729]': 734,\n",
              " '[unused730]': 735,\n",
              " '[unused731]': 736,\n",
              " '[unused732]': 737,\n",
              " '[unused733]': 738,\n",
              " '[unused734]': 739,\n",
              " '[unused735]': 740,\n",
              " '[unused736]': 741,\n",
              " '[unused737]': 742,\n",
              " '[unused738]': 743,\n",
              " '[unused739]': 744,\n",
              " '[unused740]': 745,\n",
              " '[unused741]': 746,\n",
              " '[unused742]': 747,\n",
              " '[unused743]': 748,\n",
              " '[unused744]': 749,\n",
              " '[unused745]': 750,\n",
              " '[unused746]': 751,\n",
              " '[unused747]': 752,\n",
              " '[unused748]': 753,\n",
              " '[unused749]': 754,\n",
              " '[unused750]': 755,\n",
              " '[unused751]': 756,\n",
              " '[unused752]': 757,\n",
              " '[unused753]': 758,\n",
              " '[unused754]': 759,\n",
              " '[unused755]': 760,\n",
              " '[unused756]': 761,\n",
              " '[unused757]': 762,\n",
              " '[unused758]': 763,\n",
              " '[unused759]': 764,\n",
              " '[unused760]': 765,\n",
              " '[unused761]': 766,\n",
              " '[unused762]': 767,\n",
              " '[unused763]': 768,\n",
              " '[unused764]': 769,\n",
              " '[unused765]': 770,\n",
              " '[unused766]': 771,\n",
              " '[unused767]': 772,\n",
              " '[unused768]': 773,\n",
              " '[unused769]': 774,\n",
              " '[unused770]': 775,\n",
              " '[unused771]': 776,\n",
              " '[unused772]': 777,\n",
              " '[unused773]': 778,\n",
              " '[unused774]': 779,\n",
              " '[unused775]': 780,\n",
              " '[unused776]': 781,\n",
              " '[unused777]': 782,\n",
              " '[unused778]': 783,\n",
              " '[unused779]': 784,\n",
              " '[unused780]': 785,\n",
              " '[unused781]': 786,\n",
              " '[unused782]': 787,\n",
              " '[unused783]': 788,\n",
              " '[unused784]': 789,\n",
              " '[unused785]': 790,\n",
              " '[unused786]': 791,\n",
              " '[unused787]': 792,\n",
              " '[unused788]': 793,\n",
              " '[unused789]': 794,\n",
              " '[unused790]': 795,\n",
              " '[unused791]': 796,\n",
              " '[unused792]': 797,\n",
              " '[unused793]': 798,\n",
              " '[unused794]': 799,\n",
              " '[unused795]': 800,\n",
              " '[unused796]': 801,\n",
              " '[unused797]': 802,\n",
              " '[unused798]': 803,\n",
              " '[unused799]': 804,\n",
              " '[unused800]': 805,\n",
              " '[unused801]': 806,\n",
              " '[unused802]': 807,\n",
              " '[unused803]': 808,\n",
              " '[unused804]': 809,\n",
              " '[unused805]': 810,\n",
              " '[unused806]': 811,\n",
              " '[unused807]': 812,\n",
              " '[unused808]': 813,\n",
              " '[unused809]': 814,\n",
              " '[unused810]': 815,\n",
              " '[unused811]': 816,\n",
              " '[unused812]': 817,\n",
              " '[unused813]': 818,\n",
              " '[unused814]': 819,\n",
              " '[unused815]': 820,\n",
              " '[unused816]': 821,\n",
              " '[unused817]': 822,\n",
              " '[unused818]': 823,\n",
              " '[unused819]': 824,\n",
              " '[unused820]': 825,\n",
              " '[unused821]': 826,\n",
              " '[unused822]': 827,\n",
              " '[unused823]': 828,\n",
              " '[unused824]': 829,\n",
              " '[unused825]': 830,\n",
              " '[unused826]': 831,\n",
              " '[unused827]': 832,\n",
              " '[unused828]': 833,\n",
              " '[unused829]': 834,\n",
              " '[unused830]': 835,\n",
              " '[unused831]': 836,\n",
              " '[unused832]': 837,\n",
              " '[unused833]': 838,\n",
              " '[unused834]': 839,\n",
              " '[unused835]': 840,\n",
              " '[unused836]': 841,\n",
              " '[unused837]': 842,\n",
              " '[unused838]': 843,\n",
              " '[unused839]': 844,\n",
              " '[unused840]': 845,\n",
              " '[unused841]': 846,\n",
              " '[unused842]': 847,\n",
              " '[unused843]': 848,\n",
              " '[unused844]': 849,\n",
              " '[unused845]': 850,\n",
              " '[unused846]': 851,\n",
              " '[unused847]': 852,\n",
              " '[unused848]': 853,\n",
              " '[unused849]': 854,\n",
              " '[unused850]': 855,\n",
              " '[unused851]': 856,\n",
              " '[unused852]': 857,\n",
              " '[unused853]': 858,\n",
              " '[unused854]': 859,\n",
              " '[unused855]': 860,\n",
              " '[unused856]': 861,\n",
              " '[unused857]': 862,\n",
              " '[unused858]': 863,\n",
              " '[unused859]': 864,\n",
              " '[unused860]': 865,\n",
              " '[unused861]': 866,\n",
              " '[unused862]': 867,\n",
              " '[unused863]': 868,\n",
              " '[unused864]': 869,\n",
              " '[unused865]': 870,\n",
              " '[unused866]': 871,\n",
              " '[unused867]': 872,\n",
              " '[unused868]': 873,\n",
              " '[unused869]': 874,\n",
              " '[unused870]': 875,\n",
              " '[unused871]': 876,\n",
              " '[unused872]': 877,\n",
              " '[unused873]': 878,\n",
              " '[unused874]': 879,\n",
              " '[unused875]': 880,\n",
              " '[unused876]': 881,\n",
              " '[unused877]': 882,\n",
              " '[unused878]': 883,\n",
              " '[unused879]': 884,\n",
              " '[unused880]': 885,\n",
              " '[unused881]': 886,\n",
              " '[unused882]': 887,\n",
              " '[unused883]': 888,\n",
              " '[unused884]': 889,\n",
              " '[unused885]': 890,\n",
              " '[unused886]': 891,\n",
              " '[unused887]': 892,\n",
              " '[unused888]': 893,\n",
              " '[unused889]': 894,\n",
              " '[unused890]': 895,\n",
              " '[unused891]': 896,\n",
              " '[unused892]': 897,\n",
              " '[unused893]': 898,\n",
              " '[unused894]': 899,\n",
              " '[unused895]': 900,\n",
              " '[unused896]': 901,\n",
              " '[unused897]': 902,\n",
              " '[unused898]': 903,\n",
              " '[unused899]': 904,\n",
              " '[unused900]': 905,\n",
              " '[unused901]': 906,\n",
              " '[unused902]': 907,\n",
              " '[unused903]': 908,\n",
              " '[unused904]': 909,\n",
              " '[unused905]': 910,\n",
              " '[unused906]': 911,\n",
              " '[unused907]': 912,\n",
              " '[unused908]': 913,\n",
              " '[unused909]': 914,\n",
              " '[unused910]': 915,\n",
              " '[unused911]': 916,\n",
              " '[unused912]': 917,\n",
              " '[unused913]': 918,\n",
              " '[unused914]': 919,\n",
              " '[unused915]': 920,\n",
              " '[unused916]': 921,\n",
              " '[unused917]': 922,\n",
              " '[unused918]': 923,\n",
              " '[unused919]': 924,\n",
              " '[unused920]': 925,\n",
              " '[unused921]': 926,\n",
              " '[unused922]': 927,\n",
              " '[unused923]': 928,\n",
              " '[unused924]': 929,\n",
              " '[unused925]': 930,\n",
              " '[unused926]': 931,\n",
              " '[unused927]': 932,\n",
              " '[unused928]': 933,\n",
              " '[unused929]': 934,\n",
              " '[unused930]': 935,\n",
              " '[unused931]': 936,\n",
              " '[unused932]': 937,\n",
              " '[unused933]': 938,\n",
              " '[unused934]': 939,\n",
              " '[unused935]': 940,\n",
              " '[unused936]': 941,\n",
              " '[unused937]': 942,\n",
              " '[unused938]': 943,\n",
              " '[unused939]': 944,\n",
              " '[unused940]': 945,\n",
              " '[unused941]': 946,\n",
              " '[unused942]': 947,\n",
              " '[unused943]': 948,\n",
              " '[unused944]': 949,\n",
              " '[unused945]': 950,\n",
              " '[unused946]': 951,\n",
              " '[unused947]': 952,\n",
              " '[unused948]': 953,\n",
              " '[unused949]': 954,\n",
              " '[unused950]': 955,\n",
              " '[unused951]': 956,\n",
              " '[unused952]': 957,\n",
              " '[unused953]': 958,\n",
              " '[unused954]': 959,\n",
              " '[unused955]': 960,\n",
              " '[unused956]': 961,\n",
              " '[unused957]': 962,\n",
              " '[unused958]': 963,\n",
              " '[unused959]': 964,\n",
              " '[unused960]': 965,\n",
              " '[unused961]': 966,\n",
              " '[unused962]': 967,\n",
              " '[unused963]': 968,\n",
              " '[unused964]': 969,\n",
              " '[unused965]': 970,\n",
              " '[unused966]': 971,\n",
              " '[unused967]': 972,\n",
              " '[unused968]': 973,\n",
              " '[unused969]': 974,\n",
              " '[unused970]': 975,\n",
              " '[unused971]': 976,\n",
              " '[unused972]': 977,\n",
              " '[unused973]': 978,\n",
              " '[unused974]': 979,\n",
              " '[unused975]': 980,\n",
              " '[unused976]': 981,\n",
              " '[unused977]': 982,\n",
              " '[unused978]': 983,\n",
              " '[unused979]': 984,\n",
              " '[unused980]': 985,\n",
              " '[unused981]': 986,\n",
              " '[unused982]': 987,\n",
              " '[unused983]': 988,\n",
              " '[unused984]': 989,\n",
              " '[unused985]': 990,\n",
              " '[unused986]': 991,\n",
              " '[unused987]': 992,\n",
              " '[unused988]': 993,\n",
              " '[unused989]': 994,\n",
              " '[unused990]': 995,\n",
              " '[unused991]': 996,\n",
              " '[unused992]': 997,\n",
              " '[unused993]': 998,\n",
              " '!': 999,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 441
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer Construction\n",
        "tokenizer = Tokenizer(token_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "7-Jy3ZukLDNf",
        "outputId": "d5c75430-e8c0-427e-f46a-a6769c39cf99"
      },
      "execution_count": 442,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if the tokenizer works well\n",
        "print(tokenizer.tokenize(\"Youngsun is really awesome.\"))\n",
        "print(tokenizer.tokenize(\"We can manipulate AI.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "id": "ogXL1ScjLDK_",
        "outputId": "9f6e29b5-8eff-4b5e-8f9b-b8fab0975639"
      },
      "execution_count": 443,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'young', '##sun', 'is', 'really', 'awesome', '.', '[SEP]']\n",
            "['[CLS]', 'we', 'can', 'manipulate', 'ai', '.', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For later use, updated dictionary constructed (number - word order)\n",
        "reverse_token_dict = {v : k for k, v in token_dict.items()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "_niyf0VxLDIT",
        "outputId": "26c1137c-72a3-4827-dca8-35ca28dbccf7"
      },
      "execution_count": 444,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_token_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qo7YX6mELDF2",
        "outputId": "dd6de639-f7c0-4287-9ceb-497cd602aba2"
      },
      "execution_count": 445,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '[PAD]',\n",
              " 1: '[unused0]',\n",
              " 2: '[unused1]',\n",
              " 3: '[unused2]',\n",
              " 4: '[unused3]',\n",
              " 5: '[unused4]',\n",
              " 6: '[unused5]',\n",
              " 7: '[unused6]',\n",
              " 8: '[unused7]',\n",
              " 9: '[unused8]',\n",
              " 10: '[unused9]',\n",
              " 11: '[unused10]',\n",
              " 12: '[unused11]',\n",
              " 13: '[unused12]',\n",
              " 14: '[unused13]',\n",
              " 15: '[unused14]',\n",
              " 16: '[unused15]',\n",
              " 17: '[unused16]',\n",
              " 18: '[unused17]',\n",
              " 19: '[unused18]',\n",
              " 20: '[unused19]',\n",
              " 21: '[unused20]',\n",
              " 22: '[unused21]',\n",
              " 23: '[unused22]',\n",
              " 24: '[unused23]',\n",
              " 25: '[unused24]',\n",
              " 26: '[unused25]',\n",
              " 27: '[unused26]',\n",
              " 28: '[unused27]',\n",
              " 29: '[unused28]',\n",
              " 30: '[unused29]',\n",
              " 31: '[unused30]',\n",
              " 32: '[unused31]',\n",
              " 33: '[unused32]',\n",
              " 34: '[unused33]',\n",
              " 35: '[unused34]',\n",
              " 36: '[unused35]',\n",
              " 37: '[unused36]',\n",
              " 38: '[unused37]',\n",
              " 39: '[unused38]',\n",
              " 40: '[unused39]',\n",
              " 41: '[unused40]',\n",
              " 42: '[unused41]',\n",
              " 43: '[unused42]',\n",
              " 44: '[unused43]',\n",
              " 45: '[unused44]',\n",
              " 46: '[unused45]',\n",
              " 47: '[unused46]',\n",
              " 48: '[unused47]',\n",
              " 49: '[unused48]',\n",
              " 50: '[unused49]',\n",
              " 51: '[unused50]',\n",
              " 52: '[unused51]',\n",
              " 53: '[unused52]',\n",
              " 54: '[unused53]',\n",
              " 55: '[unused54]',\n",
              " 56: '[unused55]',\n",
              " 57: '[unused56]',\n",
              " 58: '[unused57]',\n",
              " 59: '[unused58]',\n",
              " 60: '[unused59]',\n",
              " 61: '[unused60]',\n",
              " 62: '[unused61]',\n",
              " 63: '[unused62]',\n",
              " 64: '[unused63]',\n",
              " 65: '[unused64]',\n",
              " 66: '[unused65]',\n",
              " 67: '[unused66]',\n",
              " 68: '[unused67]',\n",
              " 69: '[unused68]',\n",
              " 70: '[unused69]',\n",
              " 71: '[unused70]',\n",
              " 72: '[unused71]',\n",
              " 73: '[unused72]',\n",
              " 74: '[unused73]',\n",
              " 75: '[unused74]',\n",
              " 76: '[unused75]',\n",
              " 77: '[unused76]',\n",
              " 78: '[unused77]',\n",
              " 79: '[unused78]',\n",
              " 80: '[unused79]',\n",
              " 81: '[unused80]',\n",
              " 82: '[unused81]',\n",
              " 83: '[unused82]',\n",
              " 84: '[unused83]',\n",
              " 85: '[unused84]',\n",
              " 86: '[unused85]',\n",
              " 87: '[unused86]',\n",
              " 88: '[unused87]',\n",
              " 89: '[unused88]',\n",
              " 90: '[unused89]',\n",
              " 91: '[unused90]',\n",
              " 92: '[unused91]',\n",
              " 93: '[unused92]',\n",
              " 94: '[unused93]',\n",
              " 95: '[unused94]',\n",
              " 96: '[unused95]',\n",
              " 97: '[unused96]',\n",
              " 98: '[unused97]',\n",
              " 99: '[unused98]',\n",
              " 100: '[UNK]',\n",
              " 101: '[CLS]',\n",
              " 102: '[SEP]',\n",
              " 103: '[MASK]',\n",
              " 104: '[unused99]',\n",
              " 105: '[unused100]',\n",
              " 106: '[unused101]',\n",
              " 107: '[unused102]',\n",
              " 108: '[unused103]',\n",
              " 109: '[unused104]',\n",
              " 110: '[unused105]',\n",
              " 111: '[unused106]',\n",
              " 112: '[unused107]',\n",
              " 113: '[unused108]',\n",
              " 114: '[unused109]',\n",
              " 115: '[unused110]',\n",
              " 116: '[unused111]',\n",
              " 117: '[unused112]',\n",
              " 118: '[unused113]',\n",
              " 119: '[unused114]',\n",
              " 120: '[unused115]',\n",
              " 121: '[unused116]',\n",
              " 122: '[unused117]',\n",
              " 123: '[unused118]',\n",
              " 124: '[unused119]',\n",
              " 125: '[unused120]',\n",
              " 126: '[unused121]',\n",
              " 127: '[unused122]',\n",
              " 128: '[unused123]',\n",
              " 129: '[unused124]',\n",
              " 130: '[unused125]',\n",
              " 131: '[unused126]',\n",
              " 132: '[unused127]',\n",
              " 133: '[unused128]',\n",
              " 134: '[unused129]',\n",
              " 135: '[unused130]',\n",
              " 136: '[unused131]',\n",
              " 137: '[unused132]',\n",
              " 138: '[unused133]',\n",
              " 139: '[unused134]',\n",
              " 140: '[unused135]',\n",
              " 141: '[unused136]',\n",
              " 142: '[unused137]',\n",
              " 143: '[unused138]',\n",
              " 144: '[unused139]',\n",
              " 145: '[unused140]',\n",
              " 146: '[unused141]',\n",
              " 147: '[unused142]',\n",
              " 148: '[unused143]',\n",
              " 149: '[unused144]',\n",
              " 150: '[unused145]',\n",
              " 151: '[unused146]',\n",
              " 152: '[unused147]',\n",
              " 153: '[unused148]',\n",
              " 154: '[unused149]',\n",
              " 155: '[unused150]',\n",
              " 156: '[unused151]',\n",
              " 157: '[unused152]',\n",
              " 158: '[unused153]',\n",
              " 159: '[unused154]',\n",
              " 160: '[unused155]',\n",
              " 161: '[unused156]',\n",
              " 162: '[unused157]',\n",
              " 163: '[unused158]',\n",
              " 164: '[unused159]',\n",
              " 165: '[unused160]',\n",
              " 166: '[unused161]',\n",
              " 167: '[unused162]',\n",
              " 168: '[unused163]',\n",
              " 169: '[unused164]',\n",
              " 170: '[unused165]',\n",
              " 171: '[unused166]',\n",
              " 172: '[unused167]',\n",
              " 173: '[unused168]',\n",
              " 174: '[unused169]',\n",
              " 175: '[unused170]',\n",
              " 176: '[unused171]',\n",
              " 177: '[unused172]',\n",
              " 178: '[unused173]',\n",
              " 179: '[unused174]',\n",
              " 180: '[unused175]',\n",
              " 181: '[unused176]',\n",
              " 182: '[unused177]',\n",
              " 183: '[unused178]',\n",
              " 184: '[unused179]',\n",
              " 185: '[unused180]',\n",
              " 186: '[unused181]',\n",
              " 187: '[unused182]',\n",
              " 188: '[unused183]',\n",
              " 189: '[unused184]',\n",
              " 190: '[unused185]',\n",
              " 191: '[unused186]',\n",
              " 192: '[unused187]',\n",
              " 193: '[unused188]',\n",
              " 194: '[unused189]',\n",
              " 195: '[unused190]',\n",
              " 196: '[unused191]',\n",
              " 197: '[unused192]',\n",
              " 198: '[unused193]',\n",
              " 199: '[unused194]',\n",
              " 200: '[unused195]',\n",
              " 201: '[unused196]',\n",
              " 202: '[unused197]',\n",
              " 203: '[unused198]',\n",
              " 204: '[unused199]',\n",
              " 205: '[unused200]',\n",
              " 206: '[unused201]',\n",
              " 207: '[unused202]',\n",
              " 208: '[unused203]',\n",
              " 209: '[unused204]',\n",
              " 210: '[unused205]',\n",
              " 211: '[unused206]',\n",
              " 212: '[unused207]',\n",
              " 213: '[unused208]',\n",
              " 214: '[unused209]',\n",
              " 215: '[unused210]',\n",
              " 216: '[unused211]',\n",
              " 217: '[unused212]',\n",
              " 218: '[unused213]',\n",
              " 219: '[unused214]',\n",
              " 220: '[unused215]',\n",
              " 221: '[unused216]',\n",
              " 222: '[unused217]',\n",
              " 223: '[unused218]',\n",
              " 224: '[unused219]',\n",
              " 225: '[unused220]',\n",
              " 226: '[unused221]',\n",
              " 227: '[unused222]',\n",
              " 228: '[unused223]',\n",
              " 229: '[unused224]',\n",
              " 230: '[unused225]',\n",
              " 231: '[unused226]',\n",
              " 232: '[unused227]',\n",
              " 233: '[unused228]',\n",
              " 234: '[unused229]',\n",
              " 235: '[unused230]',\n",
              " 236: '[unused231]',\n",
              " 237: '[unused232]',\n",
              " 238: '[unused233]',\n",
              " 239: '[unused234]',\n",
              " 240: '[unused235]',\n",
              " 241: '[unused236]',\n",
              " 242: '[unused237]',\n",
              " 243: '[unused238]',\n",
              " 244: '[unused239]',\n",
              " 245: '[unused240]',\n",
              " 246: '[unused241]',\n",
              " 247: '[unused242]',\n",
              " 248: '[unused243]',\n",
              " 249: '[unused244]',\n",
              " 250: '[unused245]',\n",
              " 251: '[unused246]',\n",
              " 252: '[unused247]',\n",
              " 253: '[unused248]',\n",
              " 254: '[unused249]',\n",
              " 255: '[unused250]',\n",
              " 256: '[unused251]',\n",
              " 257: '[unused252]',\n",
              " 258: '[unused253]',\n",
              " 259: '[unused254]',\n",
              " 260: '[unused255]',\n",
              " 261: '[unused256]',\n",
              " 262: '[unused257]',\n",
              " 263: '[unused258]',\n",
              " 264: '[unused259]',\n",
              " 265: '[unused260]',\n",
              " 266: '[unused261]',\n",
              " 267: '[unused262]',\n",
              " 268: '[unused263]',\n",
              " 269: '[unused264]',\n",
              " 270: '[unused265]',\n",
              " 271: '[unused266]',\n",
              " 272: '[unused267]',\n",
              " 273: '[unused268]',\n",
              " 274: '[unused269]',\n",
              " 275: '[unused270]',\n",
              " 276: '[unused271]',\n",
              " 277: '[unused272]',\n",
              " 278: '[unused273]',\n",
              " 279: '[unused274]',\n",
              " 280: '[unused275]',\n",
              " 281: '[unused276]',\n",
              " 282: '[unused277]',\n",
              " 283: '[unused278]',\n",
              " 284: '[unused279]',\n",
              " 285: '[unused280]',\n",
              " 286: '[unused281]',\n",
              " 287: '[unused282]',\n",
              " 288: '[unused283]',\n",
              " 289: '[unused284]',\n",
              " 290: '[unused285]',\n",
              " 291: '[unused286]',\n",
              " 292: '[unused287]',\n",
              " 293: '[unused288]',\n",
              " 294: '[unused289]',\n",
              " 295: '[unused290]',\n",
              " 296: '[unused291]',\n",
              " 297: '[unused292]',\n",
              " 298: '[unused293]',\n",
              " 299: '[unused294]',\n",
              " 300: '[unused295]',\n",
              " 301: '[unused296]',\n",
              " 302: '[unused297]',\n",
              " 303: '[unused298]',\n",
              " 304: '[unused299]',\n",
              " 305: '[unused300]',\n",
              " 306: '[unused301]',\n",
              " 307: '[unused302]',\n",
              " 308: '[unused303]',\n",
              " 309: '[unused304]',\n",
              " 310: '[unused305]',\n",
              " 311: '[unused306]',\n",
              " 312: '[unused307]',\n",
              " 313: '[unused308]',\n",
              " 314: '[unused309]',\n",
              " 315: '[unused310]',\n",
              " 316: '[unused311]',\n",
              " 317: '[unused312]',\n",
              " 318: '[unused313]',\n",
              " 319: '[unused314]',\n",
              " 320: '[unused315]',\n",
              " 321: '[unused316]',\n",
              " 322: '[unused317]',\n",
              " 323: '[unused318]',\n",
              " 324: '[unused319]',\n",
              " 325: '[unused320]',\n",
              " 326: '[unused321]',\n",
              " 327: '[unused322]',\n",
              " 328: '[unused323]',\n",
              " 329: '[unused324]',\n",
              " 330: '[unused325]',\n",
              " 331: '[unused326]',\n",
              " 332: '[unused327]',\n",
              " 333: '[unused328]',\n",
              " 334: '[unused329]',\n",
              " 335: '[unused330]',\n",
              " 336: '[unused331]',\n",
              " 337: '[unused332]',\n",
              " 338: '[unused333]',\n",
              " 339: '[unused334]',\n",
              " 340: '[unused335]',\n",
              " 341: '[unused336]',\n",
              " 342: '[unused337]',\n",
              " 343: '[unused338]',\n",
              " 344: '[unused339]',\n",
              " 345: '[unused340]',\n",
              " 346: '[unused341]',\n",
              " 347: '[unused342]',\n",
              " 348: '[unused343]',\n",
              " 349: '[unused344]',\n",
              " 350: '[unused345]',\n",
              " 351: '[unused346]',\n",
              " 352: '[unused347]',\n",
              " 353: '[unused348]',\n",
              " 354: '[unused349]',\n",
              " 355: '[unused350]',\n",
              " 356: '[unused351]',\n",
              " 357: '[unused352]',\n",
              " 358: '[unused353]',\n",
              " 359: '[unused354]',\n",
              " 360: '[unused355]',\n",
              " 361: '[unused356]',\n",
              " 362: '[unused357]',\n",
              " 363: '[unused358]',\n",
              " 364: '[unused359]',\n",
              " 365: '[unused360]',\n",
              " 366: '[unused361]',\n",
              " 367: '[unused362]',\n",
              " 368: '[unused363]',\n",
              " 369: '[unused364]',\n",
              " 370: '[unused365]',\n",
              " 371: '[unused366]',\n",
              " 372: '[unused367]',\n",
              " 373: '[unused368]',\n",
              " 374: '[unused369]',\n",
              " 375: '[unused370]',\n",
              " 376: '[unused371]',\n",
              " 377: '[unused372]',\n",
              " 378: '[unused373]',\n",
              " 379: '[unused374]',\n",
              " 380: '[unused375]',\n",
              " 381: '[unused376]',\n",
              " 382: '[unused377]',\n",
              " 383: '[unused378]',\n",
              " 384: '[unused379]',\n",
              " 385: '[unused380]',\n",
              " 386: '[unused381]',\n",
              " 387: '[unused382]',\n",
              " 388: '[unused383]',\n",
              " 389: '[unused384]',\n",
              " 390: '[unused385]',\n",
              " 391: '[unused386]',\n",
              " 392: '[unused387]',\n",
              " 393: '[unused388]',\n",
              " 394: '[unused389]',\n",
              " 395: '[unused390]',\n",
              " 396: '[unused391]',\n",
              " 397: '[unused392]',\n",
              " 398: '[unused393]',\n",
              " 399: '[unused394]',\n",
              " 400: '[unused395]',\n",
              " 401: '[unused396]',\n",
              " 402: '[unused397]',\n",
              " 403: '[unused398]',\n",
              " 404: '[unused399]',\n",
              " 405: '[unused400]',\n",
              " 406: '[unused401]',\n",
              " 407: '[unused402]',\n",
              " 408: '[unused403]',\n",
              " 409: '[unused404]',\n",
              " 410: '[unused405]',\n",
              " 411: '[unused406]',\n",
              " 412: '[unused407]',\n",
              " 413: '[unused408]',\n",
              " 414: '[unused409]',\n",
              " 415: '[unused410]',\n",
              " 416: '[unused411]',\n",
              " 417: '[unused412]',\n",
              " 418: '[unused413]',\n",
              " 419: '[unused414]',\n",
              " 420: '[unused415]',\n",
              " 421: '[unused416]',\n",
              " 422: '[unused417]',\n",
              " 423: '[unused418]',\n",
              " 424: '[unused419]',\n",
              " 425: '[unused420]',\n",
              " 426: '[unused421]',\n",
              " 427: '[unused422]',\n",
              " 428: '[unused423]',\n",
              " 429: '[unused424]',\n",
              " 430: '[unused425]',\n",
              " 431: '[unused426]',\n",
              " 432: '[unused427]',\n",
              " 433: '[unused428]',\n",
              " 434: '[unused429]',\n",
              " 435: '[unused430]',\n",
              " 436: '[unused431]',\n",
              " 437: '[unused432]',\n",
              " 438: '[unused433]',\n",
              " 439: '[unused434]',\n",
              " 440: '[unused435]',\n",
              " 441: '[unused436]',\n",
              " 442: '[unused437]',\n",
              " 443: '[unused438]',\n",
              " 444: '[unused439]',\n",
              " 445: '[unused440]',\n",
              " 446: '[unused441]',\n",
              " 447: '[unused442]',\n",
              " 448: '[unused443]',\n",
              " 449: '[unused444]',\n",
              " 450: '[unused445]',\n",
              " 451: '[unused446]',\n",
              " 452: '[unused447]',\n",
              " 453: '[unused448]',\n",
              " 454: '[unused449]',\n",
              " 455: '[unused450]',\n",
              " 456: '[unused451]',\n",
              " 457: '[unused452]',\n",
              " 458: '[unused453]',\n",
              " 459: '[unused454]',\n",
              " 460: '[unused455]',\n",
              " 461: '[unused456]',\n",
              " 462: '[unused457]',\n",
              " 463: '[unused458]',\n",
              " 464: '[unused459]',\n",
              " 465: '[unused460]',\n",
              " 466: '[unused461]',\n",
              " 467: '[unused462]',\n",
              " 468: '[unused463]',\n",
              " 469: '[unused464]',\n",
              " 470: '[unused465]',\n",
              " 471: '[unused466]',\n",
              " 472: '[unused467]',\n",
              " 473: '[unused468]',\n",
              " 474: '[unused469]',\n",
              " 475: '[unused470]',\n",
              " 476: '[unused471]',\n",
              " 477: '[unused472]',\n",
              " 478: '[unused473]',\n",
              " 479: '[unused474]',\n",
              " 480: '[unused475]',\n",
              " 481: '[unused476]',\n",
              " 482: '[unused477]',\n",
              " 483: '[unused478]',\n",
              " 484: '[unused479]',\n",
              " 485: '[unused480]',\n",
              " 486: '[unused481]',\n",
              " 487: '[unused482]',\n",
              " 488: '[unused483]',\n",
              " 489: '[unused484]',\n",
              " 490: '[unused485]',\n",
              " 491: '[unused486]',\n",
              " 492: '[unused487]',\n",
              " 493: '[unused488]',\n",
              " 494: '[unused489]',\n",
              " 495: '[unused490]',\n",
              " 496: '[unused491]',\n",
              " 497: '[unused492]',\n",
              " 498: '[unused493]',\n",
              " 499: '[unused494]',\n",
              " 500: '[unused495]',\n",
              " 501: '[unused496]',\n",
              " 502: '[unused497]',\n",
              " 503: '[unused498]',\n",
              " 504: '[unused499]',\n",
              " 505: '[unused500]',\n",
              " 506: '[unused501]',\n",
              " 507: '[unused502]',\n",
              " 508: '[unused503]',\n",
              " 509: '[unused504]',\n",
              " 510: '[unused505]',\n",
              " 511: '[unused506]',\n",
              " 512: '[unused507]',\n",
              " 513: '[unused508]',\n",
              " 514: '[unused509]',\n",
              " 515: '[unused510]',\n",
              " 516: '[unused511]',\n",
              " 517: '[unused512]',\n",
              " 518: '[unused513]',\n",
              " 519: '[unused514]',\n",
              " 520: '[unused515]',\n",
              " 521: '[unused516]',\n",
              " 522: '[unused517]',\n",
              " 523: '[unused518]',\n",
              " 524: '[unused519]',\n",
              " 525: '[unused520]',\n",
              " 526: '[unused521]',\n",
              " 527: '[unused522]',\n",
              " 528: '[unused523]',\n",
              " 529: '[unused524]',\n",
              " 530: '[unused525]',\n",
              " 531: '[unused526]',\n",
              " 532: '[unused527]',\n",
              " 533: '[unused528]',\n",
              " 534: '[unused529]',\n",
              " 535: '[unused530]',\n",
              " 536: '[unused531]',\n",
              " 537: '[unused532]',\n",
              " 538: '[unused533]',\n",
              " 539: '[unused534]',\n",
              " 540: '[unused535]',\n",
              " 541: '[unused536]',\n",
              " 542: '[unused537]',\n",
              " 543: '[unused538]',\n",
              " 544: '[unused539]',\n",
              " 545: '[unused540]',\n",
              " 546: '[unused541]',\n",
              " 547: '[unused542]',\n",
              " 548: '[unused543]',\n",
              " 549: '[unused544]',\n",
              " 550: '[unused545]',\n",
              " 551: '[unused546]',\n",
              " 552: '[unused547]',\n",
              " 553: '[unused548]',\n",
              " 554: '[unused549]',\n",
              " 555: '[unused550]',\n",
              " 556: '[unused551]',\n",
              " 557: '[unused552]',\n",
              " 558: '[unused553]',\n",
              " 559: '[unused554]',\n",
              " 560: '[unused555]',\n",
              " 561: '[unused556]',\n",
              " 562: '[unused557]',\n",
              " 563: '[unused558]',\n",
              " 564: '[unused559]',\n",
              " 565: '[unused560]',\n",
              " 566: '[unused561]',\n",
              " 567: '[unused562]',\n",
              " 568: '[unused563]',\n",
              " 569: '[unused564]',\n",
              " 570: '[unused565]',\n",
              " 571: '[unused566]',\n",
              " 572: '[unused567]',\n",
              " 573: '[unused568]',\n",
              " 574: '[unused569]',\n",
              " 575: '[unused570]',\n",
              " 576: '[unused571]',\n",
              " 577: '[unused572]',\n",
              " 578: '[unused573]',\n",
              " 579: '[unused574]',\n",
              " 580: '[unused575]',\n",
              " 581: '[unused576]',\n",
              " 582: '[unused577]',\n",
              " 583: '[unused578]',\n",
              " 584: '[unused579]',\n",
              " 585: '[unused580]',\n",
              " 586: '[unused581]',\n",
              " 587: '[unused582]',\n",
              " 588: '[unused583]',\n",
              " 589: '[unused584]',\n",
              " 590: '[unused585]',\n",
              " 591: '[unused586]',\n",
              " 592: '[unused587]',\n",
              " 593: '[unused588]',\n",
              " 594: '[unused589]',\n",
              " 595: '[unused590]',\n",
              " 596: '[unused591]',\n",
              " 597: '[unused592]',\n",
              " 598: '[unused593]',\n",
              " 599: '[unused594]',\n",
              " 600: '[unused595]',\n",
              " 601: '[unused596]',\n",
              " 602: '[unused597]',\n",
              " 603: '[unused598]',\n",
              " 604: '[unused599]',\n",
              " 605: '[unused600]',\n",
              " 606: '[unused601]',\n",
              " 607: '[unused602]',\n",
              " 608: '[unused603]',\n",
              " 609: '[unused604]',\n",
              " 610: '[unused605]',\n",
              " 611: '[unused606]',\n",
              " 612: '[unused607]',\n",
              " 613: '[unused608]',\n",
              " 614: '[unused609]',\n",
              " 615: '[unused610]',\n",
              " 616: '[unused611]',\n",
              " 617: '[unused612]',\n",
              " 618: '[unused613]',\n",
              " 619: '[unused614]',\n",
              " 620: '[unused615]',\n",
              " 621: '[unused616]',\n",
              " 622: '[unused617]',\n",
              " 623: '[unused618]',\n",
              " 624: '[unused619]',\n",
              " 625: '[unused620]',\n",
              " 626: '[unused621]',\n",
              " 627: '[unused622]',\n",
              " 628: '[unused623]',\n",
              " 629: '[unused624]',\n",
              " 630: '[unused625]',\n",
              " 631: '[unused626]',\n",
              " 632: '[unused627]',\n",
              " 633: '[unused628]',\n",
              " 634: '[unused629]',\n",
              " 635: '[unused630]',\n",
              " 636: '[unused631]',\n",
              " 637: '[unused632]',\n",
              " 638: '[unused633]',\n",
              " 639: '[unused634]',\n",
              " 640: '[unused635]',\n",
              " 641: '[unused636]',\n",
              " 642: '[unused637]',\n",
              " 643: '[unused638]',\n",
              " 644: '[unused639]',\n",
              " 645: '[unused640]',\n",
              " 646: '[unused641]',\n",
              " 647: '[unused642]',\n",
              " 648: '[unused643]',\n",
              " 649: '[unused644]',\n",
              " 650: '[unused645]',\n",
              " 651: '[unused646]',\n",
              " 652: '[unused647]',\n",
              " 653: '[unused648]',\n",
              " 654: '[unused649]',\n",
              " 655: '[unused650]',\n",
              " 656: '[unused651]',\n",
              " 657: '[unused652]',\n",
              " 658: '[unused653]',\n",
              " 659: '[unused654]',\n",
              " 660: '[unused655]',\n",
              " 661: '[unused656]',\n",
              " 662: '[unused657]',\n",
              " 663: '[unused658]',\n",
              " 664: '[unused659]',\n",
              " 665: '[unused660]',\n",
              " 666: '[unused661]',\n",
              " 667: '[unused662]',\n",
              " 668: '[unused663]',\n",
              " 669: '[unused664]',\n",
              " 670: '[unused665]',\n",
              " 671: '[unused666]',\n",
              " 672: '[unused667]',\n",
              " 673: '[unused668]',\n",
              " 674: '[unused669]',\n",
              " 675: '[unused670]',\n",
              " 676: '[unused671]',\n",
              " 677: '[unused672]',\n",
              " 678: '[unused673]',\n",
              " 679: '[unused674]',\n",
              " 680: '[unused675]',\n",
              " 681: '[unused676]',\n",
              " 682: '[unused677]',\n",
              " 683: '[unused678]',\n",
              " 684: '[unused679]',\n",
              " 685: '[unused680]',\n",
              " 686: '[unused681]',\n",
              " 687: '[unused682]',\n",
              " 688: '[unused683]',\n",
              " 689: '[unused684]',\n",
              " 690: '[unused685]',\n",
              " 691: '[unused686]',\n",
              " 692: '[unused687]',\n",
              " 693: '[unused688]',\n",
              " 694: '[unused689]',\n",
              " 695: '[unused690]',\n",
              " 696: '[unused691]',\n",
              " 697: '[unused692]',\n",
              " 698: '[unused693]',\n",
              " 699: '[unused694]',\n",
              " 700: '[unused695]',\n",
              " 701: '[unused696]',\n",
              " 702: '[unused697]',\n",
              " 703: '[unused698]',\n",
              " 704: '[unused699]',\n",
              " 705: '[unused700]',\n",
              " 706: '[unused701]',\n",
              " 707: '[unused702]',\n",
              " 708: '[unused703]',\n",
              " 709: '[unused704]',\n",
              " 710: '[unused705]',\n",
              " 711: '[unused706]',\n",
              " 712: '[unused707]',\n",
              " 713: '[unused708]',\n",
              " 714: '[unused709]',\n",
              " 715: '[unused710]',\n",
              " 716: '[unused711]',\n",
              " 717: '[unused712]',\n",
              " 718: '[unused713]',\n",
              " 719: '[unused714]',\n",
              " 720: '[unused715]',\n",
              " 721: '[unused716]',\n",
              " 722: '[unused717]',\n",
              " 723: '[unused718]',\n",
              " 724: '[unused719]',\n",
              " 725: '[unused720]',\n",
              " 726: '[unused721]',\n",
              " 727: '[unused722]',\n",
              " 728: '[unused723]',\n",
              " 729: '[unused724]',\n",
              " 730: '[unused725]',\n",
              " 731: '[unused726]',\n",
              " 732: '[unused727]',\n",
              " 733: '[unused728]',\n",
              " 734: '[unused729]',\n",
              " 735: '[unused730]',\n",
              " 736: '[unused731]',\n",
              " 737: '[unused732]',\n",
              " 738: '[unused733]',\n",
              " 739: '[unused734]',\n",
              " 740: '[unused735]',\n",
              " 741: '[unused736]',\n",
              " 742: '[unused737]',\n",
              " 743: '[unused738]',\n",
              " 744: '[unused739]',\n",
              " 745: '[unused740]',\n",
              " 746: '[unused741]',\n",
              " 747: '[unused742]',\n",
              " 748: '[unused743]',\n",
              " 749: '[unused744]',\n",
              " 750: '[unused745]',\n",
              " 751: '[unused746]',\n",
              " 752: '[unused747]',\n",
              " 753: '[unused748]',\n",
              " 754: '[unused749]',\n",
              " 755: '[unused750]',\n",
              " 756: '[unused751]',\n",
              " 757: '[unused752]',\n",
              " 758: '[unused753]',\n",
              " 759: '[unused754]',\n",
              " 760: '[unused755]',\n",
              " 761: '[unused756]',\n",
              " 762: '[unused757]',\n",
              " 763: '[unused758]',\n",
              " 764: '[unused759]',\n",
              " 765: '[unused760]',\n",
              " 766: '[unused761]',\n",
              " 767: '[unused762]',\n",
              " 768: '[unused763]',\n",
              " 769: '[unused764]',\n",
              " 770: '[unused765]',\n",
              " 771: '[unused766]',\n",
              " 772: '[unused767]',\n",
              " 773: '[unused768]',\n",
              " 774: '[unused769]',\n",
              " 775: '[unused770]',\n",
              " 776: '[unused771]',\n",
              " 777: '[unused772]',\n",
              " 778: '[unused773]',\n",
              " 779: '[unused774]',\n",
              " 780: '[unused775]',\n",
              " 781: '[unused776]',\n",
              " 782: '[unused777]',\n",
              " 783: '[unused778]',\n",
              " 784: '[unused779]',\n",
              " 785: '[unused780]',\n",
              " 786: '[unused781]',\n",
              " 787: '[unused782]',\n",
              " 788: '[unused783]',\n",
              " 789: '[unused784]',\n",
              " 790: '[unused785]',\n",
              " 791: '[unused786]',\n",
              " 792: '[unused787]',\n",
              " 793: '[unused788]',\n",
              " 794: '[unused789]',\n",
              " 795: '[unused790]',\n",
              " 796: '[unused791]',\n",
              " 797: '[unused792]',\n",
              " 798: '[unused793]',\n",
              " 799: '[unused794]',\n",
              " 800: '[unused795]',\n",
              " 801: '[unused796]',\n",
              " 802: '[unused797]',\n",
              " 803: '[unused798]',\n",
              " 804: '[unused799]',\n",
              " 805: '[unused800]',\n",
              " 806: '[unused801]',\n",
              " 807: '[unused802]',\n",
              " 808: '[unused803]',\n",
              " 809: '[unused804]',\n",
              " 810: '[unused805]',\n",
              " 811: '[unused806]',\n",
              " 812: '[unused807]',\n",
              " 813: '[unused808]',\n",
              " 814: '[unused809]',\n",
              " 815: '[unused810]',\n",
              " 816: '[unused811]',\n",
              " 817: '[unused812]',\n",
              " 818: '[unused813]',\n",
              " 819: '[unused814]',\n",
              " 820: '[unused815]',\n",
              " 821: '[unused816]',\n",
              " 822: '[unused817]',\n",
              " 823: '[unused818]',\n",
              " 824: '[unused819]',\n",
              " 825: '[unused820]',\n",
              " 826: '[unused821]',\n",
              " 827: '[unused822]',\n",
              " 828: '[unused823]',\n",
              " 829: '[unused824]',\n",
              " 830: '[unused825]',\n",
              " 831: '[unused826]',\n",
              " 832: '[unused827]',\n",
              " 833: '[unused828]',\n",
              " 834: '[unused829]',\n",
              " 835: '[unused830]',\n",
              " 836: '[unused831]',\n",
              " 837: '[unused832]',\n",
              " 838: '[unused833]',\n",
              " 839: '[unused834]',\n",
              " 840: '[unused835]',\n",
              " 841: '[unused836]',\n",
              " 842: '[unused837]',\n",
              " 843: '[unused838]',\n",
              " 844: '[unused839]',\n",
              " 845: '[unused840]',\n",
              " 846: '[unused841]',\n",
              " 847: '[unused842]',\n",
              " 848: '[unused843]',\n",
              " 849: '[unused844]',\n",
              " 850: '[unused845]',\n",
              " 851: '[unused846]',\n",
              " 852: '[unused847]',\n",
              " 853: '[unused848]',\n",
              " 854: '[unused849]',\n",
              " 855: '[unused850]',\n",
              " 856: '[unused851]',\n",
              " 857: '[unused852]',\n",
              " 858: '[unused853]',\n",
              " 859: '[unused854]',\n",
              " 860: '[unused855]',\n",
              " 861: '[unused856]',\n",
              " 862: '[unused857]',\n",
              " 863: '[unused858]',\n",
              " 864: '[unused859]',\n",
              " 865: '[unused860]',\n",
              " 866: '[unused861]',\n",
              " 867: '[unused862]',\n",
              " 868: '[unused863]',\n",
              " 869: '[unused864]',\n",
              " 870: '[unused865]',\n",
              " 871: '[unused866]',\n",
              " 872: '[unused867]',\n",
              " 873: '[unused868]',\n",
              " 874: '[unused869]',\n",
              " 875: '[unused870]',\n",
              " 876: '[unused871]',\n",
              " 877: '[unused872]',\n",
              " 878: '[unused873]',\n",
              " 879: '[unused874]',\n",
              " 880: '[unused875]',\n",
              " 881: '[unused876]',\n",
              " 882: '[unused877]',\n",
              " 883: '[unused878]',\n",
              " 884: '[unused879]',\n",
              " 885: '[unused880]',\n",
              " 886: '[unused881]',\n",
              " 887: '[unused882]',\n",
              " 888: '[unused883]',\n",
              " 889: '[unused884]',\n",
              " 890: '[unused885]',\n",
              " 891: '[unused886]',\n",
              " 892: '[unused887]',\n",
              " 893: '[unused888]',\n",
              " 894: '[unused889]',\n",
              " 895: '[unused890]',\n",
              " 896: '[unused891]',\n",
              " 897: '[unused892]',\n",
              " 898: '[unused893]',\n",
              " 899: '[unused894]',\n",
              " 900: '[unused895]',\n",
              " 901: '[unused896]',\n",
              " 902: '[unused897]',\n",
              " 903: '[unused898]',\n",
              " 904: '[unused899]',\n",
              " 905: '[unused900]',\n",
              " 906: '[unused901]',\n",
              " 907: '[unused902]',\n",
              " 908: '[unused903]',\n",
              " 909: '[unused904]',\n",
              " 910: '[unused905]',\n",
              " 911: '[unused906]',\n",
              " 912: '[unused907]',\n",
              " 913: '[unused908]',\n",
              " 914: '[unused909]',\n",
              " 915: '[unused910]',\n",
              " 916: '[unused911]',\n",
              " 917: '[unused912]',\n",
              " 918: '[unused913]',\n",
              " 919: '[unused914]',\n",
              " 920: '[unused915]',\n",
              " 921: '[unused916]',\n",
              " 922: '[unused917]',\n",
              " 923: '[unused918]',\n",
              " 924: '[unused919]',\n",
              " 925: '[unused920]',\n",
              " 926: '[unused921]',\n",
              " 927: '[unused922]',\n",
              " 928: '[unused923]',\n",
              " 929: '[unused924]',\n",
              " 930: '[unused925]',\n",
              " 931: '[unused926]',\n",
              " 932: '[unused927]',\n",
              " 933: '[unused928]',\n",
              " 934: '[unused929]',\n",
              " 935: '[unused930]',\n",
              " 936: '[unused931]',\n",
              " 937: '[unused932]',\n",
              " 938: '[unused933]',\n",
              " 939: '[unused934]',\n",
              " 940: '[unused935]',\n",
              " 941: '[unused936]',\n",
              " 942: '[unused937]',\n",
              " 943: '[unused938]',\n",
              " 944: '[unused939]',\n",
              " 945: '[unused940]',\n",
              " 946: '[unused941]',\n",
              " 947: '[unused942]',\n",
              " 948: '[unused943]',\n",
              " 949: '[unused944]',\n",
              " 950: '[unused945]',\n",
              " 951: '[unused946]',\n",
              " 952: '[unused947]',\n",
              " 953: '[unused948]',\n",
              " 954: '[unused949]',\n",
              " 955: '[unused950]',\n",
              " 956: '[unused951]',\n",
              " 957: '[unused952]',\n",
              " 958: '[unused953]',\n",
              " 959: '[unused954]',\n",
              " 960: '[unused955]',\n",
              " 961: '[unused956]',\n",
              " 962: '[unused957]',\n",
              " 963: '[unused958]',\n",
              " 964: '[unused959]',\n",
              " 965: '[unused960]',\n",
              " 966: '[unused961]',\n",
              " 967: '[unused962]',\n",
              " 968: '[unused963]',\n",
              " 969: '[unused964]',\n",
              " 970: '[unused965]',\n",
              " 971: '[unused966]',\n",
              " 972: '[unused967]',\n",
              " 973: '[unused968]',\n",
              " 974: '[unused969]',\n",
              " 975: '[unused970]',\n",
              " 976: '[unused971]',\n",
              " 977: '[unused972]',\n",
              " 978: '[unused973]',\n",
              " 979: '[unused974]',\n",
              " 980: '[unused975]',\n",
              " 981: '[unused976]',\n",
              " 982: '[unused977]',\n",
              " 983: '[unused978]',\n",
              " 984: '[unused979]',\n",
              " 985: '[unused980]',\n",
              " 986: '[unused981]',\n",
              " 987: '[unused982]',\n",
              " 988: '[unused983]',\n",
              " 989: '[unused984]',\n",
              " 990: '[unused985]',\n",
              " 991: '[unused986]',\n",
              " 992: '[unused987]',\n",
              " 993: '[unused988]',\n",
              " 994: '[unused989]',\n",
              " 995: '[unused990]',\n",
              " 996: '[unused991]',\n",
              " 997: '[unused992]',\n",
              " 998: '[unused993]',\n",
              " 999: '!',\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 445
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if the tokenizer works well again in QnA format ([CLS} Question [SEP] Context [SEP] order)\n",
        "print(tokenizer.tokenize(\"Where does Youngsun live in now?\", \"My name is Youngsun Jang, and I live in US now.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "id": "Gp2yFOGgLDDU",
        "outputId": "d2a744c2-a89d-419d-f0a0-9934b756a0c0"
      },
      "execution_count": 446,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'where', 'does', 'young', '##sun', 'live', 'in', 'now', '?', '[SEP]', 'my', 'name', 'is', 'young', '##sun', 'jang', ',', 'and', 'i', 'live', 'in', 'us', 'now', '.', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Embedding (Numberical representation)\n",
        "print(tokenizer.encode(\"Where does Youngsun live in now?\", \"My name is Youngsun Jang, and I live in US now.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "CjOItcBWLDAt",
        "outputId": "d73eebbb-d10f-4123-bfcb-0115a580131f"
      },
      "execution_count": 447,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([101, 2073, 2515, 2402, 19729, 2444, 1999, 2085, 1029, 102, 2026, 2171, 2003, 2402, 19729, 23769, 1010, 1998, 1045, 2444, 1999, 2149, 2085, 1012, 102], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fine_train.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "1UqLrciCLKN9",
        "outputId": "443ebbe9-321d-44aa-b54f-8fd566a53d1c"
      },
      "execution_count": 448,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-827b90c6-e223-434a-979e-36edf29dda99\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Index</th>\n",
              "      <th>Paper</th>\n",
              "      <th>Category</th>\n",
              "      <th>Question</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Experimental Studies of Brain Tumor Developmen...</td>\n",
              "      <td>Research Subject</td>\n",
              "      <td>What animal has been used?</td>\n",
              "      <td>It has been suggested that electromagnetic fie...</td>\n",
              "      <td>Fischer 344 rats</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Experimental Studies of Brain Tumor Developmen...</td>\n",
              "      <td>Number of Research Subject</td>\n",
              "      <td>How many animals were used?</td>\n",
              "      <td>It has been suggested that electromagnetic fie...</td>\n",
              "      <td>37 experimental rats and 37 matched controls</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Experimental Studies of Brain Tumor Developmen...</td>\n",
              "      <td>Radio Frequency</td>\n",
              "      <td>What is the signal frequency?</td>\n",
              "      <td>It has been suggested that electromagnetic fie...</td>\n",
              "      <td>915 MHz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>Experimental Studies of Brain Tumor Developmen...</td>\n",
              "      <td>Other Units of Exposure Level</td>\n",
              "      <td>How much W/kg was used?</td>\n",
              "      <td>It has been suggested that electromagnetic fie...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>Chronic Exposure of Cancer-Prone Mice to Low-L...</td>\n",
              "      <td>Research Subject</td>\n",
              "      <td>What animal has been used?</td>\n",
              "      <td>The purpose of this study was to determine whe...</td>\n",
              "      <td>C3H/ HeJ mice</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-827b90c6-e223-434a-979e-36edf29dda99')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-827b90c6-e223-434a-979e-36edf29dda99 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-827b90c6-e223-434a-979e-36edf29dda99');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Index  ...                                        Answer\n",
              "0      1  ...                              Fischer 344 rats\n",
              "1      1  ...  37 experimental rats and 37 matched controls\n",
              "2      1  ...                                       915 MHz\n",
              "3      1  ...                                           NaN\n",
              "4      3  ...                                 C3H/ HeJ mice\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 448
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Question: \", fine_train.loc[0, 'Question'])\n",
        "print(\"Context: \", fine_train.loc[0, 'Abstract'])\n",
        "print(\"Answer: \", fine_train.loc[0, 'Answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "xIcG0IMXLKLm",
        "outputId": "9438e2bc-1d5b-4cf4-83e2-c7e88222e4f9"
      },
      "execution_count": 449,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:  What animal has been used?\n",
            "Context:  It has been suggested that electromagnetic fields (EMFs) act as a promoter late in the carcinogenesis process. To date, however, noconvincing laboratory evidence has been obtained indicating that EMFs cause tumour promotion at non-thermal exposure levels. The effects of EMF exposure in a rat brain glioma model were investigated. The exposure consisted of 915 MHz microwaves, both as continuous waves (1 W), and modulated with 4, 8, 16 and 200 Hz in 0.5 ms pulses and 50 Hz in 6 ms pulses (2 W per pulse). Fischer 344 rats of both sexes, weighing 150–250 g, were used in the experiments. 5000 RG2 cells in 5 μ1 nutrient solution were injected by the stereotaxic technique into the head of the right caudate nucleus in 37 experimental rats and 37 matched controls. The exposed animals were kept unanaesthetized in well ventilated transverse electromagnetic (TEM) cells producing 915 MHz continuous or modulated microwaves. Exposure was started on day five after inoculation. The animals were exposed for 7 hd d−1 for 5 d per week during two to three weeks. The controls were kept in an identical TEM cell without EMF exposure. All brains were examined histopathologically and the tumour size was determined. Our study does not show a significant difference in tumour size between animals exposed to 915 MHz microwaves, and those not exposed. Our preliminary results do not support that even an extensive daily exposure to EMF promotes tumour growth when given from the fifth day after the start of tumour growth in the rat brain until the death of the animal which by then has a large brain tumour. Further studies with higher specific absorption rate levels are in progress.\n",
            "Answer:  Fischer 344 rats\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QA is to confirm the position of the Answer token in Context \n",
        "# position of Saint Bernadette Soubirous !\n",
        "# That is, from 3002 - 2271\n",
        "\n",
        "print(\"Question: \", tokenizer.tokenize(fine_train.loc[0, 'Question']))\n",
        "print(\"Context: \", tokenizer.tokenize(fine_train.loc[0, 'Abstract']))\n",
        "print(\"Answer: \", tokenizer.tokenize(fine_train.loc[0, 'Answer']))\n",
        "\n",
        "print(\"Question: \", tokenizer.encode(fine_train.loc[0, 'Question']))\n",
        "print(\"Context: \", tokenizer.encode(fine_train.loc[0, 'Abstract']))\n",
        "print(\"Answer: \", tokenizer.encode(fine_train.loc[0, 'Answer']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "o2SFrtVKLKJM",
        "outputId": "0f016de2-6a24-4d67-baff-f59537341cb5"
      },
      "execution_count": 450,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:  ['[CLS]', 'what', 'animal', 'has', 'been', 'used', '?', '[SEP]']\n",
            "Context:  ['[CLS]', 'it', 'has', 'been', 'suggested', 'that', 'electromagnetic', 'fields', '(', 'em', '##fs', ')', 'act', 'as', 'a', 'promoter', 'late', 'in', 'the', 'car', '##cino', '##genesis', 'process', '.', 'to', 'date', ',', 'however', ',', 'no', '##con', '##vin', '##cing', 'laboratory', 'evidence', 'has', 'been', 'obtained', 'indicating', 'that', 'em', '##fs', 'cause', 'tu', '##mour', 'promotion', 'at', 'non', '-', 'thermal', 'exposure', 'levels', '.', 'the', 'effects', 'of', 'em', '##f', 'exposure', 'in', 'a', 'rat', 'brain', 'g', '##lio', '##ma', 'model', 'were', 'investigated', '.', 'the', 'exposure', 'consisted', 'of', '91', '##5', 'mhz', 'microwave', '##s', ',', 'both', 'as', 'continuous', 'waves', '(', '1', 'w', ')', ',', 'and', 'mod', '##ulated', 'with', '4', ',', '8', ',', '16', 'and', '200', 'hz', 'in', '0', '.', '5', 'ms', 'pulses', 'and', '50', 'hz', 'in', '6', 'ms', 'pulses', '(', '2', 'w', 'per', 'pulse', ')', '.', 'fischer', '344', 'rats', 'of', 'both', 'sexes', ',', 'weighing', '150', '–', '250', 'g', ',', 'were', 'used', 'in', 'the', 'experiments', '.', '5000', 'r', '##g', '##2', 'cells', 'in', '5', 'μ', '##1', 'nutrient', 'solution', 'were', 'injected', 'by', 'the', 'stereo', '##ta', '##xi', '##c', 'technique', 'into', 'the', 'head', 'of', 'the', 'right', 'ca', '##uda', '##te', 'nucleus', 'in', '37', 'experimental', 'rats', 'and', '37', 'matched', 'controls', '.', 'the', 'exposed', 'animals', 'were', 'kept', 'una', '##nae', '##st', '##het', '##ized', 'in', 'well', 'vent', '##ila', '##ted', 'transverse', 'electromagnetic', '(', 'te', '##m', ')', 'cells', 'producing', '91', '##5', 'mhz', 'continuous', 'or', 'mod', '##ulated', 'microwave', '##s', '.', 'exposure', 'was', 'started', 'on', 'day', 'five', 'after', 'in', '##oc', '##ulation', '.', 'the', 'animals', 'were', 'exposed', 'for', '7', 'hd', 'd', '##−1', 'for', '5', 'd', 'per', 'week', 'during', 'two', 'to', 'three', 'weeks', '.', 'the', 'controls', 'were', 'kept', 'in', 'an', 'identical', 'te', '##m', 'cell', 'without', 'em', '##f', 'exposure', '.', 'all', 'brains', 'were', 'examined', 'his', '##top', '##ath', '##ological', '##ly', 'and', 'the', 'tu', '##mour', 'size', 'was', 'determined', '.', 'our', 'study', 'does', 'not', 'show', 'a', 'significant', 'difference', 'in', 'tu', '##mour', 'size', 'between', 'animals', 'exposed', 'to', '91', '##5', 'mhz', 'microwave', '##s', ',', 'and', 'those', 'not', 'exposed', '.', 'our', 'preliminary', 'results', 'do', 'not', 'support', 'that', 'even', 'an', 'extensive', 'daily', 'exposure', 'to', 'em', '##f', 'promotes', 'tu', '##mour', 'growth', 'when', 'given', 'from', 'the', 'fifth', 'day', 'after', 'the', 'start', 'of', 'tu', '##mour', 'growth', 'in', 'the', 'rat', 'brain', 'until', 'the', 'death', 'of', 'the', 'animal', 'which', 'by', 'then', 'has', 'a', 'large', 'brain', 'tu', '##mour', '.', 'further', 'studies', 'with', 'higher', 'specific', 'absorption', 'rate', 'levels', 'are', 'in', 'progress', '.', '[SEP]']\n",
            "Answer:  ['[CLS]', 'fischer', '344', 'rats', '[SEP]']\n",
            "Question:  ([101, 2054, 4111, 2038, 2042, 2109, 1029, 102], [0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Context:  ([101, 2009, 2038, 2042, 4081, 2008, 17225, 4249, 1006, 7861, 10343, 1007, 2552, 2004, 1037, 15543, 2397, 1999, 1996, 2482, 21081, 23737, 2832, 1012, 2000, 3058, 1010, 2174, 1010, 2053, 8663, 6371, 6129, 5911, 3350, 2038, 2042, 4663, 8131, 2008, 7861, 10343, 3426, 10722, 20360, 4712, 2012, 2512, 1011, 9829, 7524, 3798, 1012, 1996, 3896, 1997, 7861, 2546, 7524, 1999, 1037, 9350, 4167, 1043, 12798, 2863, 2944, 2020, 10847, 1012, 1996, 7524, 5031, 1997, 6205, 2629, 11413, 18302, 2015, 1010, 2119, 2004, 7142, 5975, 1006, 1015, 1059, 1007, 1010, 1998, 16913, 8898, 2007, 1018, 1010, 1022, 1010, 2385, 1998, 3263, 22100, 1999, 1014, 1012, 1019, 5796, 23894, 1998, 2753, 22100, 1999, 1020, 5796, 23894, 1006, 1016, 1059, 2566, 8187, 1007, 1012, 13042, 29386, 11432, 1997, 2119, 21024, 1010, 15243, 5018, 1516, 5539, 1043, 1010, 2020, 2109, 1999, 1996, 7885, 1012, 13509, 1054, 2290, 2475, 4442, 1999, 1019, 1166, 2487, 26780, 5576, 2020, 19737, 2011, 1996, 12991, 2696, 9048, 2278, 6028, 2046, 1996, 2132, 1997, 1996, 2157, 6187, 14066, 2618, 13502, 1999, 4261, 6388, 11432, 1998, 4261, 10349, 7711, 1012, 1996, 6086, 4176, 2020, 2921, 14477, 17452, 3367, 27065, 3550, 1999, 2092, 18834, 11733, 3064, 18323, 17225, 1006, 8915, 2213, 1007, 4442, 5155, 6205, 2629, 11413, 7142, 2030, 16913, 8898, 18302, 2015, 1012, 7524, 2001, 2318, 2006, 2154, 2274, 2044, 1999, 10085, 9513, 1012, 1996, 4176, 2020, 6086, 2005, 1021, 10751, 1040, 27944, 2005, 1019, 1040, 2566, 2733, 2076, 2048, 2000, 2093, 3134, 1012, 1996, 7711, 2020, 2921, 1999, 2019, 7235, 8915, 2213, 3526, 2302, 7861, 2546, 7524, 1012, 2035, 14332, 2020, 8920, 2010, 14399, 8988, 10091, 2135, 1998, 1996, 10722, 20360, 2946, 2001, 4340, 1012, 2256, 2817, 2515, 2025, 2265, 1037, 3278, 4489, 1999, 10722, 20360, 2946, 2090, 4176, 6086, 2000, 6205, 2629, 11413, 18302, 2015, 1010, 1998, 2216, 2025, 6086, 1012, 2256, 8824, 3463, 2079, 2025, 2490, 2008, 2130, 2019, 4866, 3679, 7524, 2000, 7861, 2546, 14067, 10722, 20360, 3930, 2043, 2445, 2013, 1996, 3587, 2154, 2044, 1996, 2707, 1997, 10722, 20360, 3930, 1999, 1996, 9350, 4167, 2127, 1996, 2331, 1997, 1996, 4111, 2029, 2011, 2059, 2038, 1037, 2312, 4167, 10722, 20360, 1012, 2582, 2913, 2007, 3020, 3563, 16326, 3446, 3798, 2024, 1999, 5082, 1012, 102], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Answer:  ([101, 13042, 29386, 11432, 102], [0, 0, 0, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting only token part excepting the segment part\n",
        "fine_context = tokenizer.encode(fine_train.loc[0, 'Abstract'])[0]\n",
        "fine_text = tokenizer.encode(fine_train.loc[0, 'Answer'])[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "g1N1d0hULKGv",
        "outputId": "6c2575ff-6aff-49ff-c70f-c396e9472dc0"
      },
      "execution_count": 451,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CLS, SEP token delete\n",
        "fine_text.pop(0)\n",
        "fine_text.pop(-1)\n",
        "print(fine_text)\n",
        "print(len(fine_text))\n",
        "len(fine_context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "xxGn3nYwLKEV",
        "outputId": "cd3d9187-9d46-4e59-fa78-283d911824a2"
      },
      "execution_count": 452,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13042, 29386, 11432]\n",
            "3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "367"
            ]
          },
          "metadata": {},
          "execution_count": 452
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To calculate the length of text (answer)\n",
        "# Method : Doing 'Sliding' the context with the length of the answer\n",
        "fine_text_slide_len = len(fine_text) # text_slide_len = 8\n",
        "\n",
        "for j in range(0, (len(fine_context))):\n",
        "  # exist_flag : showing whether it is answerable or not (similar with is_unanswerable in SimpleTransforemr)\n",
        "  # 0 : No answer / 1 : Having answer\n",
        "  exist_flag = 0 \n",
        "  if fine_text == fine_context[j:j+fine_text_slide_len]: # [0:8]->[1:9]->[2:10]->..->[159:160]\n",
        "    # Assign the location of answer (start, end)\n",
        "    fine_ans_start = j\n",
        "    fine_ans_end = j + fine_text_slide_len - 1\n",
        "    # If matched, exist_flag changed to 1\n",
        "    exist_flag = 1\n",
        "    break\n",
        "\n",
        "print(\"ans_start : {}, ans_end : {}\".format(fine_ans_start, fine_ans_end))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "Vf-JRUiJLKB5",
        "outputId": "a9386d6a-6710-4732-da89-372767b4f44c"
      },
      "execution_count": 453,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ans_start : 121, ans_end : 123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check\n",
        "# context[ans_start:ans_end]\n",
        "print(fine_context[fine_ans_start:fine_ans_end+1], fine_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "6oNGiPNfLJ_g",
        "outputId": "5fc71292-d3f0-40bd-c98c-a3b662590f68"
      },
      "execution_count": 454,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13042, 29386, 11432] [13042, 29386, 11432]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function calculating the length of answer in context dataset all at once\n",
        "def fine_convert_data(data_df):\n",
        "  global tokenizer\n",
        "  indices, segments, target_start, target_end = [], [], [], []\n",
        "\n",
        "  for i in tqdm(range(len(data_df))):\n",
        "    # que : List of tokenized question\n",
        "    que, _ = tokenizer.encode(data_df[FINE_QUESTION_COLUMN][i])\n",
        "    # doc : List of tokenized context\n",
        "    doc, _ = tokenizer.encode(data_df[FINE_DATA_COLUMN][i])\n",
        "\n",
        "    # [CLS] token deleted in context\n",
        "    doc.pop(0)\n",
        "\n",
        "    # Length of question & context\n",
        "    que_len = len(que)\n",
        "    doc_len = len(doc)\n",
        "    # 1. Length of question\n",
        "    # The question is cut by the length of 64\n",
        "    if que_len > 64:\n",
        "      que = que[:63]\n",
        "      que.append(102) # [SEP] token added to make it clear the question block\n",
        "    # 2. Total length of question and context\n",
        "    # The total input is cut by the length of 384\n",
        "    if len(que+doc) > SEQ_LEN:\n",
        "      while len(que+doc) != SEQ_LEN:\n",
        "        doc.pop(-1)\n",
        "      doc.pop(-1)\n",
        "      doc.append(102)\n",
        "\n",
        "    # Segment embedding\n",
        "    # Question : 0 / Context 1 / Padding : 0 (remaining part for short sentences)\n",
        "        \n",
        "    ############################\n",
        "    ###### Segment 예시 ########\n",
        "    ############################\n",
        "    \n",
        "    # question, context, padding\n",
        "    # 00000000, 1111111, 0000000\n",
        "    \n",
        "    segment = [0]*len(que) + [1]*len(doc) + [0]*(SEQ_LEN-len(que)-len(doc))\n",
        "\n",
        "    # Padding\n",
        "    if len(que+doc) <= SEQ_LEN:\n",
        "      while len(que+doc) != SEQ_LEN:\n",
        "        doc.append(0)\n",
        "\n",
        "    # Final Input 'ids' (Question + Context)\n",
        "    ids = que + doc\n",
        "\n",
        "    # Sliding Part\n",
        "    fine_text = tokenizer.encode(data_df[FINE_TEXT].astype(str)[i])[0]\n",
        "    fine_text_slide_len = len(fine_text[1:-1]) # text_slide_len = 8\n",
        "\n",
        "    for j in range(0, (len(doc))):\n",
        "      # exist_flag : showing whether it is answerable or not (similar with is_unanswerable in SimpleTransforemr)\n",
        "      # 0 : No answer / 1 : Having answer\n",
        "      exist_flag = 0 \n",
        "      if fine_text[1:-1] == doc[j:j+fine_text_slide_len]: # [0:8]->[1:9]->[2:10]->..->[159:160]\n",
        "        # Assign the location of answer (start, end)\n",
        "        fine_ans_start = j + len(que)\n",
        "        fine_ans_end = j + fine_text_slide_len - 1 + len(que)\n",
        "        # If matched, exist_flag changed to 1\n",
        "        exist_flag = 1\n",
        "        break\n",
        "\n",
        "    # When no answer case (exist_flag = 0), starting & ending value become SEQ_LEN\n",
        "    # All the data of starting, ending = 384 (SEQ_LEN) will be deleted from the list\n",
        "    if exist_flag == 0:\n",
        "      fine_ans_start = SEQ_LEN\n",
        "      fine_ans_end = SEQ_LEN\n",
        "\n",
        "    # Input(ids), Segment saving into list type (indices, segments)\n",
        "    indices.append(ids)\n",
        "    segments.append(segment)\n",
        "    # Starting and ending info saving into list type (target_start, target_end)\n",
        "    target_start.append(fine_ans_start)\n",
        "    target_end.append(fine_ans_end)\n",
        "\n",
        "  # Converting the 4 lists into numpy array\n",
        "  indices_x = np.array(indices)\n",
        "  segments = np.array(segments)\n",
        "  target_start = np.array(target_start)\n",
        "  target_end = np.array(target_end)\n",
        "\n",
        "  # The cut part saved in del_list and deleted from data\n",
        "  del_list = np.where(target_start != SEQ_LEN)[0]\n",
        "  not_del_list = np.where(target_start == SEQ_LEN)[0]\n",
        "  indices_x = indices_x[del_list]\n",
        "  segments = segments[del_list]\n",
        "  target_start = target_start[del_list]\n",
        "  target_end = target_end[del_list]\n",
        "\n",
        "  return [indices_x, segments], [target_start, target_end], not_del_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "mB_G4K7yLJ82",
        "outputId": "d4c6d150-986c-40ac-b7fd-a20381a68ef9"
      },
      "execution_count": 455,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load of the Calculator Function\n",
        "def load_data(pandas_dataframe):\n",
        "  data_df = pandas_dataframe\n",
        "  data_df[FINE_DATA_COLUMN] = data_df[FINE_DATA_COLUMN].astype(str)\n",
        "  data_df[FINE_QUESTION_COLUMN] = data_df[FINE_QUESTION_COLUMN].astype(str)\n",
        "  data_x, data_y, del_list = fine_convert_data(data_df)\n",
        "  \n",
        "  return data_x, data_y, del_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "BnxtuqUkLax0",
        "outputId": "e29a19c3-acdf-470f-9176-c98a9e05373a"
      },
      "execution_count": 456,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Data Coneversion Starting\n",
        "fine_train_x, fine_train_y, z = load_data(fine_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "P-5Nnl48Lavg",
        "outputId": "6f034823-1ee3-424d-8a02-b09e86d16055"
      },
      "execution_count": 457,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 668/668 [00:02<00:00, 249.32it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(fine_train_x[0].shape, fine_train_y[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "rz9C9pDfLatN",
        "outputId": "6e12f413-c1e2-4001-97ca-1a37369dd7d1"
      },
      "execution_count": 458,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(451, 384) (451,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For Transfer Learning, Customized Layer needs to be added after 12 Encoder\n",
        "# by defining 'Non-masking' function, BERT Model's masked tensors disclosed\n",
        "\n",
        "class NonMasking(Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    self.supports_masking = True\n",
        "    super(NonMasking, self).__init__(**kwargs)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    input_shape = input_shape\n",
        "  \n",
        "  def compute_mask(self, input, input_mask = None):\n",
        "    return None\n",
        "\n",
        "  def call(self, x, mask = None):\n",
        "    return x\n",
        "\n",
        "  def get_output_shape_for(self, input_shape):\n",
        "    return input_shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "NoQsZ96rLaoj",
        "outputId": "ccd04be6-b255-4377-d200-9383432fda0a"
      },
      "execution_count": 459,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation function definition\n",
        "def bert_gelu(x):\n",
        "  \"\"\"Gaussian Error Linear Unit.\n",
        "  This is a smoother version of the RELU.\n",
        "  Original paper: https://arxiv.org/abs/1606.08415\n",
        "  Args:\n",
        "    x : float Tensor to perform activation\n",
        "  Returns:\n",
        "    'x' with the GELU activation applied.\n",
        "  \"\"\"\n",
        "  cdf = 0.5*(1.0+ K.tanh(\n",
        "      (np.sqrt(2/np.pi)*(x+0.044715 * K.pow(x,3)))))\n",
        "  return x*cdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "TEj0b5JILein",
        "outputId": "2bd1f9d1-1fea-4986-ec12-aef8d90618bc"
      },
      "execution_count": 460,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation Function Layer attached to Transformer\n",
        "class Start_End_Prediction(Layer):\n",
        "  def __init__(self, seq_len, **kwargs):\n",
        "    self.seq_len = SEQ_LEN\n",
        "    self.supports_masking = True\n",
        "    super(Start_End_Prediction, self).__init__(**kwargs)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    # A tensor ('self.W') multiplied with the final layer (12 encoder, batch_size, 384, 768)\n",
        "    # Making Output tensor as (384, 2 dimension (768->2))\n",
        "    self.W = self.add_weight(name='kernel',\n",
        "                             shape = (input_shape[2],2),\n",
        "                             initializer = 'uniform',\n",
        "                             trainable = True)\n",
        "    super(Start_End_Prediction, self).build(input_shape)\n",
        "\n",
        "  def call(self, x):\n",
        "    # Redifine Output dimension as (384 * 768)\n",
        "    x = K.reshape(x, shape=(-1, self.seq_len, K.shape(x)[2]))\n",
        "    # Dot production between self.W and x\n",
        "    # (batch_size, 384, 768) * (384, 2) = (batch_size, 384, 2)\n",
        "    x = K.dot(x, self.W)\n",
        "    # (batch_size, 384, 2) -> (2, batch_size, 384)\n",
        "    x = K.permute_dimensions(x, (2, 0, 1))\n",
        "\n",
        "    # Split the (2, batch_size, 384) into 2 (batch_size, 384)  --> start_logits & end_logits\n",
        "    # start_logits = (batch_size, 384)\n",
        "    # end_logits = (batch_size, 384)\n",
        "    self.start_logits, self.end_logits = x[0], x[1]\n",
        "\n",
        "    # Fed into, Passed by the GELU layer\n",
        "    self.start_logits = bert_gelu(self.start_logits)\n",
        "    self.end_logits = bert_gelu(self.end_logits)\n",
        "\n",
        "    # Fed into, Passed by the Softmax layer\n",
        "    # Getting probability for 384 tokens\n",
        "    self.start_logits = K.softmax(self.start_logits, axis=-1)\n",
        "    self.end_logits = K.softmax(self.end_logits, axis=-1)\n",
        "\n",
        "    return [self.start_logits, self.end_logits]\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    # In Keras, when defining custom layer, \n",
        "    # the output dimension must be defined in compute_output_shape function\n",
        "    return [(input_shape[0], self.seq_len), (input_shape[0], self.seq_len)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "0ESWVpp3Legf",
        "outputId": "ad014e67-ebb1-4000-da82-6548597416b7"
      },
      "execution_count": 461,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training -- Error\n",
        "# sess = K.get_session()\n",
        "# uninitialized_variables = set([i.decode('ascii') for i in sess.run(tf.report_uninitialized_variables())])\n",
        "# init = tf.variables_initializer([v for v in tf.global_variables() if v.name.split(':')[0] in uninitialized_variables])\n",
        "# sess.run(init)\n",
        "\n",
        "# bert_model = get_bert_finetuning_model(bert_model)\n",
        "bert_model.summary()\n",
        "\n",
        "# Training Start!\n",
        "history = bert_model.fit(fine_train_x, fine_train_y, batch_size=5, shuffle=True, verbose=1, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PF_DwEk1LhBD",
        "outputId": "f126a532-11b8-4e23-a6e0-027658d89021"
      },
      "execution_count": 462,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Input-Token (InputLayer)       [(None, 384)]        0           []                               \n",
            "                                                                                                  \n",
            " Input-Segment (InputLayer)     [(None, 384)]        0           []                               \n",
            "                                                                                                  \n",
            " Embedding-Token (TokenEmbeddin  [(None, 384, 768),  23440896    ['Input-Token[0][0]']            \n",
            " g)                              (30522, 768)]                                                    \n",
            "                                                                                                  \n",
            " Embedding-Segment (Embedding)  (None, 384, 768)     1536        ['Input-Segment[0][0]']          \n",
            "                                                                                                  \n",
            " Embedding-Token-Segment (Add)  (None, 384, 768)     0           ['Embedding-Token[0][0]',        \n",
            "                                                                  'Embedding-Segment[0][0]']      \n",
            "                                                                                                  \n",
            " Embedding-Position (PositionEm  (None, 384, 768)    294912      ['Embedding-Token-Segment[0][0]']\n",
            " bedding)                                                                                         \n",
            "                                                                                                  \n",
            " Embedding-Dropout (Dropout)    (None, 384, 768)     0           ['Embedding-Position[0][0]']     \n",
            "                                                                                                  \n",
            " Embedding-Norm (LayerNormaliza  (None, 384, 768)    1536        ['Embedding-Dropout[0][0]']      \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " Encoder-1-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Embedding-Norm[0][0]']         \n",
            " on (MultiHeadAttention)                                                                          \n",
            "                                                                                                  \n",
            " Encoder-1-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-1-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-1-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Embedding-Norm[0][0]',         \n",
            " on-Add (Add)                                                     'Encoder-1-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-1-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-1-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-1-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-1-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-1-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-1-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-1-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-1-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-1-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-1-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-1-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-2-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-1-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-2-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-2-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-2-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-1-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-2-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-2-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-2-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-2-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-2-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-2-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-2-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-2-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-2-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-2-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-2-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-2-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-3-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-2-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-3-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-3-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-3-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-2-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-3-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-3-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-3-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-3-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-3-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-3-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-3-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-3-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-3-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-3-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-3-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-3-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-4-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-3-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-4-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-4-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-4-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-3-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-4-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-4-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-4-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-4-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-4-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-4-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-4-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-4-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-4-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-4-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-4-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-4-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-5-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-4-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-5-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-5-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-5-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-4-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-5-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-5-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-5-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-5-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-5-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-5-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-5-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-5-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-5-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-5-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-5-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-5-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-6-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-5-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-6-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-6-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-6-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-5-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-6-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-6-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-6-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-6-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-6-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-6-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-6-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-6-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-6-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-6-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-6-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-6-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-7-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-6-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-7-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-7-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-7-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-6-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-7-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-7-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-7-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-7-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-7-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-7-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-7-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-7-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-7-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-7-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-7-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-7-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-8-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-7-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-8-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-8-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-8-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-7-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-8-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-8-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-8-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-8-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-8-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-8-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-8-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-8-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-8-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-8-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-8-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-8-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-9-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-8-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-9-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-9-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-9-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-8-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-9-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-9-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-9-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-9-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-9-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-9-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-9-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-9-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-9-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-9-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-9-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-9-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-10-MultiHeadSelfAttent  (None, 384, 768)    2362368     ['Encoder-9-FeedForward-Norm[0][0\n",
            " ion (MultiHeadAttention)                                        ]']                              \n",
            "                                                                                                  \n",
            " Encoder-10-MultiHeadSelfAttent  (None, 384, 768)    0           ['Encoder-10-MultiHeadSelfAttenti\n",
            " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
            "                                                                                                  \n",
            " Encoder-10-MultiHeadSelfAttent  (None, 384, 768)    0           ['Encoder-9-FeedForward-Norm[0][0\n",
            " ion-Add (Add)                                                   ]',                              \n",
            "                                                                  'Encoder-10-MultiHeadSelfAttenti\n",
            "                                                                 on-Dropout[0][0]']               \n",
            "                                                                                                  \n",
            " Encoder-10-MultiHeadSelfAttent  (None, 384, 768)    1536        ['Encoder-10-MultiHeadSelfAttenti\n",
            " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-10-FeedForward (FeedFo  (None, 384, 768)    4722432     ['Encoder-10-MultiHeadSelfAttenti\n",
            " rward)                                                          on-Norm[0][0]']                  \n",
            "                                                                                                  \n",
            " Encoder-10-FeedForward-Dropout  (None, 384, 768)    0           ['Encoder-10-FeedForward[0][0]'] \n",
            "  (Dropout)                                                                                       \n",
            "                                                                                                  \n",
            " Encoder-10-FeedForward-Add (Ad  (None, 384, 768)    0           ['Encoder-10-MultiHeadSelfAttenti\n",
            " d)                                                              on-Norm[0][0]',                  \n",
            "                                                                  'Encoder-10-FeedForward-Dropout[\n",
            "                                                                 0][0]']                          \n",
            "                                                                                                  \n",
            " Encoder-10-FeedForward-Norm (L  (None, 384, 768)    1536        ['Encoder-10-FeedForward-Add[0][0\n",
            " ayerNormalization)                                              ]']                              \n",
            "                                                                                                  \n",
            " Encoder-11-MultiHeadSelfAttent  (None, 384, 768)    2362368     ['Encoder-10-FeedForward-Norm[0][\n",
            " ion (MultiHeadAttention)                                        0]']                             \n",
            "                                                                                                  \n",
            " Encoder-11-MultiHeadSelfAttent  (None, 384, 768)    0           ['Encoder-11-MultiHeadSelfAttenti\n",
            " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
            "                                                                                                  \n",
            " Encoder-11-MultiHeadSelfAttent  (None, 384, 768)    0           ['Encoder-10-FeedForward-Norm[0][\n",
            " ion-Add (Add)                                                   0]',                             \n",
            "                                                                  'Encoder-11-MultiHeadSelfAttenti\n",
            "                                                                 on-Dropout[0][0]']               \n",
            "                                                                                                  \n",
            " Encoder-11-MultiHeadSelfAttent  (None, 384, 768)    1536        ['Encoder-11-MultiHeadSelfAttenti\n",
            " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-11-FeedForward (FeedFo  (None, 384, 768)    4722432     ['Encoder-11-MultiHeadSelfAttenti\n",
            " rward)                                                          on-Norm[0][0]']                  \n",
            "                                                                                                  \n",
            " Encoder-11-FeedForward-Dropout  (None, 384, 768)    0           ['Encoder-11-FeedForward[0][0]'] \n",
            "  (Dropout)                                                                                       \n",
            "                                                                                                  \n",
            " Encoder-11-FeedForward-Add (Ad  (None, 384, 768)    0           ['Encoder-11-MultiHeadSelfAttenti\n",
            " d)                                                              on-Norm[0][0]',                  \n",
            "                                                                  'Encoder-11-FeedForward-Dropout[\n",
            "                                                                 0][0]']                          \n",
            "                                                                                                  \n",
            " Encoder-11-FeedForward-Norm (L  (None, 384, 768)    1536        ['Encoder-11-FeedForward-Add[0][0\n",
            " ayerNormalization)                                              ]']                              \n",
            "                                                                                                  \n",
            " Encoder-12-MultiHeadSelfAttent  (None, 384, 768)    2362368     ['Encoder-11-FeedForward-Norm[0][\n",
            " ion (MultiHeadAttention)                                        0]']                             \n",
            "                                                                                                  \n",
            " Encoder-12-MultiHeadSelfAttent  (None, 384, 768)    0           ['Encoder-12-MultiHeadSelfAttenti\n",
            " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
            "                                                                                                  \n",
            " Encoder-12-MultiHeadSelfAttent  (None, 384, 768)    0           ['Encoder-11-FeedForward-Norm[0][\n",
            " ion-Add (Add)                                                   0]',                             \n",
            "                                                                  'Encoder-12-MultiHeadSelfAttenti\n",
            "                                                                 on-Dropout[0][0]']               \n",
            "                                                                                                  \n",
            " Encoder-12-MultiHeadSelfAttent  (None, 384, 768)    1536        ['Encoder-12-MultiHeadSelfAttenti\n",
            " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-12-FeedForward (FeedFo  (None, 384, 768)    4722432     ['Encoder-12-MultiHeadSelfAttenti\n",
            " rward)                                                          on-Norm[0][0]']                  \n",
            "                                                                                                  \n",
            " Encoder-12-FeedForward-Dropout  (None, 384, 768)    0           ['Encoder-12-FeedForward[0][0]'] \n",
            "  (Dropout)                                                                                       \n",
            "                                                                                                  \n",
            " Encoder-12-FeedForward-Add (Ad  (None, 384, 768)    0           ['Encoder-12-MultiHeadSelfAttenti\n",
            " d)                                                              on-Norm[0][0]',                  \n",
            "                                                                  'Encoder-12-FeedForward-Dropout[\n",
            "                                                                 0][0]']                          \n",
            "                                                                                                  \n",
            " Encoder-12-FeedForward-Norm (L  (None, 384, 768)    1536        ['Encoder-12-FeedForward-Add[0][0\n",
            " ayerNormalization)                                              ]']                              \n",
            "                                                                                                  \n",
            " non_masking_2 (NonMasking)     (None, 384, 768)     0           ['Encoder-12-FeedForward-Norm[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " start__end__prediction_1 (Star  [(None, 384),       1536        ['non_masking_2[0][0]']          \n",
            " t_End_Prediction)               (None, 384)]                                                     \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,794,880\n",
            "Trainable params: 108,794,880\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/5\n",
            "91/91 [==============================] - 71s 331ms/step - loss: 2.4518 - start__end__prediction_1_loss: 1.2514 - start__end__prediction_1_1_loss: 1.2004 - start__end__prediction_1_accuracy: 0.6718 - start__end__prediction_1_1_accuracy: 0.6829\n",
            "Epoch 2/5\n",
            "91/91 [==============================] - 30s 332ms/step - loss: 1.5971 - start__end__prediction_1_loss: 0.8296 - start__end__prediction_1_1_loss: 0.7675 - start__end__prediction_1_accuracy: 0.7583 - start__end__prediction_1_1_accuracy: 0.7849\n",
            "Epoch 3/5\n",
            "91/91 [==============================] - 30s 333ms/step - loss: 1.1408 - start__end__prediction_1_loss: 0.5819 - start__end__prediction_1_1_loss: 0.5588 - start__end__prediction_1_accuracy: 0.8248 - start__end__prediction_1_1_accuracy: 0.8404\n",
            "Epoch 4/5\n",
            "91/91 [==============================] - 30s 334ms/step - loss: 0.7764 - start__end__prediction_1_loss: 0.4117 - start__end__prediction_1_1_loss: 0.3646 - start__end__prediction_1_accuracy: 0.8758 - start__end__prediction_1_1_accuracy: 0.8936\n",
            "Epoch 5/5\n",
            "91/91 [==============================] - 30s 332ms/step - loss: 0.5253 - start__end__prediction_1_loss: 0.2748 - start__end__prediction_1_1_loss: 0.2506 - start__end__prediction_1_accuracy: 0.9113 - start__end__prediction_1_1_accuracy: 0.9246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# It shows accuracy about 76% in the above\n",
        "# Now need to get f1 score as alternative\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "fine_preds = bert_model.predict(fine_train_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "jdrAPkfSLg-r",
        "outputId": "f5c2a56b-e049-4123-8a6c-3f6e323d5c08"
      },
      "execution_count": 463,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fine_start_indexes = np.argmax(fine_preds[0], axis=-1)\n",
        "fine_end_indexes = np.argmax(fine_preds[1], axis=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "BF355v_ULg8L",
        "outputId": "eb0a658a-4c8d-4cbf-c7ee-c7e82c76a60b"
      },
      "execution_count": 464,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing F1 score of two tokens (start, end)\n",
        "\n",
        "# Start token ('start_indexes') : 82%\n",
        "print(classification_report(fine_train_y[0], fine_start_indexes))\n",
        "# End token ('end_indexes') : 85%\n",
        "print(classification_report(fine_train_y[1], fine_end_indexes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NYYZClczLamF",
        "outputId": "ad22e72c-ac41-4db9-b2c1-696bffbbf424"
      },
      "execution_count": 465,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           8       1.00      1.00      1.00         4\n",
            "           9       1.00      1.00      1.00         3\n",
            "          11       1.00      1.00      1.00         3\n",
            "          12       1.00      1.00      1.00         2\n",
            "          14       1.00      1.00      1.00         2\n",
            "          15       1.00      1.00      1.00         4\n",
            "          16       1.00      1.00      1.00         5\n",
            "          17       1.00      1.00      1.00         1\n",
            "          18       1.00      1.00      1.00         1\n",
            "          19       1.00      1.00      1.00         5\n",
            "          20       1.00      1.00      1.00         2\n",
            "          21       1.00      1.00      1.00         6\n",
            "          22       1.00      1.00      1.00         2\n",
            "          24       1.00      1.00      1.00         1\n",
            "          25       1.00      1.00      1.00         2\n",
            "          26       1.00      1.00      1.00         1\n",
            "          27       1.00      1.00      1.00         3\n",
            "          28       1.00      1.00      1.00         1\n",
            "          29       0.50      1.00      0.67         1\n",
            "          30       1.00      1.00      1.00         1\n",
            "          31       1.00      1.00      1.00         6\n",
            "          32       1.00      1.00      1.00         3\n",
            "          33       1.00      1.00      1.00         7\n",
            "          34       1.00      1.00      1.00         2\n",
            "          35       1.00      1.00      1.00         3\n",
            "          36       1.00      1.00      1.00         1\n",
            "          37       1.00      1.00      1.00         1\n",
            "          38       1.00      1.00      1.00         1\n",
            "          39       1.00      0.50      0.67         2\n",
            "          40       1.00      1.00      1.00         1\n",
            "          41       1.00      1.00      1.00         2\n",
            "          42       1.00      1.00      1.00         4\n",
            "          43       1.00      1.00      1.00         5\n",
            "          44       0.75      1.00      0.86         3\n",
            "          45       1.00      0.80      0.89         5\n",
            "          46       0.80      0.80      0.80         5\n",
            "          47       1.00      1.00      1.00         4\n",
            "          48       1.00      1.00      1.00         1\n",
            "          49       1.00      1.00      1.00         8\n",
            "          50       1.00      1.00      1.00         2\n",
            "          52       1.00      1.00      1.00        10\n",
            "          53       1.00      1.00      1.00         5\n",
            "          54       0.86      1.00      0.92         6\n",
            "          55       1.00      1.00      1.00         3\n",
            "          56       1.00      0.75      0.86         4\n",
            "          57       1.00      1.00      1.00         4\n",
            "          58       1.00      1.00      1.00         7\n",
            "          59       1.00      1.00      1.00         1\n",
            "          60       1.00      1.00      1.00         6\n",
            "          61       1.00      1.00      1.00         2\n",
            "          62       1.00      1.00      1.00         2\n",
            "          63       1.00      1.00      1.00         5\n",
            "          64       1.00      1.00      1.00         3\n",
            "          65       1.00      0.86      0.92         7\n",
            "          66       0.67      1.00      0.80         2\n",
            "          67       1.00      1.00      1.00         6\n",
            "          68       1.00      1.00      1.00         3\n",
            "          69       1.00      1.00      1.00         6\n",
            "          70       1.00      1.00      1.00         4\n",
            "          71       1.00      1.00      1.00         5\n",
            "          72       1.00      1.00      1.00         7\n",
            "          73       1.00      1.00      1.00         1\n",
            "          74       1.00      1.00      1.00         3\n",
            "          75       1.00      1.00      1.00         2\n",
            "          76       1.00      1.00      1.00         2\n",
            "          77       1.00      1.00      1.00         3\n",
            "          78       1.00      1.00      1.00         7\n",
            "          79       1.00      1.00      1.00         4\n",
            "          80       1.00      1.00      1.00         2\n",
            "          81       1.00      1.00      1.00         2\n",
            "          82       1.00      1.00      1.00         5\n",
            "          83       0.80      1.00      0.89         4\n",
            "          86       1.00      1.00      1.00         2\n",
            "          87       1.00      0.80      0.89         5\n",
            "          88       1.00      1.00      1.00         3\n",
            "          89       1.00      1.00      1.00         4\n",
            "          90       1.00      1.00      1.00         5\n",
            "          91       1.00      1.00      1.00         2\n",
            "          93       1.00      1.00      1.00         5\n",
            "          94       1.00      1.00      1.00         4\n",
            "          95       1.00      1.00      1.00         3\n",
            "          96       1.00      1.00      1.00         2\n",
            "          97       1.00      1.00      1.00         3\n",
            "          98       1.00      1.00      1.00         1\n",
            "          99       1.00      1.00      1.00         2\n",
            "         100       1.00      1.00      1.00         3\n",
            "         101       1.00      1.00      1.00         4\n",
            "         102       1.00      1.00      1.00         4\n",
            "         103       1.00      1.00      1.00         8\n",
            "         104       1.00      1.00      1.00         4\n",
            "         105       1.00      1.00      1.00         2\n",
            "         107       1.00      1.00      1.00         1\n",
            "         108       1.00      0.60      0.75         5\n",
            "         109       0.71      1.00      0.83         5\n",
            "         110       1.00      1.00      1.00         3\n",
            "         111       1.00      1.00      1.00         2\n",
            "         112       0.50      1.00      0.67         1\n",
            "         113       1.00      1.00      1.00         2\n",
            "         114       1.00      1.00      1.00         3\n",
            "         115       1.00      1.00      1.00         5\n",
            "         116       1.00      1.00      1.00         2\n",
            "         117       1.00      1.00      1.00         1\n",
            "         118       1.00      1.00      1.00         2\n",
            "         119       0.00      0.00      0.00         1\n",
            "         120       1.00      1.00      1.00         5\n",
            "         122       1.00      1.00      1.00         2\n",
            "         123       1.00      1.00      1.00         1\n",
            "         124       1.00      1.00      1.00         1\n",
            "         125       1.00      1.00      1.00         3\n",
            "         126       1.00      1.00      1.00         1\n",
            "         127       1.00      1.00      1.00         4\n",
            "         128       1.00      1.00      1.00         3\n",
            "         129       1.00      1.00      1.00         1\n",
            "         130       1.00      1.00      1.00         1\n",
            "         131       1.00      1.00      1.00         1\n",
            "         132       1.00      1.00      1.00         1\n",
            "         133       1.00      1.00      1.00         1\n",
            "         134       1.00      0.50      0.67         2\n",
            "         135       1.00      1.00      1.00         2\n",
            "         136       0.00      0.00      0.00         1\n",
            "         137       0.50      1.00      0.67         1\n",
            "         138       1.00      1.00      1.00         1\n",
            "         139       1.00      1.00      1.00         1\n",
            "         140       1.00      1.00      1.00         1\n",
            "         142       1.00      1.00      1.00         3\n",
            "         143       1.00      1.00      1.00         2\n",
            "         144       1.00      1.00      1.00         1\n",
            "         145       1.00      1.00      1.00         1\n",
            "         146       1.00      1.00      1.00         1\n",
            "         147       1.00      1.00      1.00         3\n",
            "         148       1.00      1.00      1.00         2\n",
            "         149       1.00      1.00      1.00         1\n",
            "         152       1.00      1.00      1.00         3\n",
            "         153       1.00      1.00      1.00         1\n",
            "         155       0.00      0.00      0.00         0\n",
            "         156       1.00      1.00      1.00         1\n",
            "         158       1.00      1.00      1.00         2\n",
            "         159       1.00      0.67      0.80         3\n",
            "         162       1.00      1.00      1.00         1\n",
            "         163       1.00      1.00      1.00         2\n",
            "         164       1.00      1.00      1.00         1\n",
            "         168       1.00      1.00      1.00         2\n",
            "         170       1.00      1.00      1.00         1\n",
            "         173       1.00      1.00      1.00         1\n",
            "         174       0.50      1.00      0.67         1\n",
            "         176       1.00      1.00      1.00         1\n",
            "         178       1.00      1.00      1.00         1\n",
            "         180       1.00      1.00      1.00         1\n",
            "         181       1.00      1.00      1.00         1\n",
            "         182       1.00      1.00      1.00         1\n",
            "         184       1.00      1.00      1.00         1\n",
            "         189       1.00      1.00      1.00         1\n",
            "         192       1.00      1.00      1.00         1\n",
            "         195       1.00      1.00      1.00         1\n",
            "         197       1.00      1.00      1.00         2\n",
            "         199       1.00      1.00      1.00         2\n",
            "         203       1.00      1.00      1.00         2\n",
            "         204       1.00      1.00      1.00         1\n",
            "         205       1.00      0.50      0.67         2\n",
            "         206       0.50      1.00      0.67         1\n",
            "         209       1.00      1.00      1.00         2\n",
            "         210       1.00      1.00      1.00         1\n",
            "         216       1.00      1.00      1.00         1\n",
            "         219       1.00      1.00      1.00         2\n",
            "         225       1.00      1.00      1.00         1\n",
            "         227       1.00      1.00      1.00         1\n",
            "         234       0.50      1.00      0.67         1\n",
            "         237       1.00      1.00      1.00         1\n",
            "         262       1.00      1.00      1.00         1\n",
            "         276       1.00      1.00      1.00         1\n",
            "         287       0.00      0.00      0.00         1\n",
            "         313       0.00      0.00      0.00         1\n",
            "         323       1.00      1.00      1.00         1\n",
            "         325       1.00      1.00      1.00         1\n",
            "         330       0.00      0.00      0.00         0\n",
            "         334       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           0.97       451\n",
            "   macro avg       0.94      0.95      0.94       451\n",
            "weighted avg       0.97      0.97      0.97       451\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           8       1.00      1.00      1.00         2\n",
            "           9       1.00      1.00      1.00         1\n",
            "          10       1.00      1.00      1.00         1\n",
            "          11       1.00      1.00      1.00         2\n",
            "          12       1.00      1.00      1.00         2\n",
            "          14       1.00      1.00      1.00         3\n",
            "          15       1.00      1.00      1.00         4\n",
            "          16       1.00      1.00      1.00         4\n",
            "          17       1.00      1.00      1.00         2\n",
            "          18       1.00      1.00      1.00         1\n",
            "          19       1.00      1.00      1.00         2\n",
            "          20       1.00      1.00      1.00         3\n",
            "          21       1.00      1.00      1.00         1\n",
            "          22       1.00      1.00      1.00         5\n",
            "          23       1.00      1.00      1.00         3\n",
            "          24       1.00      1.00      1.00         3\n",
            "          25       1.00      1.00      1.00         1\n",
            "          26       1.00      1.00      1.00         2\n",
            "          28       1.00      1.00      1.00         4\n",
            "          29       1.00      1.00      1.00         2\n",
            "          30       1.00      1.00      1.00         1\n",
            "          31       1.00      1.00      1.00         3\n",
            "          32       1.00      1.00      1.00         2\n",
            "          33       1.00      1.00      1.00         4\n",
            "          34       1.00      1.00      1.00         4\n",
            "          35       1.00      1.00      1.00         6\n",
            "          37       1.00      1.00      1.00         1\n",
            "          39       1.00      1.00      1.00         1\n",
            "          40       1.00      1.00      1.00         2\n",
            "          41       1.00      1.00      1.00         2\n",
            "          42       1.00      1.00      1.00         4\n",
            "          43       1.00      1.00      1.00         5\n",
            "          44       1.00      1.00      1.00         4\n",
            "          45       1.00      1.00      1.00         5\n",
            "          46       1.00      1.00      1.00         5\n",
            "          47       1.00      1.00      1.00         1\n",
            "          48       1.00      1.00      1.00         3\n",
            "          49       1.00      1.00      1.00         4\n",
            "          50       1.00      1.00      1.00         5\n",
            "          51       1.00      1.00      1.00         1\n",
            "          52       1.00      1.00      1.00         4\n",
            "          53       1.00      1.00      1.00         7\n",
            "          54       1.00      1.00      1.00         3\n",
            "          55       1.00      1.00      1.00         4\n",
            "          56       1.00      1.00      1.00         5\n",
            "          57       1.00      1.00      1.00         4\n",
            "          58       1.00      1.00      1.00         4\n",
            "          59       1.00      1.00      1.00         4\n",
            "          60       1.00      1.00      1.00         4\n",
            "          61       1.00      1.00      1.00         1\n",
            "          62       1.00      1.00      1.00         3\n",
            "          63       1.00      1.00      1.00         5\n",
            "          64       1.00      1.00      1.00         2\n",
            "          65       1.00      1.00      1.00         6\n",
            "          66       1.00      1.00      1.00         5\n",
            "          67       1.00      1.00      1.00         6\n",
            "          68       1.00      1.00      1.00         2\n",
            "          69       1.00      1.00      1.00         3\n",
            "          70       1.00      1.00      1.00         3\n",
            "          71       1.00      1.00      1.00         3\n",
            "          72       1.00      1.00      1.00         7\n",
            "          73       1.00      1.00      1.00         2\n",
            "          74       1.00      1.00      1.00         4\n",
            "          75       1.00      1.00      1.00         2\n",
            "          76       1.00      1.00      1.00         2\n",
            "          77       1.00      1.00      1.00         2\n",
            "          78       0.50      1.00      0.67         1\n",
            "          79       1.00      1.00      1.00         3\n",
            "          80       1.00      1.00      1.00         3\n",
            "          81       1.00      1.00      1.00         2\n",
            "          82       1.00      1.00      1.00         2\n",
            "          83       1.00      1.00      1.00         5\n",
            "          84       1.00      1.00      1.00         4\n",
            "          85       1.00      1.00      1.00         3\n",
            "          86       1.00      1.00      1.00         2\n",
            "          87       1.00      1.00      1.00         5\n",
            "          88       1.00      1.00      1.00         1\n",
            "          89       1.00      1.00      1.00         3\n",
            "          90       1.00      1.00      1.00         6\n",
            "          91       1.00      1.00      1.00         3\n",
            "          92       1.00      1.00      1.00         2\n",
            "          93       1.00      1.00      1.00         3\n",
            "          94       1.00      1.00      1.00         2\n",
            "          95       1.00      1.00      1.00         1\n",
            "          97       1.00      1.00      1.00         6\n",
            "          99       1.00      1.00      1.00         5\n",
            "         100       1.00      1.00      1.00         6\n",
            "         101       1.00      1.00      1.00         4\n",
            "         102       1.00      1.00      1.00         5\n",
            "         103       1.00      0.80      0.89         5\n",
            "         104       1.00      1.00      1.00         5\n",
            "         105       1.00      1.00      1.00         2\n",
            "         106       1.00      1.00      1.00         2\n",
            "         107       1.00      1.00      1.00         3\n",
            "         108       1.00      1.00      1.00         5\n",
            "         109       1.00      1.00      1.00         4\n",
            "         110       1.00      1.00      1.00         2\n",
            "         111       1.00      1.00      1.00         7\n",
            "         114       1.00      1.00      1.00         1\n",
            "         115       1.00      1.00      1.00         3\n",
            "         116       1.00      1.00      1.00         2\n",
            "         117       1.00      1.00      1.00         2\n",
            "         118       0.83      1.00      0.91         5\n",
            "         119       1.00      1.00      1.00         1\n",
            "         120       1.00      0.67      0.80         3\n",
            "         121       1.00      1.00      1.00         3\n",
            "         123       1.00      1.00      1.00         2\n",
            "         124       1.00      1.00      1.00         1\n",
            "         125       0.50      1.00      0.67         1\n",
            "         126       1.00      1.00      1.00         1\n",
            "         127       1.00      1.00      1.00         5\n",
            "         128       1.00      1.00      1.00         1\n",
            "         129       1.00      1.00      1.00         1\n",
            "         130       1.00      1.00      1.00         5\n",
            "         131       1.00      1.00      1.00         4\n",
            "         132       1.00      1.00      1.00         1\n",
            "         133       1.00      1.00      1.00         1\n",
            "         134       1.00      1.00      1.00         1\n",
            "         135       1.00      0.50      0.67         2\n",
            "         137       1.00      1.00      1.00         1\n",
            "         138       0.00      0.00      0.00         0\n",
            "         139       1.00      1.00      1.00         2\n",
            "         140       1.00      1.00      1.00         1\n",
            "         141       1.00      1.00      1.00         3\n",
            "         142       1.00      1.00      1.00         2\n",
            "         143       1.00      1.00      1.00         1\n",
            "         144       1.00      1.00      1.00         1\n",
            "         145       1.00      1.00      1.00         1\n",
            "         146       1.00      1.00      1.00         1\n",
            "         147       1.00      1.00      1.00         2\n",
            "         148       1.00      1.00      1.00         3\n",
            "         151       1.00      1.00      1.00         1\n",
            "         152       1.00      1.00      1.00         1\n",
            "         153       1.00      1.00      1.00         2\n",
            "         154       1.00      0.50      0.67         2\n",
            "         157       0.00      0.00      0.00         1\n",
            "         158       1.00      1.00      1.00         3\n",
            "         159       1.00      1.00      1.00         2\n",
            "         161       1.00      1.00      1.00         3\n",
            "         162       1.00      1.00      1.00         3\n",
            "         164       1.00      1.00      1.00         2\n",
            "         165       1.00      1.00      1.00         1\n",
            "         167       1.00      1.00      1.00         1\n",
            "         168       1.00      1.00      1.00         1\n",
            "         170       1.00      1.00      1.00         2\n",
            "         172       1.00      1.00      1.00         1\n",
            "         173       0.50      1.00      0.67         1\n",
            "         174       1.00      0.50      0.67         2\n",
            "         176       1.00      1.00      1.00         1\n",
            "         177       0.00      0.00      0.00         0\n",
            "         178       0.00      0.00      0.00         0\n",
            "         184       0.00      0.00      0.00         1\n",
            "         187       1.00      1.00      1.00         1\n",
            "         189       1.00      1.00      1.00         1\n",
            "         190       1.00      1.00      1.00         1\n",
            "         192       1.00      1.00      1.00         2\n",
            "         196       1.00      1.00      1.00         2\n",
            "         197       1.00      1.00      1.00         1\n",
            "         202       1.00      1.00      1.00         1\n",
            "         204       1.00      1.00      1.00         1\n",
            "         205       1.00      1.00      1.00         1\n",
            "         206       1.00      1.00      1.00         3\n",
            "         210       1.00      1.00      1.00         1\n",
            "         211       1.00      1.00      1.00         1\n",
            "         215       1.00      1.00      1.00         1\n",
            "         217       1.00      1.00      1.00         1\n",
            "         224       1.00      1.00      1.00         3\n",
            "         225       1.00      1.00      1.00         1\n",
            "         226       0.00      0.00      0.00         0\n",
            "         227       1.00      1.00      1.00         1\n",
            "         230       1.00      1.00      1.00         1\n",
            "         237       1.00      1.00      1.00         2\n",
            "         240       1.00      1.00      1.00         2\n",
            "         243       0.00      0.00      0.00         1\n",
            "         262       1.00      1.00      1.00         1\n",
            "         281       1.00      1.00      1.00         1\n",
            "         299       1.00      1.00      1.00         1\n",
            "         323       1.00      1.00      1.00         1\n",
            "         326       1.00      1.00      1.00         1\n",
            "         335       1.00      1.00      1.00         1\n",
            "         339       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           0.98       451\n",
            "   macro avg       0.95      0.95      0.95       451\n",
            "weighted avg       0.99      0.98      0.98       451\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Fine_tuned trained model save\n",
        "# # Since it takes too much time, trained model saved in the 'gdrive'\n",
        "# path = \"gdrive/MyDrive/Colab Notebooks/squad\"\n",
        "# bert_model.save_weights(path+\"/(Uncased)Squad.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "o1BcwlvZLks_",
        "outputId": "79df7346-e009-4eba-dca4-5b1ba45a8798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Code for loading pretrained BERT and fine_tuned trained model\n",
        "# bert_model = get_bert_finetuning_model(model)\n",
        "# path = \"gdrive/MyDrive/Colab Notebooks/squad\"\n",
        "# bert_model.load_weights(path+\"/(Uncased)Squad.h5\")"
      ],
      "metadata": {
        "id": "3BvzR_cjLC5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Fine-tuning Testing"
      ],
      "metadata": {
        "id": "IBZgY4YWu3fO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test dataset loading\n",
        "import pandas as pd\n",
        "test_context = pd.read_excel('Test_QA_OneByOne.xlsx')\n",
        "test_question = pd.read_excel('Question.xlsx')"
      ],
      "metadata": {
        "id": "A7ZzHKgAu50G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "8e13e204-7f29-4adb-b155-9904a730ef58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_context"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Yv2juRnzwVs4",
        "outputId": "0b52b558-dd6d-4911-94e5-52eeff47492b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b9fd6e54-ffcf-4d6c-a520-b7eae82fbfe9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Index</th>\n",
              "      <th>Paper</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Target</th>\n",
              "      <th>Method In-Vivo</th>\n",
              "      <th>Method In-Vitro</th>\n",
              "      <th>Method Others</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>371</td>\n",
              "      <td>Ginkgo biloba prevents mobile phone-induced ox...</td>\n",
              "      <td>Background: The widespread use of mobile phone...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>372</td>\n",
              "      <td>Effects of radiofrequency exposure on the GABA...</td>\n",
              "      <td>The widespread use of cellular phones raises t...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>376</td>\n",
              "      <td>Long-term exposure to electromagnetic radiatio...</td>\n",
              "      <td>The aim of the present study was the investiga...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>377</td>\n",
              "      <td>Effect of mobile phone signal radiation on epi...</td>\n",
              "      <td>Exponential increase in mobile phone uses, giv...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>378</td>\n",
              "      <td>Effects of a Single Head Exposure to GSM-1800 ...</td>\n",
              "      <td>Mobile communications are propagated by electr...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>379</td>\n",
              "      <td>Melatonin Modulates NMDA-Receptor 2B/Calpain-1...</td>\n",
              "      <td>Aim: To investigate the potential protective e...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>382</td>\n",
              "      <td>Hippocampal lipidome and transcriptome profile...</td>\n",
              "      <td>Background: The widespread use of wireless dev...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>383</td>\n",
              "      <td>Exposure to 835 MHz RF-EMF decreases the expre...</td>\n",
              "      <td>The exponential increase in the use of mobile ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>386</td>\n",
              "      <td>Effects of 2G mobile phone exposure on both be...</td>\n",
              "      <td>Mobile communications are expanded day by day ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>387</td>\n",
              "      <td>Evaluation of bax, bcl-2, p21 and p53 genes ex...</td>\n",
              "      <td>Objectives: The increasing rate of over using ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>388</td>\n",
              "      <td>The effect of 1800MHz radio-frequency radiatio...</td>\n",
              "      <td>Background: The aim of this study was to evalu...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>389</td>\n",
              "      <td>Effects of acute and chronic exposure to both ...</td>\n",
              "      <td>Purpose: To demonstrate the molecular effects ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>390</td>\n",
              "      <td>Proteomic analysis of continuous 900-MHz radio...</td>\n",
              "      <td>Although cell phones have been used worldwide,...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>391</td>\n",
              "      <td>Activation of autophagy at cerebral cortex and...</td>\n",
              "      <td>With the explosive increase in exposure to rad...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>393</td>\n",
              "      <td>Long-term exposure to 835 MHz RF-EMF induces h...</td>\n",
              "      <td>Radiofrequency electromagnetic field (RF-EMF) ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>394</td>\n",
              "      <td>Study of potential health effects of electroma...</td>\n",
              "      <td>The objective of this study is to investigate ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>395</td>\n",
              "      <td>Mobile-phone radiation-induced perturbation of...</td>\n",
              "      <td>The daily use by people of wireless communicat...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>401</td>\n",
              "      <td>Long term and excessive use of 900 MHz radiofr...</td>\n",
              "      <td>Purpose: We still do not have any information ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>404</td>\n",
              "      <td>Analysis of rat testicular proteome following ...</td>\n",
              "      <td>The use of electromagnetic field (EMF) generat...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>406</td>\n",
              "      <td>Effect of 3G Cell Phone Exposure with Computer...</td>\n",
              "      <td>Cell phone radiation exposure and its biologic...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>415</td>\n",
              "      <td>Brain proteome response following whole body e...</td>\n",
              "      <td>The objective of this study was to investigate...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>417</td>\n",
              "      <td>Effects of electromagnetic radiation produced ...</td>\n",
              "      <td>Objective: The effects of electromagnetic radi...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>424</td>\n",
              "      <td>Qualitative Effect on mRNAs of Injury-Associat...</td>\n",
              "      <td>Rats were exposed to cell phone radiation for ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>431</td>\n",
              "      <td>Upregulation of specific mRNA levels in rat br...</td>\n",
              "      <td>Adult Sprague-Dawley rats were exposed to regu...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>433</td>\n",
              "      <td>No evidence of major transcriptional changes i...</td>\n",
              "      <td>To analyze possible effects of microwaves on g...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>446</td>\n",
              "      <td>Exposure of rat brain to 915 MHz GSM microwave...</td>\n",
              "      <td>We investigated whether exposure of rat brain ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>449</td>\n",
              "      <td>Expression of the immediate early gene, c-fos,...</td>\n",
              "      <td>Aims: To study the effect of acute exposure to...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>453</td>\n",
              "      <td>Bone morphogenetic protein expression in newbo...</td>\n",
              "      <td>Effects of nonthermal radiofrequency radiation...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>455</td>\n",
              "      <td>IRIDIUM exposure increases c-fos expression in...</td>\n",
              "      <td>With the rapid development of wireless communi...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b9fd6e54-ffcf-4d6c-a520-b7eae82fbfe9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b9fd6e54-ffcf-4d6c-a520-b7eae82fbfe9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b9fd6e54-ffcf-4d6c-a520-b7eae82fbfe9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    Index  ... Method Others\n",
              "0     371  ...             0\n",
              "1     372  ...             0\n",
              "2     376  ...             0\n",
              "3     377  ...             0\n",
              "4     378  ...             0\n",
              "5     379  ...             0\n",
              "6     382  ...             0\n",
              "7     383  ...             0\n",
              "8     386  ...             0\n",
              "9     387  ...             0\n",
              "10    388  ...             0\n",
              "11    389  ...             0\n",
              "12    390  ...             0\n",
              "13    391  ...             0\n",
              "14    393  ...             0\n",
              "15    394  ...             0\n",
              "16    395  ...             0\n",
              "17    401  ...             0\n",
              "18    404  ...             0\n",
              "19    406  ...             0\n",
              "20    415  ...             0\n",
              "21    417  ...             0\n",
              "22    424  ...             0\n",
              "23    431  ...             0\n",
              "24    433  ...             0\n",
              "25    446  ...             0\n",
              "26    449  ...             0\n",
              "27    453  ...             0\n",
              "28    455  ...             0\n",
              "\n",
              "[29 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_temp = test_context['Abstract']\n",
        "test_context = df_temp.to_frame()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "YmpqWv-dGzH2",
        "outputId": "f1c38ff1-a64c-4054-e7e9-367c55b9766f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_context"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 996
        },
        "id": "Pl8nGxM1QUAE",
        "outputId": "ebac3d99-f7f0-49a4-ffda-d1e180c633bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-70b6096c-a2bd-46c3-a99b-0976e308cea2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Background: The widespread use of mobile phone...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The widespread use of cellular phones raises t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The aim of the present study was the investiga...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Exponential increase in mobile phone uses, giv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Mobile communications are propagated by electr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Aim: To investigate the potential protective e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Background: The widespread use of wireless dev...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>The exponential increase in the use of mobile ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Mobile communications are expanded day by day ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Objectives: The increasing rate of over using ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Background: The aim of this study was to evalu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Purpose: To demonstrate the molecular effects ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Although cell phones have been used worldwide,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>With the explosive increase in exposure to rad...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Radiofrequency electromagnetic field (RF-EMF) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>The objective of this study is to investigate ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>The daily use by people of wireless communicat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Purpose: We still do not have any information ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>The use of electromagnetic field (EMF) generat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Cell phone radiation exposure and its biologic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>The objective of this study was to investigate...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Objective: The effects of electromagnetic radi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Rats were exposed to cell phone radiation for ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Adult Sprague-Dawley rats were exposed to regu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>To analyze possible effects of microwaves on g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>We investigated whether exposure of rat brain ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>Aims: To study the effect of acute exposure to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Effects of nonthermal radiofrequency radiation...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>With the rapid development of wireless communi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-70b6096c-a2bd-46c3-a99b-0976e308cea2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-70b6096c-a2bd-46c3-a99b-0976e308cea2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-70b6096c-a2bd-46c3-a99b-0976e308cea2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                             Abstract\n",
              "0   Background: The widespread use of mobile phone...\n",
              "1   The widespread use of cellular phones raises t...\n",
              "2   The aim of the present study was the investiga...\n",
              "3   Exponential increase in mobile phone uses, giv...\n",
              "4   Mobile communications are propagated by electr...\n",
              "5   Aim: To investigate the potential protective e...\n",
              "6   Background: The widespread use of wireless dev...\n",
              "7   The exponential increase in the use of mobile ...\n",
              "8   Mobile communications are expanded day by day ...\n",
              "9   Objectives: The increasing rate of over using ...\n",
              "10  Background: The aim of this study was to evalu...\n",
              "11  Purpose: To demonstrate the molecular effects ...\n",
              "12  Although cell phones have been used worldwide,...\n",
              "13  With the explosive increase in exposure to rad...\n",
              "14  Radiofrequency electromagnetic field (RF-EMF) ...\n",
              "15  The objective of this study is to investigate ...\n",
              "16  The daily use by people of wireless communicat...\n",
              "17  Purpose: We still do not have any information ...\n",
              "18  The use of electromagnetic field (EMF) generat...\n",
              "19  Cell phone radiation exposure and its biologic...\n",
              "20  The objective of this study was to investigate...\n",
              "21  Objective: The effects of electromagnetic radi...\n",
              "22  Rats were exposed to cell phone radiation for ...\n",
              "23  Adult Sprague-Dawley rats were exposed to regu...\n",
              "24  To analyze possible effects of microwaves on g...\n",
              "25  We investigated whether exposure of rat brain ...\n",
              "26  Aims: To study the effect of acute exposure to...\n",
              "27  Effects of nonthermal radiofrequency radiation...\n",
              "28  With the rapid development of wireless communi..."
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_question"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "6-fvGGU5QVa_",
        "outputId": "25c1187e-5509-433a-f552-6663fada70c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-5aed0822-9938-4844-909a-ab442617eb16\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What animal has been used?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How many animals were used?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What is the signal frequency?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How much W/kg was used?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5aed0822-9938-4844-909a-ab442617eb16')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5aed0822-9938-4844-909a-ab442617eb16 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5aed0822-9938-4844-909a-ab442617eb16');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                        Question\n",
              "0     What animal has been used?\n",
              "1    How many animals were used?\n",
              "2  What is the signal frequency?\n",
              "3        How much W/kg was used?"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "8GRLlRTwSrOA",
        "outputId": "0d3aef0a-82bd-47ee-c545-3b091dd9b45c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copied & Pasted, and a bit modified from the above function\n",
        "# Define a function calculating the length of answer in context dataset all at once\n",
        "def convert_data(test_context, test_question):\n",
        "  global tokenizer\n",
        "  indices, segments = [], []\n",
        "\n",
        "  que, _ = tokenizer.encode(test_question['Question'])\n",
        "  doc, _ = tokenizer.encode(test_context['Abstract'])\n",
        "  doc.pop(0)\n",
        "\n",
        "  que_len = len(que)\n",
        "  doc_len = len(doc)\n",
        "\n",
        "  # 1. Length of question\n",
        "  # The question is cut by the length of 64\n",
        "  if que_len > 64:\n",
        "    que = que[:63]\n",
        "    que.append(102) # [SEP] token added to make it clear the question block\n",
        "  \n",
        "  # 2. Total length of question and context\n",
        "  # The total input is cut by the length of 384\n",
        "  if len(que+doc) > SEQ_LEN:\n",
        "    while len(que+doc) != SEQ_LEN:\n",
        "      doc.pop(-1)\n",
        "    doc.pop(-1)\n",
        "    doc.append(102)\n",
        "\n",
        "    # Segment embedding\n",
        "    # Question : 0 / Context 1 / Padding : 0 (remaining part for short sentences)\n",
        "        \n",
        "    ############################\n",
        "    ###### Segment 예시 ########\n",
        "    ############################\n",
        "    \n",
        "    # question, context, padding\n",
        "    # 00000000, 1111111, 0000000\n",
        "    \n",
        "  segment = [0]*len(que) + [1]*len(doc) + [0]*(SEQ_LEN-len(que)-len(doc))\n",
        "\n",
        "  # Padding\n",
        "  if len(que+doc) <= SEQ_LEN:\n",
        "    while len(que+doc) != SEQ_LEN:\n",
        "      doc.append(0)\n",
        "\n",
        "  # Final Input 'ids' (Question + Context)\n",
        "  ids = que + doc\n",
        "\n",
        "  # Input(ids), Segment saving into list type (indices, segments)\n",
        "  indices.append(ids)\n",
        "  segments.append(segment)\n",
        "\n",
        "  # Converting the 4 lists into numpy array\n",
        "  indices = np.array(indices)\n",
        "  segments = np.array(segments)\n",
        "  \n",
        "  # print(indices, segments)\n",
        "  return [indices, segments]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "DMwTMIjcwWZw",
        "outputId": "78dfc6db-c635-4303-80a9-6247774e88cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert_data(test_question.iloc[0], test_context[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "-aSljit0_OcV",
        "outputId": "f1bca43e-6eaf-41e1-b5c4-5932018660a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_letter(df_context, df_question):\n",
        "  test_input = convert_data(df_context, df_question)\n",
        "  test_start, test_end = bert_model.predict(test_input)\n",
        "\n",
        "  indexes = test_input[0].tolist()[0]\n",
        "  start = np.argmax(test_start, axis=1).item()\n",
        "  end = np.argmax(test_end, axis=1).item()\n",
        "  start_tok = indexes[start]\n",
        "  end_tok = indexes[end]\n",
        "\n",
        "  # # print(\"Context: \", end = \" \")\n",
        "  # print(\"Context\")\n",
        "  # print(\"-\"*100)\n",
        "\n",
        "  # def split_text(text, n):\n",
        "  #   for line in text.splitlines():\n",
        "  #     while len(line) > n:\n",
        "  #       x, line = line[:n], line[n:]\n",
        "  #       yield x\n",
        "  #     yield line\n",
        "  \n",
        "  # for line in split_text(df_context['Abstract'], 100):\n",
        "  #   print(line)\n",
        "\n",
        "  # print(\"-\"*100)\n",
        "  \n",
        "  print(\"Question: \", df_question['Question'])\n",
        "  print(\"ANSWER: \", end = \" \")\n",
        "  # print(\"\\n\")\n",
        "  sentences = []\n",
        "\n",
        "  for i in range(start, end+1):\n",
        "    token_based_word = reverse_token_dict[indexes[i]]\n",
        "    sentences.append(token_based_word)\n",
        "    # print(token_based_word, end= \" \")\n",
        "  # print(\"\\n\")\n",
        "  # print(\"Untokenized Answer: \", end= \"\")\n",
        "  \n",
        "  for w in sentences:\n",
        "    if w.startswith(\"##\"):\n",
        "      w = w.replace(\"##\", \"\")\n",
        "    else:\n",
        "      w = \" \" + w\n",
        "    print(w, end=\"\")\n",
        "  # print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "aMjhGuIZXDtA",
        "outputId": "74d372c5-810f-475f-863a-846b32c96630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  \n",
        "  print(\"Context [\", i+1, \"]\")\n",
        "  print(\"-\"*100)\n",
        "  def split_text(text, n):\n",
        "    for line in text.splitlines():\n",
        "      while len(line) > n:\n",
        "        x, line = line[:n], line[n:]\n",
        "        yield x\n",
        "      yield line\n",
        "  \n",
        "  for line in split_text(test_context.iloc[i]['Abstract'], 100):\n",
        "    print(line)\n",
        "\n",
        "  print(\"-\"*100)\n",
        "\n",
        "  for j in range(4):\n",
        "  # answers = test['answers'][i]\n",
        "    predict_letter(test_context.iloc[i], test_question.iloc[j])\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "  print(\"\")\n",
        "  print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CjQq9VXPXRJp",
        "outputId": "cccff747-d23a-4952-fabf-67db24931d66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context [ 1 ]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Background: The widespread use of mobile phones (MP) in recent years has raised the research activit\n",
            "ies in many countries to determine the consequences of exposure to the low-intensity electromagnetic\n",
            " radiation (EMR) of mobile phones. Since several experimental studies suggest a role of reactive oxy\n",
            "gen species (ROS) in EMR-induced oxidative damage in tissues, in this study, we investigated the eff\n",
            "ect of Ginkgo biloba (Gb) on MP-induced oxidative damage in brain tissue of rats.\n",
            "\n",
            "Methods: Rats (EMR+) were exposed to 900 MHz EMR from MP for 7 days (1 h/day). In the EMR+Gb groups,\n",
            " rats were exposed to EMR and pretreated with Gb. Control and Gb-administrated groups were produced \n",
            "by turning off the mobile phone while the animals were in the same exposure conditions. Subsequently\n",
            ", oxidative stress markers and pathological changes in brain tissue were examined for each groups.\n",
            "\n",
            "Results: Oxidative damage was evident by the: (i) increase in malondialdehyde (MDA) and nitric oxide\n",
            " (NO) levels in brain tissue, (ii) decrease in brain superoxide dismutase (SOD) and glutathione pero\n",
            "xidase (GSH-Px) activities and (iii) increase in brain xanthine oxidase (XO) and adenosine deaminase\n",
            " (ADA) activities. These alterations were prevented by Gb treatment. Furthermore, Gb prevented the M\n",
            "P-induced cellular injury in brain tissue histopathologically.\n",
            "\n",
            "Conclusion: Reactive oxygen species may play a role in the mechanism that has been proposed to expla\n",
            "in the biological side effects of MP, and Gb prevents the MP-induced oxidative stress to preserve an\n",
            "tioxidant enzymes activity in brain tissue.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question:  What animal has been used?\n",
            "ANSWER:   rats\n",
            "\n",
            "Question:  How many animals were used?\n",
            "ANSWER:   rats ( emr + ) were exposed to 900 mhz emr from mp for 7 days ( 1 h / day ) .\n",
            "\n",
            "Question:  What is the signal frequency?\n",
            "ANSWER:   900 mhz\n",
            "\n",
            "Question:  How much W/kg was used?\n",
            "ANSWER:   rats . methods : rats ( emr + ) were exposed to 900 mhz emr from mp for 7 days ( 1 h / day )\n",
            "\n",
            "\n",
            "\n",
            "Context [ 2 ]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "The widespread use of cellular phones raises the problem of interaction of electromagnetic fields wi\n",
            "th the central nervous system (CNS). In order to measure these effects on neurotransmitter content i\n",
            "n the CNS, we developed a protocol of neurotransmitter detection based on immunohistochemistry and i\n",
            "mage analysis. Gamma-vinyl-GABA (GVG), an inhibitor of the GABA-transaminase was injected in rats to\n",
            " increase GABA concentration in the CNS. The cellular GABA contents were then revealed by immunohist\n",
            "ochemistry and semi-quantified by image analysis thanks to three parameters: optical density (O.D.),\n",
            " staining area, and number of positive cells. The increase in cerebellar GABA content induced by GVG\n",
            " 1200 mg/kg was reflected in these three parameters in the molecular and the granular layers. Theref\n",
            "ore, control of immunohistochemistry parameters, together with appropriate image analysis, allowed b\n",
            "oth the location and the detection of variations in cellular neurotransmitter content. This protocol\n",
            " was used to investigate the effects of exposure to 900 MHz radiofrequencies on cerebellar GABA cont\n",
            "ent. Both pulsed emission with a specific absorption rate (SAR) of 4 W/kg and continuous emission wi\n",
            "th high SAR (32 W/kg) were tested. We observed a selective diminution of the stained processes area \n",
            "in the Purkinje cell layer after exposure to pulsed radiofrequency and, in addition, a decrease in O\n",
            ".D. in the three cell layers after exposure to continuous waves. Whether this effect is, at least pa\n",
            "rtly, due to a local heating of the tissues is not known. Overall, it appears that high energetic ra\n",
            "diofrequency exposure induces a diminution in cellular GABA content in the cerebellum.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question:  What animal has been used?\n",
            "ANSWER:   rats\n",
            "\n",
            "Question:  How many animals were used?\n",
            "ANSWER:   gamma - vinyl - gaba ( gvg ) , an inhibitor of the gaba - transaminase was injected in rats to increase gaba concentration in the cns . the cellular gaba contents were then revealed by immunohistochemistry and semi - quantified by image analysis thanks to three parameters : optical density ( o . d . ) , staining area , and number of positive cells . the increase in cerebellar gaba content induced by gvg 1200 mg / kg was reflected in these three parameters in the molecular and the granular layers . therefore , control of immunohistochemistry parameters , together with appropriate image analysis , allowed both the location and the detection of variations in cellular neurotransmitter content . this protocol was used to investigate the effects of exposure to 900 mhz radiofrequencies on cerebellar gaba content . both pulsed emission with a specific absorption rate ( sar ) of 4 w / kg and continuous emission with high sar ( 32 w / kg )\n",
            "\n",
            "Question:  What is the signal frequency?\n",
            "ANSWER:   900 mhz\n",
            "\n",
            "Question:  How much W/kg was used?\n",
            "ANSWER:   4 w / kg\n",
            "\n",
            "\n",
            "\n",
            "Context [ 3 ]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "The aim of the present study was the investigation of the effects of mobile phones at different dail\n",
            "y exposure times on the hippocampal expression of two apoptotic genes. Forty-eight male BALB/c mice \n",
            "were randomly divided into six groups with 8 animals in each group. Four experimental groups were re\n",
            "spectively exposed to electromagnetic waves for 0.5, 1, 2 and 4 hours twice a day for 30 consecutive\n",
            " days. One experimental group was radiated for 4 hours once a day, while the control group did not r\n",
            "eceive any radiation during the experiment. The expression of both Bax and Bcl2 mRNAs was upregulate\n",
            "d in the mice exposed for one and two hours. Whilst the highest expressions were observed in the two\n",
            "-hours radiation in the exposed group, the expression of both studied genes was downregulated in ani\n",
            "mals with longer exposure to radiation in a duration-dependent manner. The highest ratio of Bax/Bcl2\n",
            " expression was observed in the mice that received radiation for four hours twice a day. These resul\n",
            "ts revealed that mobile phone radiation can cause considerable changes in the balance of Bax/Bcl2 mR\n",
            "NA expression in laboratory mice hippocampus.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question:  What animal has been used?\n",
            "ANSWER:   balb / c mice\n",
            "\n",
            "Question:  How many animals were used?\n",
            "ANSWER:   forty - eight\n",
            "\n",
            "Question:  What is the signal frequency?\n",
            "ANSWER:  \n",
            "\n",
            "Question:  How much W/kg was used?\n",
            "ANSWER:   forty - eight\n",
            "\n",
            "\n",
            "\n",
            "Context [ 4 ]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Exponential increase in mobile phone uses, given rise to public concern regarding the alleged delete\n",
            "rious health hazards as a consequence of prolonged exposure. In 2018, the U.S. National toxicology p\n",
            "rogram reported, two year toxicological studies for potential health hazards from exposure to cell p\n",
            "hone radiations. Epigenetic modulations play a critical regulatory role in many cellular functions a\n",
            "nd pathological conditions. In this study, we assessed the dose-dependent and frequency-dependent ep\n",
            "igenetic modulation (DNA and Histone methylation) in the hippocampus of Wistar rats. A Total of 96 m\n",
            "ale Wistar rats were segregated into 12 groups exposed to 900 MHz, 1800 MHz and 2450 MHz RF-MW at a \n",
            "specific absorption rate (SAR) of 5.84 × 10-4 W/kg, 5.94 × 10-4 W/kg and 6.4 × 10-4 W/kg respectivel\n",
            "y for 2 h per day for 1-month, 3-month and 6-month periods. At the end of the exposure duration, ani\n",
            "mals were sacrificed to collect the hippocampus. Global hippocampal DNA methylation and histone meth\n",
            "ylation were estimated by ELISA. However, DNA methylating enzymes, DNA methyltransferase1 (DNMT1) an\n",
            "d histone methylating enzymes euchromatic histone methylthransferase1 (EHMT1) expression was evaluat\n",
            "ed by real-time PCR, as well as further validated with Western blot. Alteration in epigenetic modula\n",
            "tion was observed in the hippocampus. Global DNA methylation was decreased and histone methylation w\n",
            "as increased in the hippocampus. We observed that microwave exposure led to significant epigenetic m\n",
            "odulations in the hippocampus with increasing frequency and duration of exposure. Microwave exposure\n",
            " with increasing frequency and exposure duration brings significant (p < 0.05) epigenetic modulation\n",
            "s which alters gene expression in the hippocampus.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question:  What animal has been used?\n",
            "ANSWER:   wistar rats\n",
            "\n",
            "Question:  How many animals were used?\n",
            "ANSWER:   96\n",
            "\n",
            "Question:  What is the signal frequency?\n",
            "ANSWER:   900 mhz , 1800 mhz and 2450 mhz\n",
            "\n",
            "Question:  How much W/kg was used?\n",
            "ANSWER:   5 . 84 × 10 - 4 w / kg\n",
            "\n",
            "\n",
            "\n",
            "Context [ 5 ]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Mobile communications are propagated by electromagnetic fields (EMFs), and since the 1990s, they ope\n",
            "rate with pulse-modulated signals such as the GSM-1800 MHz. The biological effects of GSM-EMF in hum\n",
            "ans affected by neuropathological processes remain seldom investigated. In this study, a 2-h head-on\n",
            "ly exposure to GSM-1800 MHz was applied to (i) rats undergoing an acute neuroinflammation triggered \n",
            "by a lipopolysaccharide (LPS) treatment, (ii) age-matched healthy rats, or (iii) transgenic hSOD1G93\n",
            "A rats that modeled a presymptomatic phase of human amyotrophic lateral sclerosis (ALS). Gene respon\n",
            "ses were assessed 24 h after the GSM head-only exposure in a motor area of the cerebral cortex (mCx)\n",
            " where the mean specific absorption rate (SAR) was estimated to be 3.22 W/kg. In LPS-treated rats, a\n",
            " genome-wide mRNA profiling was performed by RNA-seq analysis and revealed significant (adjusted p v\n",
            "alue < 0.05) but moderate (fold changes < 2) upregulations or downregulations affecting 2.7% of the \n",
            "expressed genes, including genes expressed predominantly in neuronal or in glial cell types and grou\n",
            "ps of genes involved in protein ubiquitination or dephosphorylation. Reverse transcription-quantitat\n",
            "ive PCR analyses confirmed gene modulations uncovered by RNA-seq data and showed that in a set of 15\n",
            " PCR-assessed genes, significant gene responses to GSM-1800 MHz depended upon the acute neuroinflamm\n",
            "atory state triggered in LPS-treated rats, because they were not observed in healthy or in hSOD1G93A\n",
            " rats. Together, our data specify the extent of cortical gene modulations triggered by GSM-EMF in th\n",
            "e course of an acute neuroinflammation and indicate that GSM-induced gene responses can differ accor\n",
            "ding to pathologies affecting the CNS.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question:  What animal has been used?\n",
            "ANSWER:   rats\n",
            "\n",
            "Question:  How many animals were used?\n",
            "ANSWER:   ( i ) rats undergoing an acute neuroinflammation triggered by a lipopolysaccharide ( lps ) treatment , ( ii ) age - matched healthy rats , or ( iii ) transgenic hsod1g93a rats\n",
            "\n",
            "Question:  What is the signal frequency?\n",
            "ANSWER:   gsm - 1800 mhz\n",
            "\n",
            "Question:  How much W/kg was used?\n",
            "ANSWER:   3 . 22 w / kg\n",
            "\n",
            "\n",
            "\n",
            "Context [ 6 ]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Aim: To investigate the potential protective effects of melatonin on the chronic radiation emitted b\n",
            "y third generation mobile phones on the brain.\n",
            "\n",
            "Material and methods: A total of 24 male Wistar albino rats were divided into four equal groups. Thr\n",
            "oughout a 90-day experiment, no application was performed on the control group. The second group was\n",
            " exposed to 2100 MHz radiation for 30 minutes. Subcutaneous melatonin was injected into the third gr\n",
            "oup. Subcutaneous melatonin injection was applied 40 minutes before radiation and then the fourth gr\n",
            "oup was exposed to radiation for 30 minutes. At the end of the experiment, brain (cerebrum and cereb\n",
            "ellum) tissues were taken from the subjects. Histochemical, immunohistochemical, ultrastructural and\n",
            " western blot analyses were applied. In addition to brain weight, Purkinje cellsâ€™ number, immunohi\n",
            "stochemical H Score analyses and the results of the Western blot were examined statistically.\n",
            "\n",
            "Results: With the application of radiation, neuronal edema, relatively-decreased numbers of neurons \n",
            "on hippocampal CA1 and CA3 regions, displacement of the Purkinje neurons and dark neurons findings w\n",
            "ere observed as a result of histochemical stainings. Radiation also activated the NMDA-receptor 2B/C\n",
            "alpain-1/Caspase-12 pathway, NMDA-receptor 2B and Calpain-1 with the findings being supported by wes\n",
            "tern blot analyses. Pre-increased protein synthesis before apoptosis was identified by electron micr\n",
            "oscopy.\n",
            "\n",
            "Conclusion: Mobile phone radiation caused certain (ultra) structural changes on the brain and activa\n",
            "ted the NMDA-receptor 2B/ Calpain-1/Caspase-12 pathway; in addition, melatonin was found to be effec\n",
            "tive, but insufficient in demonstrating the protective effects.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question:  What animal has been used?\n",
            "ANSWER:   wistar albino rats\n",
            "\n",
            "Question:  How many animals were used?\n",
            "ANSWER:   24\n",
            "\n",
            "Question:  What is the signal frequency?\n",
            "ANSWER:   2100 mhz\n",
            "\n",
            "Question:  How much W/kg was used?\n",
            "ANSWER:   40 minutes before radiation and then the fourth group was exposed to radiation for 30 minutes .\n",
            "\n",
            "\n",
            "\n",
            "Context [ 7 ]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Background: The widespread use of wireless devices during the last decades is raising concerns about\n",
            " adverse health effects of the radiofrequency electromagnetic radiation (RF-EMR) emitted from these \n",
            "devices. Recent research is focusing on unraveling the underlying mechanisms of RF-EMR and potential\n",
            " cellular targets. The \"omics\" high-throughput approaches are powerful tools to investigate the glob\n",
            "al effects of RF-EMR on cellular physiology.\n",
            "\n",
            "Methods: In this work, C57BL/6 adult male mice were whole-body exposed (nExp = 8) for 2 hr to GSM 18\n",
            "00 MHz mobile phone radiation at an average electric field intensity range of 4.3-17.5 V/m or sham-e\n",
            "xposed (nSE = 8), and the RF-EMR effects on the hippocampal lipidome and transcriptome profiles were\n",
            " assessed 6 hr later.\n",
            "\n",
            "Results: The data analysis of the phospholipid fatty acid residues revealed that the levels of four \n",
            "fatty acids [16:0, 16:1 (6c + 7c), 18:1 9c, eicosapentaenoic acid omega-3 (EPA, 20:5 ω3)] and the tw\n",
            "o fatty acid sums of saturated and monounsaturated fatty acids (SFA and MUFA) were significantly alt\n",
            "ered (p < 0.05) in the exposed group. The observed changes indicate a membrane remodeling response o\n",
            "f the tissue phospholipids after nonionizing radiation exposure, reducing SFA and EPA, while increas\n",
            "ing MUFA residues. The microarray data analysis demonstrated that the expression of 178 genes change\n",
            "d significantly (p < 0.05) between the two groups, revealing an impact on genes involved in critical\n",
            " biological processes, such as cell cycle, DNA replication and repair, cell death, cell signaling, n\n",
            "ervous system development and function, immune system response, lipid metabolism, and carcinogenesis\n",
            ".\n",
            "\n",
            "Conclusions: This study provides preliminary evidence that mobile phone radiation induces hippocampa\n",
            "l lipidome and transcriptome changes that may explain the brain proteome changes and memory deficits\n",
            " previously shown by our group.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question:  What animal has been used?\n",
            "ANSWER:   c57bl / 6 adult male mice\n",
            "\n",
            "Question:  How many animals were used?\n",
            "ANSWER:   c57bl / 6\n",
            "\n",
            "Question:  What is the signal frequency?\n",
            "ANSWER:   1800 mhz\n",
            "\n",
            "Question:  How much W/kg was used?\n",
            "ANSWER:   c57bl / 6 adult male mice were whole - body exposed ( nexp = 8 ) for 2 hr to gsm 1800 mhz mobile phone radiation at an average electric field intensity range of 4 . 3 - 17 . 5 v / m or sham - exposed ( nse = 8 )\n",
            "\n",
            "\n",
            "\n",
            "Context [ 8 ]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "The exponential increase in the use of mobile communication has triggered public concerns about the \n",
            "potential adverse effects of radiofrequency electromagnetic fields (RF-EMF) emitted by mobile phones\n",
            " on the central nervous system (CNS). In this study, we explored the relationship between calcium ch\n",
            "annels and apoptosis or autophagy in the hippocampus of C57BL/6 mice after RF-EMF exposure with a sp\n",
            "ecific absorption rate (SAR) of 4.0 W/kg for 4 weeks. Firstly, the expression level of voltage-gated\n",
            " calcium channels (VGCCs), a key regulator of the entry of calcium ions into the cell, was confirmed\n",
            " by immunoblots. We investigated and confirmed that pan-calcium channel expression in hippocampal ne\n",
            "urons were significantly decreased after exposure to RF-EMF. With the observed accumulation of autol\n",
            "ysosomes in hippocampal neurons via TEM, the expressions of autophagy-related genes and proteins (e.\n",
            "g., LC3B-II) had significantly increased. However, down-regulation of the apoptotic pathway may cont\n",
            "ribute to the decrease in calcium channel expression, and thus lower levels of calcium in hippocampa\n",
            "l neurons. These results suggested that exposure of RF-EMF could alter intracellular calcium homeost\n",
            "asis by decreasing calcium channel expression in the hippocampus; presumably by activating the autop\n",
            "hagy pathway, while inhibiting apoptotic regulation as an adaptation process for 835 MHz RF-EMF expo\n",
            "sure.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question:  What animal has been used?\n",
            "ANSWER:   c57bl / 6 mice\n",
            "\n",
            "Question:  How many animals were used?\n",
            "ANSWER:   c57bl / 6 mice\n",
            "\n",
            "Question:  What is the signal frequency?\n",
            "ANSWER:   835 mhz\n",
            "\n",
            "Question:  How much W/kg was used?\n",
            "ANSWER:   4 . 0 w / kg\n",
            "\n",
            "\n",
            "\n",
            "Context [ 9 ]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Mobile communications are expanded day by day and bring along their potential adverse effects on hum\n",
            "an brain. One of the affected brain regions is known as hippocampus which is the most studied struct\n",
            "ure because of its well documented role in learning and memory. The major intracellular signaling pa\n",
            "thway implemented in memory formation in the hippocampus is N-methyl-D-aspartate (NMDA)-dependent pa\n",
            "thway including activation of kinases. Experimental animal studies have demonstrated several effects\n",
            " of short and/or long term RF-EMR exposure on cognitive functions and behaviors of animals. In the l\n",
            "iterature, little is known about the effects of RF-EMR exposure on NMDA receptor signalling pathway.\n",
            " Therefore, in the present study an attempt was taken to demonstrate possible effects of acute and c\n",
            "hronic 900 MHz RF-EMR exposure on both passive avoidance behavior and hippocampal level of the enzym\n",
            "es from NMDA receptor related signalling pathway including p44/42 MAPK and it is phosphorylated form\n",
            " (phopsho p44/42 MAPK) using Western Blotting technique. Rats were divided into following groups: Sh\n",
            "am rats, rats exposed to 900 MHz RF-EMR for 2h/day for 1 (acute) or 10 (chronic) weeks, respectively\n",
            ". Overall results indicated that both acute and chronic exposure to 900 MHz RF-EMR can impair passiv\n",
            "e avoidance behavior with minor effect on behavior of chronic group of rats. In addition, hippocampa\n",
            "l levels of both p44/42 MAPK and phopsho p44/42 MAPK were significantly higher in chronic group of r\n",
            "ats. These findings demonstrated that different duration time (1 week versus 10 weeks) of 900 MHz RF\n",
            "-EMR exposure has different effects on both passive avoidance behavior of rats and hippocampal level\n",
            "s of p44/42 MAPK and phopsho p44/42 MAPK from NMDA dependent pathway.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question:  What animal has been used?\n",
            "ANSWER:   rats\n",
            "\n",
            "Question:  How many animals were used?\n",
            "ANSWER:  \n",
            "\n",
            "Question:  What is the signal frequency?\n",
            "ANSWER:   900 mhz\n",
            "\n",
            "Question:  How much W/kg was used?\n",
            "ANSWER:   900 mhz\n",
            "\n",
            "\n",
            "\n",
            "Context [ 10 ]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Objectives: The increasing rate of over using cell phones has been considerable in youths and pregna\n",
            "nt women. We examined the effect of mobile phones radiation on genes expression variation on cerebel\n",
            "lum of BALB/c mice before and after of the birth.\n",
            "\n",
            "Materials and methods: In this study, a mobile phone jammer, which is an instrument to prevent recei\n",
            "ving signals between cellular phones and base transceiver stations (two frequencies 900 and 1800 MHz\n",
            ") for exposure was used and twelve pregnant mice (BALB/c) divided into two groups (n=6), first group\n",
            " irradiated in pregnancy period (19th day), the second group did not irradiate in pregnancy period. \n",
            "After childbirth, offspring were classified into four groups (n=4): Group1: control, Group 2: B1 (Ir\n",
            "radiated after birth), Group 3: B2 (Irradiated in pregnancy period and after birth), Group 4: B3 (Ir\n",
            "radiated in pregnancy period). When maturity was completed (8-10 weeks old), mice were dissected and\n",
            " cerebellum was isolated. The expression level of bax, bcl-2, p21 and p53 genes examined by real-tim\n",
            "e reverse transcription polymerase chain reaction (Real-Time RT- PCR).\n",
            "\n",
            "Results: The data showed that mobile phone radio waves were ineffective on the expression level of b\n",
            "cl-2 and p53 genes) P>0.05(. Also gene expression level of bax decreased and gene expression level o\n",
            "f p21 increased comparing to the control group (P<0.05).\n",
            "\n",
            "Conclusion: From the obtained data it could be concluded that the mobile phone radiations did not in\n",
            "duce apoptosis in cells of the cerebellum and the injured cells can be repaired by cell cycle arrest\n",
            ".\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question:  What animal has been used?\n",
            "ANSWER:   balb / c mice\n",
            "\n",
            "Question:  How many animals were used?\n",
            "ANSWER:   twelve\n",
            "\n",
            "Question:  What is the signal frequency?\n",
            "ANSWER:   900 and 1800 mhz\n",
            "\n",
            "Question:  How much W/kg was used?\n",
            "ANSWER:   twelve\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Fine-tuning Evaluation - F1 & EM"
      ],
      "metadata": {
        "id": "NE5v3CmhzrhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the length of test data\n",
        "length_fine = len(test_context)"
      ],
      "metadata": {
        "id": "A68XkIxzhmJ0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "7a92bb67-7319-41b6-d845-be7957b1287a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# It shows accuracy about 76% in the above\n",
        "# Now need to get f1 score as alternative\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "preds = bert_model.predict(fine_train_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "1nE3UUpixZ1q",
        "outputId": "2c883bcd-9a48-416f-ecc7-f28ffab0a0cc"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-c991bb36cdd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfine_train_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'fine_train_x' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_indexes = np.argmax(preds[0], axis=-1)\n",
        "end_indexes = np.argmax(preds[1], axis=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "jjAKjOM2xXPC",
        "outputId": "cafa94a7-d6e9-4946-85b6-74201a19b0cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indexes = dev_bert_input[0]"
      ],
      "metadata": {
        "id": "EVW3jiPzzfmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_sentences = []\n",
        "fine_untokenized = []\n",
        "\n",
        "for j in range(len(start_indexes)):\n",
        "  sentence = []\n",
        "  # Saving each tokenized word into the list of sentence (sentence = [])\n",
        "  for i in range(start_indexes[j], end_indexes[j]+1):\n",
        "    token_based_word = reverse_token_dict[indexes[j][i]]\n",
        "    sentence.append(token_based_word)\n",
        "  sentence_string = \"\"\n",
        "\n",
        "  for w in sentence:\n",
        "    # Special token ## delete, if a token starts with ##\n",
        "    if w.startswith(\"##\"):\n",
        "      w = w.replace(\"##\", \"\")\n",
        "    # If a token has no ##, a space added to the word\n",
        "    else:\n",
        "      w = \" \" + w\n",
        "    # Putting together all the tokens in list format\n",
        "    sentence_string += w\n",
        "  \n",
        "  # If senetence_string starts with a space (\" \"), delete the space\n",
        "  if sentence_string.startswith(\" \"):\n",
        "    sentence_string = \"\" + sentence_string[1:]\n",
        "  \n",
        "  # After putting together all the list of tokens, and assigning it in the list of the 'Untokenized'\n",
        "  untokenized.append(sentence_string)\n",
        "  sentences.append(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "5WCrTICQ0FHY",
        "outputId": "b429cfd0-f09f-4bf2-e328-90ec43dea313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-98-68161038ed14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m# Saving each tokenized word into the list of sentence (sentence = [])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_indexes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_indexes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtoken_based_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreverse_token_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_based_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0msentence_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'indexes' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(fine_sentences[:30])\n",
        "print('\\n')\n",
        "print(fine_untokenized[:30])"
      ],
      "metadata": {
        "id": "nh_C4fZE0fDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the answers into list format\n",
        "fine_test_answers = []\n",
        "for i in range(length_fine):\n",
        "  fine_test_answer = []\n",
        "  texts_dict = test_context['answers'][i]\n",
        "\n",
        "  for j in range(len(texts_dict)):\n",
        "    dev_answer.append(texts_dict[j]['text'])\n",
        "  dev_answers.append(dev_answer)"
      ],
      "metadata": {
        "id": "jSQ75_Lb0gsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_answers[0:10]"
      ],
      "metadata": {
        "id": "AUPUThek1EwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the answers\n",
        "dev_tokens = []\n",
        "for i in dev_answers:\n",
        "  dev_tokened = []\n",
        "  for j in i:\n",
        "    temp_token = tokenizer.tokenize(j)\n",
        "    # Tokenize an answer\n",
        "    temp_token.pop(0)\n",
        "    # [CLS] elimination\n",
        "    temp_token.pop(-1)\n",
        "    # [SEP] elimination\n",
        "    dev_tokened.append(temp_token)\n",
        "  dev_tokens.append(dev_tokened)"
      ],
      "metadata": {
        "id": "mY6j6LUt1Ftl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dev_tokens[:5])"
      ],
      "metadata": {
        "id": "VKjqSjfw1HZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting tokenized answers into sentences\n",
        "# And putting all together\n",
        "dev_answer_lists = []\n",
        "for dev_answers in dev_tokens:\n",
        "  dev_answer_list = []\n",
        "  for dev_answer in dev_answers:\n",
        "    dev_answer_string = \" \".join(dev_answer)\n",
        "    dev_answer_list.append(dev_answer_string)\n",
        "  dev_answer_lists.append(dev_answer_list)"
      ],
      "metadata": {
        "id": "rFh5iO6b1Ipa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dev_answer_lists[:5])"
      ],
      "metadata": {
        "id": "sbwATbmn1KGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Untokenizing (including deleting \" ##\" etc)\n",
        "dev_strings_end = []\n",
        "for dev_strings in dev_answer_lists:\n",
        "  dev_strings_processed = []\n",
        "  for dev_string in dev_strings:\n",
        "    dev_string = dev_string.replace(\" ##\", \"\")\n",
        "    dev_strings_processed.append(dev_string)\n",
        "  dev_strings_end.append(dev_strings_processed)"
      ],
      "metadata": {
        "id": "9Pnxs8Ro1LMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_answers = dev_strings_end"
      ],
      "metadata": {
        "id": "5VhcS7vC1MxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dev_answers[:5])"
      ],
      "metadata": {
        "id": "xgnNN7Bv1NvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# F1 & EM function defined by myself\n",
        "from collections import Counter\n",
        "import string\n",
        "\n",
        "# In normalize step, doing some works such as converting words into lower case,\n",
        "# and deleting punctuations, clearing unnecessary spaces etc\n",
        "def normalize_answer(s):\n",
        "  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "  def remove_articles(text):\n",
        "    return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "  \n",
        "  def white_space_fix(text):\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "  def remove_punc(text):\n",
        "    exclude = set(string.punctuation)\n",
        "    return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "  def lower(text):\n",
        "    return text.lower()\n",
        "  \n",
        "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "  prediction_tokens = normalize_answer(prediction).split()\n",
        "  ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "  common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "  num_same = sum(common.values())\n",
        "  if num_same == 0:\n",
        "    return 0\n",
        "  precision = 1.0 * num_same / len(prediction_tokens)\n",
        "  recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "  f1 = (2 * precision * recall) / (precision + recall)\n",
        "  return f1\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "  return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "  scores_for_ground_truths = []\n",
        "  for ground_truth in ground_truths:\n",
        "    score = metric_fn(prediction, ground_truth)\n",
        "    scores_for_ground_truths.append(score)\n",
        "  return max(scores_for_ground_truths)"
      ],
      "metadata": {
        "id": "Ysb0xAlt1PF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating F1 Score\n",
        "# Theoretical Max : 88%\n",
        "f1_sum_fine = 0\n",
        "\n",
        "for i in range(len(fine_untokenized)):\n",
        "  f1 = metric_max_over_ground_truths(f1_score, fine_untokenized[i], dev_answers[i])\n",
        "  f1_sum_fine += f1\n",
        "print(\"f1 score: \", f1_sum_fine/length_fine)"
      ],
      "metadata": {
        "id": "TKKgCe121Rwm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "ced6cde5-73c6-4a63-e752-6b0c61311e46"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-bcf0521a1f0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mf1_sum_fine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfine_untokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric_max_over_ground_truths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfine_untokenized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_answers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mf1_sum_fine\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'fine_untokenized' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating EM score\n",
        "# Theoretical Max : 80%\n",
        "EM_sum_fine = 0\n",
        "\n",
        "for i in range(len(fine_untokenized)):\n",
        "  EM = metric_max_over_ground_truths(exact_match_score, untokenized[i], dev_answers[i])\n",
        "  EM_sum_fine += EM\n",
        "print(\"EM score: \", EM_sum_fine/length_fine)"
      ],
      "metadata": {
        "id": "539fb3v41TK-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}