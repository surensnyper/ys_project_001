{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2. Copy of Finetuned_Keras_QA.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yvk0NddfsrNJ",
        "dnAhc8NNaEkO",
        "84LS4gvdfcaX"
      ],
      "authorship_tag": "ABX9TyPzjVGQF7ZHWkJGIsFO8G8i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/surensnyper/ys_project_001/blob/main/2_Copy_of_Finetuned_Keras_QA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   video : https://www.youtube.com/watch?v=LuApA264Wbs&t=947s\n",
        "2.   code : https://github.com/kimwoonggon/publicservant_AI/blob/master/(Uncased_Squad_V1_1)_%EC%BC%80%EB%9D%BC%EC%8A%A4%EB%A1%9C_Q%26A_%EA%B5%AC%ED%98%84%ED%95%98%EA%B8%B0.ipynb"
      ],
      "metadata": {
        "id": "qH4ySjYTxxyI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1s3L3AYmRa6H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "0b2c4858-cdaa-423d-aa4c-75c82728fe77"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-30 23:24:10--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.203.128, 74.125.197.128, 74.125.142.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.203.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 407727028 (389M) [application/zip]\n",
            "Saving to: ‘uncased_L-12_H-768_A-12.zip.1’\n",
            "\n",
            "uncased_L-12_H-768_ 100%[===================>] 388.84M   307MB/s    in 1.3s    \n",
            "\n",
            "2022-01-30 23:24:12 (307 MB/s) - ‘uncased_L-12_H-768_A-12.zip.1’ saved [407727028/407727028]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# BERT Model Download\n",
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
        "\n",
        "# bert folder construction in the Colab\n",
        "if \"bert\" not in os.listdir():\n",
        "  os.makedirs(\"bert\")\n",
        "else:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the Model zip file in the bert folder\n",
        "bert_zip  = zipfile.ZipFile('uncased_L-12_H-768_A-12.zip')\n",
        "bert_zip.extractall('bert')\n",
        "\n",
        "bert_zip.close()"
      ],
      "metadata": {
        "id": "KGCk0CNuUo68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "cb807ef2-67fd-4512-f457-293921aaee97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Functoin Construction for copying the folder\n",
        "def copytree(src, dst, symlinks=False, ignore=None):\n",
        "  for item in os.listdir(src):\n",
        "    s = os.path.join(src, item)\n",
        "    d = os.path.join(dst, item)\n",
        "    if os.path.isdir(s):\n",
        "      shutil.copytrees(s, d, symlinks, ignore)\n",
        "    else:\n",
        "      shutil.copy2(s, d)"
      ],
      "metadata": {
        "id": "MfAmqbVFUwnz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "1cb607c1-2ad2-4075-91c8-ce530c8d88cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unzipped folder is \"bert/uncased_L-12_H-768_A-12\", but copying & moving the model to the bert folder for easier analysis\n",
        "copytree(\"bert/uncased_L-12_H-768_A-12\", \"bert\")"
      ],
      "metadata": {
        "id": "y_8xHilJVz_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SQuAD v1.1 dataset download for training (train-v1.1) and validation (dev-v1.1)\n",
        "!wget https://raw.githubusercontent.com/nate-parrott/squad/master/data/train-v1.1.json\n",
        "!wget https://raw.githubusercontent.com/nate-parrott/squad/master/data/dev-v1.1.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YtrjnUcV8HC",
        "outputId": "c7367f1b-f2c2-4948-dc05-d2ed5e1baa57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-30 19:07:08--  https://raw.githubusercontent.com/nate-parrott/squad/master/data/train-v1.1.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30288272 (29M) [text/plain]\n",
            "Saving to: ‘train-v1.1.json’\n",
            "\n",
            "train-v1.1.json     100%[===================>]  28.88M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-01-30 19:07:09 (243 MB/s) - ‘train-v1.1.json’ saved [30288272/30288272]\n",
            "\n",
            "--2022-01-30 19:07:09--  https://raw.githubusercontent.com/nate-parrott/squad/master/data/dev-v1.1.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4854279 (4.6M) [text/plain]\n",
            "Saving to: ‘dev-v1.1.json’\n",
            "\n",
            "dev-v1.1.json       100%[===================>]   4.63M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2022-01-30 19:07:10 (74.2 MB/s) - ‘dev-v1.1.json’ saved [4854279/4854279]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Google drive and Colab Connecting\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlEKI_0IW42k",
        "outputId": "842eb5ad-258b-49ba-bfdd-1ba8163fc448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Required modules are imported, e.g. Tensorflow, Pandas, Numpy, Keras etc\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "import keras as keras\n",
        "from keras.models import load_model\n",
        "from keras import backend as K\n",
        "from keras import Input, Model\n",
        "from keras import optimizers\n",
        "from keras.layers import Layer\n",
        "\n",
        "import codecs\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "import json"
      ],
      "metadata": {
        "id": "cdZWE1XhXx6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To make various logs nor popped up\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "os.environ['TF_CPP_WIN_LOG_LEVEL']='3'\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR) # Version updated (tensorflow logging tf.logging -> tf.compat.v1.logging)"
      ],
      "metadata": {
        "id": "u2NAZBYMYl8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A module 'keras-bert' install, for using bert easily in keras\n",
        "!pip install keras-bert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ET_bXlFHZIyk",
        "outputId": "abb2681a-023f-4e3c-af17-9bd3ceca1224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-bert\n",
            "  Downloading keras-bert-0.89.0.tar.gz (25 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-bert) (1.19.5)\n",
            "Collecting keras-transformer==0.40.0\n",
            "  Downloading keras-transformer-0.40.0.tar.gz (9.7 kB)\n",
            "Collecting keras-pos-embd==0.13.0\n",
            "  Downloading keras-pos-embd-0.13.0.tar.gz (5.6 kB)\n",
            "Collecting keras-multi-head==0.29.0\n",
            "  Downloading keras-multi-head-0.29.0.tar.gz (13 kB)\n",
            "Collecting keras-layer-normalization==0.16.0\n",
            "  Downloading keras-layer-normalization-0.16.0.tar.gz (3.9 kB)\n",
            "Collecting keras-position-wise-feed-forward==0.8.0\n",
            "  Downloading keras-position-wise-feed-forward-0.8.0.tar.gz (4.1 kB)\n",
            "Collecting keras-embed-sim==0.10.0\n",
            "  Downloading keras-embed-sim-0.10.0.tar.gz (3.6 kB)\n",
            "Collecting keras-self-attention==0.51.0\n",
            "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
            "Building wheels for collected packages: keras-bert, keras-transformer, keras-embed-sim, keras-layer-normalization, keras-multi-head, keras-pos-embd, keras-position-wise-feed-forward, keras-self-attention\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.89.0-py3-none-any.whl size=33517 sha256=e97066c329fd7c04f8576a1a8cedb1ac5ef04a926b3ca9a4a070e242e905244b\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/e8/45/842b3a39831261aef9154b907eacbc4ac99499a99ae829b06f\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.40.0-py3-none-any.whl size=12305 sha256=31366ef64c4ed86944b62022c182468b9196a7c0ba85c232e913f63188691674\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/68/26/692ed21edd832833c3b0a0e21615bcacd99ca458b3f9ed571f\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.10.0-py3-none-any.whl size=3960 sha256=d17a235810ca82b66ed5f3e0169b0ee6abdd6d85ef5d8fc94b982ae7473970c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/67/b5/d847588d075895281e1cf5590f819bd4cf076a554872268bd5\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.16.0-py3-none-any.whl size=4668 sha256=8203e2352d647fda8ae2b56e9f6811a89d4bd82a9f32c6d988cf9258c44e25b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/5d/1c/2e619f594f69fbcf8bc20943b27d414871c409be053994813e\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.29.0-py3-none-any.whl size=14993 sha256=bc1f08dd857b1ebac167f9290106b80baed305cef6594587690652c912d49976\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/aa/3c/9d15d24005179dae08ff291ce99c754b296347817d076fd9fb\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.13.0-py3-none-any.whl size=6962 sha256=1ad5f0654b5912c104a1009f96c7ce96471d9aecfa8eecc6ff7156584f162959\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/c1/a0/dc44fcf68c857b7ff6be9a97e675e5adf51022eff1169b042f\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.8.0-py3-none-any.whl size=4983 sha256=507e6e62c17133cb5433b9e779dd3a2dcbad220feaf79cf168a493c11dc2add3\n",
            "  Stored in directory: /root/.cache/pip/wheels/c2/75/6f/d42f6e051506f442daeba53ff1e2d21a5f20ef8c411610f2bb\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18912 sha256=1b08b15cccd8e75c9f0decfc9ac43e201825615e558fe696e3b5b4fa8891c1e0\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/b1/a8/5ee00cc137940b2f6fa198212e8f45d813d0e0d9c3a04035a3\n",
            "Successfully built keras-bert keras-transformer keras-embed-sim keras-layer-normalization keras-multi-head keras-pos-embd keras-position-wise-feed-forward keras-self-attention\n",
            "Installing collected packages: keras-self-attention, keras-position-wise-feed-forward, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-embed-sim, keras-transformer, keras-bert\n",
            "Successfully installed keras-bert-0.89.0 keras-embed-sim-0.10.0 keras-layer-normalization-0.16.0 keras-multi-head-0.29.0 keras-pos-embd-0.13.0 keras-position-wise-feed-forward-0.8.0 keras-self-attention-0.51.0 keras-transformer-0.40.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# An optimizer 'keras-radam' import, which is an updated optimizer of 'Adam'\n",
        "# 'radam' is to update learning rate from 0 to target value 'gradually', so training session is more stable\n",
        "!pip install keras-radam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVYNcL3Jcs2E",
        "outputId": "b80a45b4-4969-48f2-faa6-3f87070e1929"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-radam\n",
            "  Downloading keras-radam-0.15.0.tar.gz (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-radam) (1.19.5)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (from keras-radam) (2.7.0)\n",
            "Building wheels for collected packages: keras-radam\n",
            "  Building wheel for keras-radam (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-radam: filename=keras_radam-0.15.0-py3-none-any.whl size=14686 sha256=ab82104b47ddaa6a39ad25f10c4ab4f1f6636f6a40179dbbc6c89a11fec3b639\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/6a/5f/d674f3b7b4d504b03148abd675e3703ba00c31763c04a2fc20\n",
            "Successfully built keras-radam\n",
            "Installing collected packages: keras-radam\n",
            "Successfully installed keras-radam-0.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Other libraries for using bert model in the keras-bert libraries\n",
        "from keras_bert import load_trained_model_from_checkpoint, load_vocabulary\n",
        "from keras_bert import Tokenizer\n",
        "from keras_bert import AdamWarmup, calc_train_steps\n",
        "\n",
        "from keras_radam.training import RAdamOptimizer # Original RAdam changed to RAdamOptimizer\n",
        "# from keras_radam import RAdam\n",
        "# https://github.com/CyberZHG/keras-radam (KERAS-RADAM)"
      ],
      "metadata": {
        "id": "bR6OnIj-cxsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It is easy to use Pandas data framework in Python, so\n",
        "# converting SQuAD json file into Pandas format by a function\n",
        "\n",
        "def squad_json_to_dataframe_train(input_file_path, record_path = ['data','paragraphs','qas','answers'],\n",
        "                           verbose = 1):\n",
        "    \"\"\"\n",
        "    input_file_path: path to the squad json file.\n",
        "    record_path: path to deepest level in json file default value is\n",
        "    ['data','paragraphs','qas','answers']\n",
        "    verbose: 0 to suppress it default is 1\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\"Reading the json file\")    \n",
        "    file = json.loads(open(input_file_path).read())\n",
        "    if verbose:\n",
        "        print(\"processing...\")\n",
        "    # parsing different level's in the json file\n",
        "    js = pd.io.json.json_normalize(file , record_path )\n",
        "    m = pd.io.json.json_normalize(file, record_path[:-1] )\n",
        "    r = pd.io.json.json_normalize(file,record_path[:-2])\n",
        "    \n",
        "    #combining it into single dataframe\n",
        "    idx = np.repeat(r['context'].values, r.qas.str.len())\n",
        "    ndx  = np.repeat(m['id'].values,m['answers'].str.len())\n",
        "    m['context'] = idx\n",
        "    js['q_idx'] = ndx\n",
        "    main = pd.concat([ m[['id','question','context']].set_index('id'),js.set_index('q_idx')],1,sort=False).reset_index()\n",
        "    main['c_id'] = main['context'].factorize()[0]\n",
        "    if verbose:\n",
        "        print(\"shape of the dataframe is {}\".format(main.shape))\n",
        "        print(\"Done\")\n",
        "    return main"
      ],
      "metadata": {
        "id": "LbWH--t7dLls",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "68d9a77f-259a-41df-86c9-7418a508a58e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SQuAD dataset loading in Pandas format\n",
        "train = squad_json_to_dataframe_train(\"train-v1.1.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nttPydzfdpfL",
        "outputId": "4cbfd4e2-8eae-465a-a6ab-04b27cf9b840"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading the json file\n",
            "processing...\n",
            "shape of the dataframe is (87599, 6)\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "i4pOX25hd4D7",
        "outputId": "70b4cca3-9bf0-4d41-df4b-d04ec625d2f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c737cf11-5103-4ea2-a726-7293b90a389b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>text</th>\n",
              "      <th>c_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5733be284776f41900661182</td>\n",
              "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>515</td>\n",
              "      <td>Saint Bernadette Soubirous</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5733be284776f4190066117f</td>\n",
              "      <td>What is in front of the Notre Dame Main Building?</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>188</td>\n",
              "      <td>a copper statue of Christ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5733be284776f41900661180</td>\n",
              "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>279</td>\n",
              "      <td>the Main Building</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5733be284776f41900661181</td>\n",
              "      <td>What is the Grotto at Notre Dame?</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>381</td>\n",
              "      <td>a Marian place of prayer and reflection</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5733be284776f4190066117e</td>\n",
              "      <td>What sits on top of the Main Building at Notre...</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>92</td>\n",
              "      <td>a golden statue of the Virgin Mary</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87594</th>\n",
              "      <td>5735d259012e2f140011a09d</td>\n",
              "      <td>In what US state did Kathmandu first establish...</td>\n",
              "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
              "      <td>229</td>\n",
              "      <td>Oregon</td>\n",
              "      <td>18890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87595</th>\n",
              "      <td>5735d259012e2f140011a09e</td>\n",
              "      <td>What was Yangon previously known as?</td>\n",
              "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
              "      <td>414</td>\n",
              "      <td>Rangoon</td>\n",
              "      <td>18890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87596</th>\n",
              "      <td>5735d259012e2f140011a09f</td>\n",
              "      <td>With what Belorussian city does Kathmandu have...</td>\n",
              "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
              "      <td>476</td>\n",
              "      <td>Minsk</td>\n",
              "      <td>18890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87597</th>\n",
              "      <td>5735d259012e2f140011a0a0</td>\n",
              "      <td>In what year did Kathmandu create its initial ...</td>\n",
              "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
              "      <td>199</td>\n",
              "      <td>1975</td>\n",
              "      <td>18890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87598</th>\n",
              "      <td>5735d259012e2f140011a0a1</td>\n",
              "      <td>What is KMC an initialism of?</td>\n",
              "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
              "      <td>0</td>\n",
              "      <td>Kathmandu Metropolitan City</td>\n",
              "      <td>18890</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>87599 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c737cf11-5103-4ea2-a726-7293b90a389b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c737cf11-5103-4ea2-a726-7293b90a389b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c737cf11-5103-4ea2-a726-7293b90a389b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                          index  ...   c_id\n",
              "0      5733be284776f41900661182  ...      0\n",
              "1      5733be284776f4190066117f  ...      0\n",
              "2      5733be284776f41900661180  ...      0\n",
              "3      5733be284776f41900661181  ...      0\n",
              "4      5733be284776f4190066117e  ...      0\n",
              "...                         ...  ...    ...\n",
              "87594  5735d259012e2f140011a09d  ...  18890\n",
              "87595  5735d259012e2f140011a09e  ...  18890\n",
              "87596  5735d259012e2f140011a09f  ...  18890\n",
              "87597  5735d259012e2f140011a0a0  ...  18890\n",
              "87598  5735d259012e2f140011a0a1  ...  18890\n",
              "\n",
              "[87599 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Deletig the questions having lenth less than 10\n",
        "train = train.loc[train['question'].str.len()>=10].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "Ah8K4JxFd82L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters set up\n",
        "\n",
        "# Input sequence maximum length setup (Question + Context)\n",
        "SEQ_LEN = 384\n",
        "\n",
        "# Batch size (training size at one time)\n",
        "BATCH_SIZE = 12\n",
        "\n",
        "EPOCH = 2\n",
        "\n",
        "# Learning rate is set in very small number like 1.5e-5.\n",
        "# In fine tuning session, Learning rate must be very small \n",
        "# (Optimizer RAdam starts from LR 0 to 1.5e-5)\n",
        "LR = 1.5e-5\n",
        "\n",
        "# Model location designation\n",
        "pretrained_path = \"bert\"\n",
        "# Pretrained hyperparameters like weights\n",
        "checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
        "# Word corpus file location designation\n",
        "vocab_path =  os.path.join(pretrained_path, 'vocab.txt')\n",
        "\n",
        "# Configuration file designation\n",
        "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
        "\n",
        "# Column name of context\n",
        "DATA_COLUMN = \"context\"\n",
        "# Column name of question\n",
        "QUESTION_COLUMN = \"question\"\n",
        "# Column name of answer ('text')\n",
        "TEXT = \"text\""
      ],
      "metadata": {
        "id": "orCUPU80ei-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary Construction for indexing each word\n",
        "token_dict = {}\n",
        "with codecs.open(vocab_path, 'r', 'utf8') as reader:\n",
        "  for line in reader:\n",
        "    token = line.strip()\n",
        "    token_dict[token] = len(token_dict)"
      ],
      "metadata": {
        "id": "DN8dlmZMgt59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUBmaP9AhDNj",
        "outputId": "12fed2c2-fa1f-4f9f-ebf7-dee21ba4b680"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'[PAD]': 0,\n",
              " '[unused0]': 1,\n",
              " '[unused1]': 2,\n",
              " '[unused2]': 3,\n",
              " '[unused3]': 4,\n",
              " '[unused4]': 5,\n",
              " '[unused5]': 6,\n",
              " '[unused6]': 7,\n",
              " '[unused7]': 8,\n",
              " '[unused8]': 9,\n",
              " '[unused9]': 10,\n",
              " '[unused10]': 11,\n",
              " '[unused11]': 12,\n",
              " '[unused12]': 13,\n",
              " '[unused13]': 14,\n",
              " '[unused14]': 15,\n",
              " '[unused15]': 16,\n",
              " '[unused16]': 17,\n",
              " '[unused17]': 18,\n",
              " '[unused18]': 19,\n",
              " '[unused19]': 20,\n",
              " '[unused20]': 21,\n",
              " '[unused21]': 22,\n",
              " '[unused22]': 23,\n",
              " '[unused23]': 24,\n",
              " '[unused24]': 25,\n",
              " '[unused25]': 26,\n",
              " '[unused26]': 27,\n",
              " '[unused27]': 28,\n",
              " '[unused28]': 29,\n",
              " '[unused29]': 30,\n",
              " '[unused30]': 31,\n",
              " '[unused31]': 32,\n",
              " '[unused32]': 33,\n",
              " '[unused33]': 34,\n",
              " '[unused34]': 35,\n",
              " '[unused35]': 36,\n",
              " '[unused36]': 37,\n",
              " '[unused37]': 38,\n",
              " '[unused38]': 39,\n",
              " '[unused39]': 40,\n",
              " '[unused40]': 41,\n",
              " '[unused41]': 42,\n",
              " '[unused42]': 43,\n",
              " '[unused43]': 44,\n",
              " '[unused44]': 45,\n",
              " '[unused45]': 46,\n",
              " '[unused46]': 47,\n",
              " '[unused47]': 48,\n",
              " '[unused48]': 49,\n",
              " '[unused49]': 50,\n",
              " '[unused50]': 51,\n",
              " '[unused51]': 52,\n",
              " '[unused52]': 53,\n",
              " '[unused53]': 54,\n",
              " '[unused54]': 55,\n",
              " '[unused55]': 56,\n",
              " '[unused56]': 57,\n",
              " '[unused57]': 58,\n",
              " '[unused58]': 59,\n",
              " '[unused59]': 60,\n",
              " '[unused60]': 61,\n",
              " '[unused61]': 62,\n",
              " '[unused62]': 63,\n",
              " '[unused63]': 64,\n",
              " '[unused64]': 65,\n",
              " '[unused65]': 66,\n",
              " '[unused66]': 67,\n",
              " '[unused67]': 68,\n",
              " '[unused68]': 69,\n",
              " '[unused69]': 70,\n",
              " '[unused70]': 71,\n",
              " '[unused71]': 72,\n",
              " '[unused72]': 73,\n",
              " '[unused73]': 74,\n",
              " '[unused74]': 75,\n",
              " '[unused75]': 76,\n",
              " '[unused76]': 77,\n",
              " '[unused77]': 78,\n",
              " '[unused78]': 79,\n",
              " '[unused79]': 80,\n",
              " '[unused80]': 81,\n",
              " '[unused81]': 82,\n",
              " '[unused82]': 83,\n",
              " '[unused83]': 84,\n",
              " '[unused84]': 85,\n",
              " '[unused85]': 86,\n",
              " '[unused86]': 87,\n",
              " '[unused87]': 88,\n",
              " '[unused88]': 89,\n",
              " '[unused89]': 90,\n",
              " '[unused90]': 91,\n",
              " '[unused91]': 92,\n",
              " '[unused92]': 93,\n",
              " '[unused93]': 94,\n",
              " '[unused94]': 95,\n",
              " '[unused95]': 96,\n",
              " '[unused96]': 97,\n",
              " '[unused97]': 98,\n",
              " '[unused98]': 99,\n",
              " '[UNK]': 100,\n",
              " '[CLS]': 101,\n",
              " '[SEP]': 102,\n",
              " '[MASK]': 103,\n",
              " '[unused99]': 104,\n",
              " '[unused100]': 105,\n",
              " '[unused101]': 106,\n",
              " '[unused102]': 107,\n",
              " '[unused103]': 108,\n",
              " '[unused104]': 109,\n",
              " '[unused105]': 110,\n",
              " '[unused106]': 111,\n",
              " '[unused107]': 112,\n",
              " '[unused108]': 113,\n",
              " '[unused109]': 114,\n",
              " '[unused110]': 115,\n",
              " '[unused111]': 116,\n",
              " '[unused112]': 117,\n",
              " '[unused113]': 118,\n",
              " '[unused114]': 119,\n",
              " '[unused115]': 120,\n",
              " '[unused116]': 121,\n",
              " '[unused117]': 122,\n",
              " '[unused118]': 123,\n",
              " '[unused119]': 124,\n",
              " '[unused120]': 125,\n",
              " '[unused121]': 126,\n",
              " '[unused122]': 127,\n",
              " '[unused123]': 128,\n",
              " '[unused124]': 129,\n",
              " '[unused125]': 130,\n",
              " '[unused126]': 131,\n",
              " '[unused127]': 132,\n",
              " '[unused128]': 133,\n",
              " '[unused129]': 134,\n",
              " '[unused130]': 135,\n",
              " '[unused131]': 136,\n",
              " '[unused132]': 137,\n",
              " '[unused133]': 138,\n",
              " '[unused134]': 139,\n",
              " '[unused135]': 140,\n",
              " '[unused136]': 141,\n",
              " '[unused137]': 142,\n",
              " '[unused138]': 143,\n",
              " '[unused139]': 144,\n",
              " '[unused140]': 145,\n",
              " '[unused141]': 146,\n",
              " '[unused142]': 147,\n",
              " '[unused143]': 148,\n",
              " '[unused144]': 149,\n",
              " '[unused145]': 150,\n",
              " '[unused146]': 151,\n",
              " '[unused147]': 152,\n",
              " '[unused148]': 153,\n",
              " '[unused149]': 154,\n",
              " '[unused150]': 155,\n",
              " '[unused151]': 156,\n",
              " '[unused152]': 157,\n",
              " '[unused153]': 158,\n",
              " '[unused154]': 159,\n",
              " '[unused155]': 160,\n",
              " '[unused156]': 161,\n",
              " '[unused157]': 162,\n",
              " '[unused158]': 163,\n",
              " '[unused159]': 164,\n",
              " '[unused160]': 165,\n",
              " '[unused161]': 166,\n",
              " '[unused162]': 167,\n",
              " '[unused163]': 168,\n",
              " '[unused164]': 169,\n",
              " '[unused165]': 170,\n",
              " '[unused166]': 171,\n",
              " '[unused167]': 172,\n",
              " '[unused168]': 173,\n",
              " '[unused169]': 174,\n",
              " '[unused170]': 175,\n",
              " '[unused171]': 176,\n",
              " '[unused172]': 177,\n",
              " '[unused173]': 178,\n",
              " '[unused174]': 179,\n",
              " '[unused175]': 180,\n",
              " '[unused176]': 181,\n",
              " '[unused177]': 182,\n",
              " '[unused178]': 183,\n",
              " '[unused179]': 184,\n",
              " '[unused180]': 185,\n",
              " '[unused181]': 186,\n",
              " '[unused182]': 187,\n",
              " '[unused183]': 188,\n",
              " '[unused184]': 189,\n",
              " '[unused185]': 190,\n",
              " '[unused186]': 191,\n",
              " '[unused187]': 192,\n",
              " '[unused188]': 193,\n",
              " '[unused189]': 194,\n",
              " '[unused190]': 195,\n",
              " '[unused191]': 196,\n",
              " '[unused192]': 197,\n",
              " '[unused193]': 198,\n",
              " '[unused194]': 199,\n",
              " '[unused195]': 200,\n",
              " '[unused196]': 201,\n",
              " '[unused197]': 202,\n",
              " '[unused198]': 203,\n",
              " '[unused199]': 204,\n",
              " '[unused200]': 205,\n",
              " '[unused201]': 206,\n",
              " '[unused202]': 207,\n",
              " '[unused203]': 208,\n",
              " '[unused204]': 209,\n",
              " '[unused205]': 210,\n",
              " '[unused206]': 211,\n",
              " '[unused207]': 212,\n",
              " '[unused208]': 213,\n",
              " '[unused209]': 214,\n",
              " '[unused210]': 215,\n",
              " '[unused211]': 216,\n",
              " '[unused212]': 217,\n",
              " '[unused213]': 218,\n",
              " '[unused214]': 219,\n",
              " '[unused215]': 220,\n",
              " '[unused216]': 221,\n",
              " '[unused217]': 222,\n",
              " '[unused218]': 223,\n",
              " '[unused219]': 224,\n",
              " '[unused220]': 225,\n",
              " '[unused221]': 226,\n",
              " '[unused222]': 227,\n",
              " '[unused223]': 228,\n",
              " '[unused224]': 229,\n",
              " '[unused225]': 230,\n",
              " '[unused226]': 231,\n",
              " '[unused227]': 232,\n",
              " '[unused228]': 233,\n",
              " '[unused229]': 234,\n",
              " '[unused230]': 235,\n",
              " '[unused231]': 236,\n",
              " '[unused232]': 237,\n",
              " '[unused233]': 238,\n",
              " '[unused234]': 239,\n",
              " '[unused235]': 240,\n",
              " '[unused236]': 241,\n",
              " '[unused237]': 242,\n",
              " '[unused238]': 243,\n",
              " '[unused239]': 244,\n",
              " '[unused240]': 245,\n",
              " '[unused241]': 246,\n",
              " '[unused242]': 247,\n",
              " '[unused243]': 248,\n",
              " '[unused244]': 249,\n",
              " '[unused245]': 250,\n",
              " '[unused246]': 251,\n",
              " '[unused247]': 252,\n",
              " '[unused248]': 253,\n",
              " '[unused249]': 254,\n",
              " '[unused250]': 255,\n",
              " '[unused251]': 256,\n",
              " '[unused252]': 257,\n",
              " '[unused253]': 258,\n",
              " '[unused254]': 259,\n",
              " '[unused255]': 260,\n",
              " '[unused256]': 261,\n",
              " '[unused257]': 262,\n",
              " '[unused258]': 263,\n",
              " '[unused259]': 264,\n",
              " '[unused260]': 265,\n",
              " '[unused261]': 266,\n",
              " '[unused262]': 267,\n",
              " '[unused263]': 268,\n",
              " '[unused264]': 269,\n",
              " '[unused265]': 270,\n",
              " '[unused266]': 271,\n",
              " '[unused267]': 272,\n",
              " '[unused268]': 273,\n",
              " '[unused269]': 274,\n",
              " '[unused270]': 275,\n",
              " '[unused271]': 276,\n",
              " '[unused272]': 277,\n",
              " '[unused273]': 278,\n",
              " '[unused274]': 279,\n",
              " '[unused275]': 280,\n",
              " '[unused276]': 281,\n",
              " '[unused277]': 282,\n",
              " '[unused278]': 283,\n",
              " '[unused279]': 284,\n",
              " '[unused280]': 285,\n",
              " '[unused281]': 286,\n",
              " '[unused282]': 287,\n",
              " '[unused283]': 288,\n",
              " '[unused284]': 289,\n",
              " '[unused285]': 290,\n",
              " '[unused286]': 291,\n",
              " '[unused287]': 292,\n",
              " '[unused288]': 293,\n",
              " '[unused289]': 294,\n",
              " '[unused290]': 295,\n",
              " '[unused291]': 296,\n",
              " '[unused292]': 297,\n",
              " '[unused293]': 298,\n",
              " '[unused294]': 299,\n",
              " '[unused295]': 300,\n",
              " '[unused296]': 301,\n",
              " '[unused297]': 302,\n",
              " '[unused298]': 303,\n",
              " '[unused299]': 304,\n",
              " '[unused300]': 305,\n",
              " '[unused301]': 306,\n",
              " '[unused302]': 307,\n",
              " '[unused303]': 308,\n",
              " '[unused304]': 309,\n",
              " '[unused305]': 310,\n",
              " '[unused306]': 311,\n",
              " '[unused307]': 312,\n",
              " '[unused308]': 313,\n",
              " '[unused309]': 314,\n",
              " '[unused310]': 315,\n",
              " '[unused311]': 316,\n",
              " '[unused312]': 317,\n",
              " '[unused313]': 318,\n",
              " '[unused314]': 319,\n",
              " '[unused315]': 320,\n",
              " '[unused316]': 321,\n",
              " '[unused317]': 322,\n",
              " '[unused318]': 323,\n",
              " '[unused319]': 324,\n",
              " '[unused320]': 325,\n",
              " '[unused321]': 326,\n",
              " '[unused322]': 327,\n",
              " '[unused323]': 328,\n",
              " '[unused324]': 329,\n",
              " '[unused325]': 330,\n",
              " '[unused326]': 331,\n",
              " '[unused327]': 332,\n",
              " '[unused328]': 333,\n",
              " '[unused329]': 334,\n",
              " '[unused330]': 335,\n",
              " '[unused331]': 336,\n",
              " '[unused332]': 337,\n",
              " '[unused333]': 338,\n",
              " '[unused334]': 339,\n",
              " '[unused335]': 340,\n",
              " '[unused336]': 341,\n",
              " '[unused337]': 342,\n",
              " '[unused338]': 343,\n",
              " '[unused339]': 344,\n",
              " '[unused340]': 345,\n",
              " '[unused341]': 346,\n",
              " '[unused342]': 347,\n",
              " '[unused343]': 348,\n",
              " '[unused344]': 349,\n",
              " '[unused345]': 350,\n",
              " '[unused346]': 351,\n",
              " '[unused347]': 352,\n",
              " '[unused348]': 353,\n",
              " '[unused349]': 354,\n",
              " '[unused350]': 355,\n",
              " '[unused351]': 356,\n",
              " '[unused352]': 357,\n",
              " '[unused353]': 358,\n",
              " '[unused354]': 359,\n",
              " '[unused355]': 360,\n",
              " '[unused356]': 361,\n",
              " '[unused357]': 362,\n",
              " '[unused358]': 363,\n",
              " '[unused359]': 364,\n",
              " '[unused360]': 365,\n",
              " '[unused361]': 366,\n",
              " '[unused362]': 367,\n",
              " '[unused363]': 368,\n",
              " '[unused364]': 369,\n",
              " '[unused365]': 370,\n",
              " '[unused366]': 371,\n",
              " '[unused367]': 372,\n",
              " '[unused368]': 373,\n",
              " '[unused369]': 374,\n",
              " '[unused370]': 375,\n",
              " '[unused371]': 376,\n",
              " '[unused372]': 377,\n",
              " '[unused373]': 378,\n",
              " '[unused374]': 379,\n",
              " '[unused375]': 380,\n",
              " '[unused376]': 381,\n",
              " '[unused377]': 382,\n",
              " '[unused378]': 383,\n",
              " '[unused379]': 384,\n",
              " '[unused380]': 385,\n",
              " '[unused381]': 386,\n",
              " '[unused382]': 387,\n",
              " '[unused383]': 388,\n",
              " '[unused384]': 389,\n",
              " '[unused385]': 390,\n",
              " '[unused386]': 391,\n",
              " '[unused387]': 392,\n",
              " '[unused388]': 393,\n",
              " '[unused389]': 394,\n",
              " '[unused390]': 395,\n",
              " '[unused391]': 396,\n",
              " '[unused392]': 397,\n",
              " '[unused393]': 398,\n",
              " '[unused394]': 399,\n",
              " '[unused395]': 400,\n",
              " '[unused396]': 401,\n",
              " '[unused397]': 402,\n",
              " '[unused398]': 403,\n",
              " '[unused399]': 404,\n",
              " '[unused400]': 405,\n",
              " '[unused401]': 406,\n",
              " '[unused402]': 407,\n",
              " '[unused403]': 408,\n",
              " '[unused404]': 409,\n",
              " '[unused405]': 410,\n",
              " '[unused406]': 411,\n",
              " '[unused407]': 412,\n",
              " '[unused408]': 413,\n",
              " '[unused409]': 414,\n",
              " '[unused410]': 415,\n",
              " '[unused411]': 416,\n",
              " '[unused412]': 417,\n",
              " '[unused413]': 418,\n",
              " '[unused414]': 419,\n",
              " '[unused415]': 420,\n",
              " '[unused416]': 421,\n",
              " '[unused417]': 422,\n",
              " '[unused418]': 423,\n",
              " '[unused419]': 424,\n",
              " '[unused420]': 425,\n",
              " '[unused421]': 426,\n",
              " '[unused422]': 427,\n",
              " '[unused423]': 428,\n",
              " '[unused424]': 429,\n",
              " '[unused425]': 430,\n",
              " '[unused426]': 431,\n",
              " '[unused427]': 432,\n",
              " '[unused428]': 433,\n",
              " '[unused429]': 434,\n",
              " '[unused430]': 435,\n",
              " '[unused431]': 436,\n",
              " '[unused432]': 437,\n",
              " '[unused433]': 438,\n",
              " '[unused434]': 439,\n",
              " '[unused435]': 440,\n",
              " '[unused436]': 441,\n",
              " '[unused437]': 442,\n",
              " '[unused438]': 443,\n",
              " '[unused439]': 444,\n",
              " '[unused440]': 445,\n",
              " '[unused441]': 446,\n",
              " '[unused442]': 447,\n",
              " '[unused443]': 448,\n",
              " '[unused444]': 449,\n",
              " '[unused445]': 450,\n",
              " '[unused446]': 451,\n",
              " '[unused447]': 452,\n",
              " '[unused448]': 453,\n",
              " '[unused449]': 454,\n",
              " '[unused450]': 455,\n",
              " '[unused451]': 456,\n",
              " '[unused452]': 457,\n",
              " '[unused453]': 458,\n",
              " '[unused454]': 459,\n",
              " '[unused455]': 460,\n",
              " '[unused456]': 461,\n",
              " '[unused457]': 462,\n",
              " '[unused458]': 463,\n",
              " '[unused459]': 464,\n",
              " '[unused460]': 465,\n",
              " '[unused461]': 466,\n",
              " '[unused462]': 467,\n",
              " '[unused463]': 468,\n",
              " '[unused464]': 469,\n",
              " '[unused465]': 470,\n",
              " '[unused466]': 471,\n",
              " '[unused467]': 472,\n",
              " '[unused468]': 473,\n",
              " '[unused469]': 474,\n",
              " '[unused470]': 475,\n",
              " '[unused471]': 476,\n",
              " '[unused472]': 477,\n",
              " '[unused473]': 478,\n",
              " '[unused474]': 479,\n",
              " '[unused475]': 480,\n",
              " '[unused476]': 481,\n",
              " '[unused477]': 482,\n",
              " '[unused478]': 483,\n",
              " '[unused479]': 484,\n",
              " '[unused480]': 485,\n",
              " '[unused481]': 486,\n",
              " '[unused482]': 487,\n",
              " '[unused483]': 488,\n",
              " '[unused484]': 489,\n",
              " '[unused485]': 490,\n",
              " '[unused486]': 491,\n",
              " '[unused487]': 492,\n",
              " '[unused488]': 493,\n",
              " '[unused489]': 494,\n",
              " '[unused490]': 495,\n",
              " '[unused491]': 496,\n",
              " '[unused492]': 497,\n",
              " '[unused493]': 498,\n",
              " '[unused494]': 499,\n",
              " '[unused495]': 500,\n",
              " '[unused496]': 501,\n",
              " '[unused497]': 502,\n",
              " '[unused498]': 503,\n",
              " '[unused499]': 504,\n",
              " '[unused500]': 505,\n",
              " '[unused501]': 506,\n",
              " '[unused502]': 507,\n",
              " '[unused503]': 508,\n",
              " '[unused504]': 509,\n",
              " '[unused505]': 510,\n",
              " '[unused506]': 511,\n",
              " '[unused507]': 512,\n",
              " '[unused508]': 513,\n",
              " '[unused509]': 514,\n",
              " '[unused510]': 515,\n",
              " '[unused511]': 516,\n",
              " '[unused512]': 517,\n",
              " '[unused513]': 518,\n",
              " '[unused514]': 519,\n",
              " '[unused515]': 520,\n",
              " '[unused516]': 521,\n",
              " '[unused517]': 522,\n",
              " '[unused518]': 523,\n",
              " '[unused519]': 524,\n",
              " '[unused520]': 525,\n",
              " '[unused521]': 526,\n",
              " '[unused522]': 527,\n",
              " '[unused523]': 528,\n",
              " '[unused524]': 529,\n",
              " '[unused525]': 530,\n",
              " '[unused526]': 531,\n",
              " '[unused527]': 532,\n",
              " '[unused528]': 533,\n",
              " '[unused529]': 534,\n",
              " '[unused530]': 535,\n",
              " '[unused531]': 536,\n",
              " '[unused532]': 537,\n",
              " '[unused533]': 538,\n",
              " '[unused534]': 539,\n",
              " '[unused535]': 540,\n",
              " '[unused536]': 541,\n",
              " '[unused537]': 542,\n",
              " '[unused538]': 543,\n",
              " '[unused539]': 544,\n",
              " '[unused540]': 545,\n",
              " '[unused541]': 546,\n",
              " '[unused542]': 547,\n",
              " '[unused543]': 548,\n",
              " '[unused544]': 549,\n",
              " '[unused545]': 550,\n",
              " '[unused546]': 551,\n",
              " '[unused547]': 552,\n",
              " '[unused548]': 553,\n",
              " '[unused549]': 554,\n",
              " '[unused550]': 555,\n",
              " '[unused551]': 556,\n",
              " '[unused552]': 557,\n",
              " '[unused553]': 558,\n",
              " '[unused554]': 559,\n",
              " '[unused555]': 560,\n",
              " '[unused556]': 561,\n",
              " '[unused557]': 562,\n",
              " '[unused558]': 563,\n",
              " '[unused559]': 564,\n",
              " '[unused560]': 565,\n",
              " '[unused561]': 566,\n",
              " '[unused562]': 567,\n",
              " '[unused563]': 568,\n",
              " '[unused564]': 569,\n",
              " '[unused565]': 570,\n",
              " '[unused566]': 571,\n",
              " '[unused567]': 572,\n",
              " '[unused568]': 573,\n",
              " '[unused569]': 574,\n",
              " '[unused570]': 575,\n",
              " '[unused571]': 576,\n",
              " '[unused572]': 577,\n",
              " '[unused573]': 578,\n",
              " '[unused574]': 579,\n",
              " '[unused575]': 580,\n",
              " '[unused576]': 581,\n",
              " '[unused577]': 582,\n",
              " '[unused578]': 583,\n",
              " '[unused579]': 584,\n",
              " '[unused580]': 585,\n",
              " '[unused581]': 586,\n",
              " '[unused582]': 587,\n",
              " '[unused583]': 588,\n",
              " '[unused584]': 589,\n",
              " '[unused585]': 590,\n",
              " '[unused586]': 591,\n",
              " '[unused587]': 592,\n",
              " '[unused588]': 593,\n",
              " '[unused589]': 594,\n",
              " '[unused590]': 595,\n",
              " '[unused591]': 596,\n",
              " '[unused592]': 597,\n",
              " '[unused593]': 598,\n",
              " '[unused594]': 599,\n",
              " '[unused595]': 600,\n",
              " '[unused596]': 601,\n",
              " '[unused597]': 602,\n",
              " '[unused598]': 603,\n",
              " '[unused599]': 604,\n",
              " '[unused600]': 605,\n",
              " '[unused601]': 606,\n",
              " '[unused602]': 607,\n",
              " '[unused603]': 608,\n",
              " '[unused604]': 609,\n",
              " '[unused605]': 610,\n",
              " '[unused606]': 611,\n",
              " '[unused607]': 612,\n",
              " '[unused608]': 613,\n",
              " '[unused609]': 614,\n",
              " '[unused610]': 615,\n",
              " '[unused611]': 616,\n",
              " '[unused612]': 617,\n",
              " '[unused613]': 618,\n",
              " '[unused614]': 619,\n",
              " '[unused615]': 620,\n",
              " '[unused616]': 621,\n",
              " '[unused617]': 622,\n",
              " '[unused618]': 623,\n",
              " '[unused619]': 624,\n",
              " '[unused620]': 625,\n",
              " '[unused621]': 626,\n",
              " '[unused622]': 627,\n",
              " '[unused623]': 628,\n",
              " '[unused624]': 629,\n",
              " '[unused625]': 630,\n",
              " '[unused626]': 631,\n",
              " '[unused627]': 632,\n",
              " '[unused628]': 633,\n",
              " '[unused629]': 634,\n",
              " '[unused630]': 635,\n",
              " '[unused631]': 636,\n",
              " '[unused632]': 637,\n",
              " '[unused633]': 638,\n",
              " '[unused634]': 639,\n",
              " '[unused635]': 640,\n",
              " '[unused636]': 641,\n",
              " '[unused637]': 642,\n",
              " '[unused638]': 643,\n",
              " '[unused639]': 644,\n",
              " '[unused640]': 645,\n",
              " '[unused641]': 646,\n",
              " '[unused642]': 647,\n",
              " '[unused643]': 648,\n",
              " '[unused644]': 649,\n",
              " '[unused645]': 650,\n",
              " '[unused646]': 651,\n",
              " '[unused647]': 652,\n",
              " '[unused648]': 653,\n",
              " '[unused649]': 654,\n",
              " '[unused650]': 655,\n",
              " '[unused651]': 656,\n",
              " '[unused652]': 657,\n",
              " '[unused653]': 658,\n",
              " '[unused654]': 659,\n",
              " '[unused655]': 660,\n",
              " '[unused656]': 661,\n",
              " '[unused657]': 662,\n",
              " '[unused658]': 663,\n",
              " '[unused659]': 664,\n",
              " '[unused660]': 665,\n",
              " '[unused661]': 666,\n",
              " '[unused662]': 667,\n",
              " '[unused663]': 668,\n",
              " '[unused664]': 669,\n",
              " '[unused665]': 670,\n",
              " '[unused666]': 671,\n",
              " '[unused667]': 672,\n",
              " '[unused668]': 673,\n",
              " '[unused669]': 674,\n",
              " '[unused670]': 675,\n",
              " '[unused671]': 676,\n",
              " '[unused672]': 677,\n",
              " '[unused673]': 678,\n",
              " '[unused674]': 679,\n",
              " '[unused675]': 680,\n",
              " '[unused676]': 681,\n",
              " '[unused677]': 682,\n",
              " '[unused678]': 683,\n",
              " '[unused679]': 684,\n",
              " '[unused680]': 685,\n",
              " '[unused681]': 686,\n",
              " '[unused682]': 687,\n",
              " '[unused683]': 688,\n",
              " '[unused684]': 689,\n",
              " '[unused685]': 690,\n",
              " '[unused686]': 691,\n",
              " '[unused687]': 692,\n",
              " '[unused688]': 693,\n",
              " '[unused689]': 694,\n",
              " '[unused690]': 695,\n",
              " '[unused691]': 696,\n",
              " '[unused692]': 697,\n",
              " '[unused693]': 698,\n",
              " '[unused694]': 699,\n",
              " '[unused695]': 700,\n",
              " '[unused696]': 701,\n",
              " '[unused697]': 702,\n",
              " '[unused698]': 703,\n",
              " '[unused699]': 704,\n",
              " '[unused700]': 705,\n",
              " '[unused701]': 706,\n",
              " '[unused702]': 707,\n",
              " '[unused703]': 708,\n",
              " '[unused704]': 709,\n",
              " '[unused705]': 710,\n",
              " '[unused706]': 711,\n",
              " '[unused707]': 712,\n",
              " '[unused708]': 713,\n",
              " '[unused709]': 714,\n",
              " '[unused710]': 715,\n",
              " '[unused711]': 716,\n",
              " '[unused712]': 717,\n",
              " '[unused713]': 718,\n",
              " '[unused714]': 719,\n",
              " '[unused715]': 720,\n",
              " '[unused716]': 721,\n",
              " '[unused717]': 722,\n",
              " '[unused718]': 723,\n",
              " '[unused719]': 724,\n",
              " '[unused720]': 725,\n",
              " '[unused721]': 726,\n",
              " '[unused722]': 727,\n",
              " '[unused723]': 728,\n",
              " '[unused724]': 729,\n",
              " '[unused725]': 730,\n",
              " '[unused726]': 731,\n",
              " '[unused727]': 732,\n",
              " '[unused728]': 733,\n",
              " '[unused729]': 734,\n",
              " '[unused730]': 735,\n",
              " '[unused731]': 736,\n",
              " '[unused732]': 737,\n",
              " '[unused733]': 738,\n",
              " '[unused734]': 739,\n",
              " '[unused735]': 740,\n",
              " '[unused736]': 741,\n",
              " '[unused737]': 742,\n",
              " '[unused738]': 743,\n",
              " '[unused739]': 744,\n",
              " '[unused740]': 745,\n",
              " '[unused741]': 746,\n",
              " '[unused742]': 747,\n",
              " '[unused743]': 748,\n",
              " '[unused744]': 749,\n",
              " '[unused745]': 750,\n",
              " '[unused746]': 751,\n",
              " '[unused747]': 752,\n",
              " '[unused748]': 753,\n",
              " '[unused749]': 754,\n",
              " '[unused750]': 755,\n",
              " '[unused751]': 756,\n",
              " '[unused752]': 757,\n",
              " '[unused753]': 758,\n",
              " '[unused754]': 759,\n",
              " '[unused755]': 760,\n",
              " '[unused756]': 761,\n",
              " '[unused757]': 762,\n",
              " '[unused758]': 763,\n",
              " '[unused759]': 764,\n",
              " '[unused760]': 765,\n",
              " '[unused761]': 766,\n",
              " '[unused762]': 767,\n",
              " '[unused763]': 768,\n",
              " '[unused764]': 769,\n",
              " '[unused765]': 770,\n",
              " '[unused766]': 771,\n",
              " '[unused767]': 772,\n",
              " '[unused768]': 773,\n",
              " '[unused769]': 774,\n",
              " '[unused770]': 775,\n",
              " '[unused771]': 776,\n",
              " '[unused772]': 777,\n",
              " '[unused773]': 778,\n",
              " '[unused774]': 779,\n",
              " '[unused775]': 780,\n",
              " '[unused776]': 781,\n",
              " '[unused777]': 782,\n",
              " '[unused778]': 783,\n",
              " '[unused779]': 784,\n",
              " '[unused780]': 785,\n",
              " '[unused781]': 786,\n",
              " '[unused782]': 787,\n",
              " '[unused783]': 788,\n",
              " '[unused784]': 789,\n",
              " '[unused785]': 790,\n",
              " '[unused786]': 791,\n",
              " '[unused787]': 792,\n",
              " '[unused788]': 793,\n",
              " '[unused789]': 794,\n",
              " '[unused790]': 795,\n",
              " '[unused791]': 796,\n",
              " '[unused792]': 797,\n",
              " '[unused793]': 798,\n",
              " '[unused794]': 799,\n",
              " '[unused795]': 800,\n",
              " '[unused796]': 801,\n",
              " '[unused797]': 802,\n",
              " '[unused798]': 803,\n",
              " '[unused799]': 804,\n",
              " '[unused800]': 805,\n",
              " '[unused801]': 806,\n",
              " '[unused802]': 807,\n",
              " '[unused803]': 808,\n",
              " '[unused804]': 809,\n",
              " '[unused805]': 810,\n",
              " '[unused806]': 811,\n",
              " '[unused807]': 812,\n",
              " '[unused808]': 813,\n",
              " '[unused809]': 814,\n",
              " '[unused810]': 815,\n",
              " '[unused811]': 816,\n",
              " '[unused812]': 817,\n",
              " '[unused813]': 818,\n",
              " '[unused814]': 819,\n",
              " '[unused815]': 820,\n",
              " '[unused816]': 821,\n",
              " '[unused817]': 822,\n",
              " '[unused818]': 823,\n",
              " '[unused819]': 824,\n",
              " '[unused820]': 825,\n",
              " '[unused821]': 826,\n",
              " '[unused822]': 827,\n",
              " '[unused823]': 828,\n",
              " '[unused824]': 829,\n",
              " '[unused825]': 830,\n",
              " '[unused826]': 831,\n",
              " '[unused827]': 832,\n",
              " '[unused828]': 833,\n",
              " '[unused829]': 834,\n",
              " '[unused830]': 835,\n",
              " '[unused831]': 836,\n",
              " '[unused832]': 837,\n",
              " '[unused833]': 838,\n",
              " '[unused834]': 839,\n",
              " '[unused835]': 840,\n",
              " '[unused836]': 841,\n",
              " '[unused837]': 842,\n",
              " '[unused838]': 843,\n",
              " '[unused839]': 844,\n",
              " '[unused840]': 845,\n",
              " '[unused841]': 846,\n",
              " '[unused842]': 847,\n",
              " '[unused843]': 848,\n",
              " '[unused844]': 849,\n",
              " '[unused845]': 850,\n",
              " '[unused846]': 851,\n",
              " '[unused847]': 852,\n",
              " '[unused848]': 853,\n",
              " '[unused849]': 854,\n",
              " '[unused850]': 855,\n",
              " '[unused851]': 856,\n",
              " '[unused852]': 857,\n",
              " '[unused853]': 858,\n",
              " '[unused854]': 859,\n",
              " '[unused855]': 860,\n",
              " '[unused856]': 861,\n",
              " '[unused857]': 862,\n",
              " '[unused858]': 863,\n",
              " '[unused859]': 864,\n",
              " '[unused860]': 865,\n",
              " '[unused861]': 866,\n",
              " '[unused862]': 867,\n",
              " '[unused863]': 868,\n",
              " '[unused864]': 869,\n",
              " '[unused865]': 870,\n",
              " '[unused866]': 871,\n",
              " '[unused867]': 872,\n",
              " '[unused868]': 873,\n",
              " '[unused869]': 874,\n",
              " '[unused870]': 875,\n",
              " '[unused871]': 876,\n",
              " '[unused872]': 877,\n",
              " '[unused873]': 878,\n",
              " '[unused874]': 879,\n",
              " '[unused875]': 880,\n",
              " '[unused876]': 881,\n",
              " '[unused877]': 882,\n",
              " '[unused878]': 883,\n",
              " '[unused879]': 884,\n",
              " '[unused880]': 885,\n",
              " '[unused881]': 886,\n",
              " '[unused882]': 887,\n",
              " '[unused883]': 888,\n",
              " '[unused884]': 889,\n",
              " '[unused885]': 890,\n",
              " '[unused886]': 891,\n",
              " '[unused887]': 892,\n",
              " '[unused888]': 893,\n",
              " '[unused889]': 894,\n",
              " '[unused890]': 895,\n",
              " '[unused891]': 896,\n",
              " '[unused892]': 897,\n",
              " '[unused893]': 898,\n",
              " '[unused894]': 899,\n",
              " '[unused895]': 900,\n",
              " '[unused896]': 901,\n",
              " '[unused897]': 902,\n",
              " '[unused898]': 903,\n",
              " '[unused899]': 904,\n",
              " '[unused900]': 905,\n",
              " '[unused901]': 906,\n",
              " '[unused902]': 907,\n",
              " '[unused903]': 908,\n",
              " '[unused904]': 909,\n",
              " '[unused905]': 910,\n",
              " '[unused906]': 911,\n",
              " '[unused907]': 912,\n",
              " '[unused908]': 913,\n",
              " '[unused909]': 914,\n",
              " '[unused910]': 915,\n",
              " '[unused911]': 916,\n",
              " '[unused912]': 917,\n",
              " '[unused913]': 918,\n",
              " '[unused914]': 919,\n",
              " '[unused915]': 920,\n",
              " '[unused916]': 921,\n",
              " '[unused917]': 922,\n",
              " '[unused918]': 923,\n",
              " '[unused919]': 924,\n",
              " '[unused920]': 925,\n",
              " '[unused921]': 926,\n",
              " '[unused922]': 927,\n",
              " '[unused923]': 928,\n",
              " '[unused924]': 929,\n",
              " '[unused925]': 930,\n",
              " '[unused926]': 931,\n",
              " '[unused927]': 932,\n",
              " '[unused928]': 933,\n",
              " '[unused929]': 934,\n",
              " '[unused930]': 935,\n",
              " '[unused931]': 936,\n",
              " '[unused932]': 937,\n",
              " '[unused933]': 938,\n",
              " '[unused934]': 939,\n",
              " '[unused935]': 940,\n",
              " '[unused936]': 941,\n",
              " '[unused937]': 942,\n",
              " '[unused938]': 943,\n",
              " '[unused939]': 944,\n",
              " '[unused940]': 945,\n",
              " '[unused941]': 946,\n",
              " '[unused942]': 947,\n",
              " '[unused943]': 948,\n",
              " '[unused944]': 949,\n",
              " '[unused945]': 950,\n",
              " '[unused946]': 951,\n",
              " '[unused947]': 952,\n",
              " '[unused948]': 953,\n",
              " '[unused949]': 954,\n",
              " '[unused950]': 955,\n",
              " '[unused951]': 956,\n",
              " '[unused952]': 957,\n",
              " '[unused953]': 958,\n",
              " '[unused954]': 959,\n",
              " '[unused955]': 960,\n",
              " '[unused956]': 961,\n",
              " '[unused957]': 962,\n",
              " '[unused958]': 963,\n",
              " '[unused959]': 964,\n",
              " '[unused960]': 965,\n",
              " '[unused961]': 966,\n",
              " '[unused962]': 967,\n",
              " '[unused963]': 968,\n",
              " '[unused964]': 969,\n",
              " '[unused965]': 970,\n",
              " '[unused966]': 971,\n",
              " '[unused967]': 972,\n",
              " '[unused968]': 973,\n",
              " '[unused969]': 974,\n",
              " '[unused970]': 975,\n",
              " '[unused971]': 976,\n",
              " '[unused972]': 977,\n",
              " '[unused973]': 978,\n",
              " '[unused974]': 979,\n",
              " '[unused975]': 980,\n",
              " '[unused976]': 981,\n",
              " '[unused977]': 982,\n",
              " '[unused978]': 983,\n",
              " '[unused979]': 984,\n",
              " '[unused980]': 985,\n",
              " '[unused981]': 986,\n",
              " '[unused982]': 987,\n",
              " '[unused983]': 988,\n",
              " '[unused984]': 989,\n",
              " '[unused985]': 990,\n",
              " '[unused986]': 991,\n",
              " '[unused987]': 992,\n",
              " '[unused988]': 993,\n",
              " '[unused989]': 994,\n",
              " '[unused990]': 995,\n",
              " '[unused991]': 996,\n",
              " '[unused992]': 997,\n",
              " '[unused993]': 998,\n",
              " '!': 999,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer Construction\n",
        "tokenizer = Tokenizer(token_dict)"
      ],
      "metadata": {
        "id": "SgtvhAdMhHFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if the tokenizer works well\n",
        "print(tokenizer.tokenize(\"Keras is really fun.\"))\n",
        "print(tokenizer.tokenize(\"We can manipulate AI.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxnQcoxihXGs",
        "outputId": "15eb258d-6f11-4fd4-f316-e5627634c061"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'ke', '##ras', 'is', 'really', 'fun', '.', '[SEP]']\n",
            "['[CLS]', 'we', 'can', 'manipulate', 'ai', '.', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For later use, updated dictionary constructed (number - word order)\n",
        "reverse_token_dict = {v : k for k, v in token_dict.items()}"
      ],
      "metadata": {
        "id": "mcr611L1hlv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_token_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IMkEXx5iCZ7",
        "outputId": "808d9bd9-6c2c-444a-ae39-19d84baec5d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '[PAD]',\n",
              " 1: '[unused0]',\n",
              " 2: '[unused1]',\n",
              " 3: '[unused2]',\n",
              " 4: '[unused3]',\n",
              " 5: '[unused4]',\n",
              " 6: '[unused5]',\n",
              " 7: '[unused6]',\n",
              " 8: '[unused7]',\n",
              " 9: '[unused8]',\n",
              " 10: '[unused9]',\n",
              " 11: '[unused10]',\n",
              " 12: '[unused11]',\n",
              " 13: '[unused12]',\n",
              " 14: '[unused13]',\n",
              " 15: '[unused14]',\n",
              " 16: '[unused15]',\n",
              " 17: '[unused16]',\n",
              " 18: '[unused17]',\n",
              " 19: '[unused18]',\n",
              " 20: '[unused19]',\n",
              " 21: '[unused20]',\n",
              " 22: '[unused21]',\n",
              " 23: '[unused22]',\n",
              " 24: '[unused23]',\n",
              " 25: '[unused24]',\n",
              " 26: '[unused25]',\n",
              " 27: '[unused26]',\n",
              " 28: '[unused27]',\n",
              " 29: '[unused28]',\n",
              " 30: '[unused29]',\n",
              " 31: '[unused30]',\n",
              " 32: '[unused31]',\n",
              " 33: '[unused32]',\n",
              " 34: '[unused33]',\n",
              " 35: '[unused34]',\n",
              " 36: '[unused35]',\n",
              " 37: '[unused36]',\n",
              " 38: '[unused37]',\n",
              " 39: '[unused38]',\n",
              " 40: '[unused39]',\n",
              " 41: '[unused40]',\n",
              " 42: '[unused41]',\n",
              " 43: '[unused42]',\n",
              " 44: '[unused43]',\n",
              " 45: '[unused44]',\n",
              " 46: '[unused45]',\n",
              " 47: '[unused46]',\n",
              " 48: '[unused47]',\n",
              " 49: '[unused48]',\n",
              " 50: '[unused49]',\n",
              " 51: '[unused50]',\n",
              " 52: '[unused51]',\n",
              " 53: '[unused52]',\n",
              " 54: '[unused53]',\n",
              " 55: '[unused54]',\n",
              " 56: '[unused55]',\n",
              " 57: '[unused56]',\n",
              " 58: '[unused57]',\n",
              " 59: '[unused58]',\n",
              " 60: '[unused59]',\n",
              " 61: '[unused60]',\n",
              " 62: '[unused61]',\n",
              " 63: '[unused62]',\n",
              " 64: '[unused63]',\n",
              " 65: '[unused64]',\n",
              " 66: '[unused65]',\n",
              " 67: '[unused66]',\n",
              " 68: '[unused67]',\n",
              " 69: '[unused68]',\n",
              " 70: '[unused69]',\n",
              " 71: '[unused70]',\n",
              " 72: '[unused71]',\n",
              " 73: '[unused72]',\n",
              " 74: '[unused73]',\n",
              " 75: '[unused74]',\n",
              " 76: '[unused75]',\n",
              " 77: '[unused76]',\n",
              " 78: '[unused77]',\n",
              " 79: '[unused78]',\n",
              " 80: '[unused79]',\n",
              " 81: '[unused80]',\n",
              " 82: '[unused81]',\n",
              " 83: '[unused82]',\n",
              " 84: '[unused83]',\n",
              " 85: '[unused84]',\n",
              " 86: '[unused85]',\n",
              " 87: '[unused86]',\n",
              " 88: '[unused87]',\n",
              " 89: '[unused88]',\n",
              " 90: '[unused89]',\n",
              " 91: '[unused90]',\n",
              " 92: '[unused91]',\n",
              " 93: '[unused92]',\n",
              " 94: '[unused93]',\n",
              " 95: '[unused94]',\n",
              " 96: '[unused95]',\n",
              " 97: '[unused96]',\n",
              " 98: '[unused97]',\n",
              " 99: '[unused98]',\n",
              " 100: '[UNK]',\n",
              " 101: '[CLS]',\n",
              " 102: '[SEP]',\n",
              " 103: '[MASK]',\n",
              " 104: '[unused99]',\n",
              " 105: '[unused100]',\n",
              " 106: '[unused101]',\n",
              " 107: '[unused102]',\n",
              " 108: '[unused103]',\n",
              " 109: '[unused104]',\n",
              " 110: '[unused105]',\n",
              " 111: '[unused106]',\n",
              " 112: '[unused107]',\n",
              " 113: '[unused108]',\n",
              " 114: '[unused109]',\n",
              " 115: '[unused110]',\n",
              " 116: '[unused111]',\n",
              " 117: '[unused112]',\n",
              " 118: '[unused113]',\n",
              " 119: '[unused114]',\n",
              " 120: '[unused115]',\n",
              " 121: '[unused116]',\n",
              " 122: '[unused117]',\n",
              " 123: '[unused118]',\n",
              " 124: '[unused119]',\n",
              " 125: '[unused120]',\n",
              " 126: '[unused121]',\n",
              " 127: '[unused122]',\n",
              " 128: '[unused123]',\n",
              " 129: '[unused124]',\n",
              " 130: '[unused125]',\n",
              " 131: '[unused126]',\n",
              " 132: '[unused127]',\n",
              " 133: '[unused128]',\n",
              " 134: '[unused129]',\n",
              " 135: '[unused130]',\n",
              " 136: '[unused131]',\n",
              " 137: '[unused132]',\n",
              " 138: '[unused133]',\n",
              " 139: '[unused134]',\n",
              " 140: '[unused135]',\n",
              " 141: '[unused136]',\n",
              " 142: '[unused137]',\n",
              " 143: '[unused138]',\n",
              " 144: '[unused139]',\n",
              " 145: '[unused140]',\n",
              " 146: '[unused141]',\n",
              " 147: '[unused142]',\n",
              " 148: '[unused143]',\n",
              " 149: '[unused144]',\n",
              " 150: '[unused145]',\n",
              " 151: '[unused146]',\n",
              " 152: '[unused147]',\n",
              " 153: '[unused148]',\n",
              " 154: '[unused149]',\n",
              " 155: '[unused150]',\n",
              " 156: '[unused151]',\n",
              " 157: '[unused152]',\n",
              " 158: '[unused153]',\n",
              " 159: '[unused154]',\n",
              " 160: '[unused155]',\n",
              " 161: '[unused156]',\n",
              " 162: '[unused157]',\n",
              " 163: '[unused158]',\n",
              " 164: '[unused159]',\n",
              " 165: '[unused160]',\n",
              " 166: '[unused161]',\n",
              " 167: '[unused162]',\n",
              " 168: '[unused163]',\n",
              " 169: '[unused164]',\n",
              " 170: '[unused165]',\n",
              " 171: '[unused166]',\n",
              " 172: '[unused167]',\n",
              " 173: '[unused168]',\n",
              " 174: '[unused169]',\n",
              " 175: '[unused170]',\n",
              " 176: '[unused171]',\n",
              " 177: '[unused172]',\n",
              " 178: '[unused173]',\n",
              " 179: '[unused174]',\n",
              " 180: '[unused175]',\n",
              " 181: '[unused176]',\n",
              " 182: '[unused177]',\n",
              " 183: '[unused178]',\n",
              " 184: '[unused179]',\n",
              " 185: '[unused180]',\n",
              " 186: '[unused181]',\n",
              " 187: '[unused182]',\n",
              " 188: '[unused183]',\n",
              " 189: '[unused184]',\n",
              " 190: '[unused185]',\n",
              " 191: '[unused186]',\n",
              " 192: '[unused187]',\n",
              " 193: '[unused188]',\n",
              " 194: '[unused189]',\n",
              " 195: '[unused190]',\n",
              " 196: '[unused191]',\n",
              " 197: '[unused192]',\n",
              " 198: '[unused193]',\n",
              " 199: '[unused194]',\n",
              " 200: '[unused195]',\n",
              " 201: '[unused196]',\n",
              " 202: '[unused197]',\n",
              " 203: '[unused198]',\n",
              " 204: '[unused199]',\n",
              " 205: '[unused200]',\n",
              " 206: '[unused201]',\n",
              " 207: '[unused202]',\n",
              " 208: '[unused203]',\n",
              " 209: '[unused204]',\n",
              " 210: '[unused205]',\n",
              " 211: '[unused206]',\n",
              " 212: '[unused207]',\n",
              " 213: '[unused208]',\n",
              " 214: '[unused209]',\n",
              " 215: '[unused210]',\n",
              " 216: '[unused211]',\n",
              " 217: '[unused212]',\n",
              " 218: '[unused213]',\n",
              " 219: '[unused214]',\n",
              " 220: '[unused215]',\n",
              " 221: '[unused216]',\n",
              " 222: '[unused217]',\n",
              " 223: '[unused218]',\n",
              " 224: '[unused219]',\n",
              " 225: '[unused220]',\n",
              " 226: '[unused221]',\n",
              " 227: '[unused222]',\n",
              " 228: '[unused223]',\n",
              " 229: '[unused224]',\n",
              " 230: '[unused225]',\n",
              " 231: '[unused226]',\n",
              " 232: '[unused227]',\n",
              " 233: '[unused228]',\n",
              " 234: '[unused229]',\n",
              " 235: '[unused230]',\n",
              " 236: '[unused231]',\n",
              " 237: '[unused232]',\n",
              " 238: '[unused233]',\n",
              " 239: '[unused234]',\n",
              " 240: '[unused235]',\n",
              " 241: '[unused236]',\n",
              " 242: '[unused237]',\n",
              " 243: '[unused238]',\n",
              " 244: '[unused239]',\n",
              " 245: '[unused240]',\n",
              " 246: '[unused241]',\n",
              " 247: '[unused242]',\n",
              " 248: '[unused243]',\n",
              " 249: '[unused244]',\n",
              " 250: '[unused245]',\n",
              " 251: '[unused246]',\n",
              " 252: '[unused247]',\n",
              " 253: '[unused248]',\n",
              " 254: '[unused249]',\n",
              " 255: '[unused250]',\n",
              " 256: '[unused251]',\n",
              " 257: '[unused252]',\n",
              " 258: '[unused253]',\n",
              " 259: '[unused254]',\n",
              " 260: '[unused255]',\n",
              " 261: '[unused256]',\n",
              " 262: '[unused257]',\n",
              " 263: '[unused258]',\n",
              " 264: '[unused259]',\n",
              " 265: '[unused260]',\n",
              " 266: '[unused261]',\n",
              " 267: '[unused262]',\n",
              " 268: '[unused263]',\n",
              " 269: '[unused264]',\n",
              " 270: '[unused265]',\n",
              " 271: '[unused266]',\n",
              " 272: '[unused267]',\n",
              " 273: '[unused268]',\n",
              " 274: '[unused269]',\n",
              " 275: '[unused270]',\n",
              " 276: '[unused271]',\n",
              " 277: '[unused272]',\n",
              " 278: '[unused273]',\n",
              " 279: '[unused274]',\n",
              " 280: '[unused275]',\n",
              " 281: '[unused276]',\n",
              " 282: '[unused277]',\n",
              " 283: '[unused278]',\n",
              " 284: '[unused279]',\n",
              " 285: '[unused280]',\n",
              " 286: '[unused281]',\n",
              " 287: '[unused282]',\n",
              " 288: '[unused283]',\n",
              " 289: '[unused284]',\n",
              " 290: '[unused285]',\n",
              " 291: '[unused286]',\n",
              " 292: '[unused287]',\n",
              " 293: '[unused288]',\n",
              " 294: '[unused289]',\n",
              " 295: '[unused290]',\n",
              " 296: '[unused291]',\n",
              " 297: '[unused292]',\n",
              " 298: '[unused293]',\n",
              " 299: '[unused294]',\n",
              " 300: '[unused295]',\n",
              " 301: '[unused296]',\n",
              " 302: '[unused297]',\n",
              " 303: '[unused298]',\n",
              " 304: '[unused299]',\n",
              " 305: '[unused300]',\n",
              " 306: '[unused301]',\n",
              " 307: '[unused302]',\n",
              " 308: '[unused303]',\n",
              " 309: '[unused304]',\n",
              " 310: '[unused305]',\n",
              " 311: '[unused306]',\n",
              " 312: '[unused307]',\n",
              " 313: '[unused308]',\n",
              " 314: '[unused309]',\n",
              " 315: '[unused310]',\n",
              " 316: '[unused311]',\n",
              " 317: '[unused312]',\n",
              " 318: '[unused313]',\n",
              " 319: '[unused314]',\n",
              " 320: '[unused315]',\n",
              " 321: '[unused316]',\n",
              " 322: '[unused317]',\n",
              " 323: '[unused318]',\n",
              " 324: '[unused319]',\n",
              " 325: '[unused320]',\n",
              " 326: '[unused321]',\n",
              " 327: '[unused322]',\n",
              " 328: '[unused323]',\n",
              " 329: '[unused324]',\n",
              " 330: '[unused325]',\n",
              " 331: '[unused326]',\n",
              " 332: '[unused327]',\n",
              " 333: '[unused328]',\n",
              " 334: '[unused329]',\n",
              " 335: '[unused330]',\n",
              " 336: '[unused331]',\n",
              " 337: '[unused332]',\n",
              " 338: '[unused333]',\n",
              " 339: '[unused334]',\n",
              " 340: '[unused335]',\n",
              " 341: '[unused336]',\n",
              " 342: '[unused337]',\n",
              " 343: '[unused338]',\n",
              " 344: '[unused339]',\n",
              " 345: '[unused340]',\n",
              " 346: '[unused341]',\n",
              " 347: '[unused342]',\n",
              " 348: '[unused343]',\n",
              " 349: '[unused344]',\n",
              " 350: '[unused345]',\n",
              " 351: '[unused346]',\n",
              " 352: '[unused347]',\n",
              " 353: '[unused348]',\n",
              " 354: '[unused349]',\n",
              " 355: '[unused350]',\n",
              " 356: '[unused351]',\n",
              " 357: '[unused352]',\n",
              " 358: '[unused353]',\n",
              " 359: '[unused354]',\n",
              " 360: '[unused355]',\n",
              " 361: '[unused356]',\n",
              " 362: '[unused357]',\n",
              " 363: '[unused358]',\n",
              " 364: '[unused359]',\n",
              " 365: '[unused360]',\n",
              " 366: '[unused361]',\n",
              " 367: '[unused362]',\n",
              " 368: '[unused363]',\n",
              " 369: '[unused364]',\n",
              " 370: '[unused365]',\n",
              " 371: '[unused366]',\n",
              " 372: '[unused367]',\n",
              " 373: '[unused368]',\n",
              " 374: '[unused369]',\n",
              " 375: '[unused370]',\n",
              " 376: '[unused371]',\n",
              " 377: '[unused372]',\n",
              " 378: '[unused373]',\n",
              " 379: '[unused374]',\n",
              " 380: '[unused375]',\n",
              " 381: '[unused376]',\n",
              " 382: '[unused377]',\n",
              " 383: '[unused378]',\n",
              " 384: '[unused379]',\n",
              " 385: '[unused380]',\n",
              " 386: '[unused381]',\n",
              " 387: '[unused382]',\n",
              " 388: '[unused383]',\n",
              " 389: '[unused384]',\n",
              " 390: '[unused385]',\n",
              " 391: '[unused386]',\n",
              " 392: '[unused387]',\n",
              " 393: '[unused388]',\n",
              " 394: '[unused389]',\n",
              " 395: '[unused390]',\n",
              " 396: '[unused391]',\n",
              " 397: '[unused392]',\n",
              " 398: '[unused393]',\n",
              " 399: '[unused394]',\n",
              " 400: '[unused395]',\n",
              " 401: '[unused396]',\n",
              " 402: '[unused397]',\n",
              " 403: '[unused398]',\n",
              " 404: '[unused399]',\n",
              " 405: '[unused400]',\n",
              " 406: '[unused401]',\n",
              " 407: '[unused402]',\n",
              " 408: '[unused403]',\n",
              " 409: '[unused404]',\n",
              " 410: '[unused405]',\n",
              " 411: '[unused406]',\n",
              " 412: '[unused407]',\n",
              " 413: '[unused408]',\n",
              " 414: '[unused409]',\n",
              " 415: '[unused410]',\n",
              " 416: '[unused411]',\n",
              " 417: '[unused412]',\n",
              " 418: '[unused413]',\n",
              " 419: '[unused414]',\n",
              " 420: '[unused415]',\n",
              " 421: '[unused416]',\n",
              " 422: '[unused417]',\n",
              " 423: '[unused418]',\n",
              " 424: '[unused419]',\n",
              " 425: '[unused420]',\n",
              " 426: '[unused421]',\n",
              " 427: '[unused422]',\n",
              " 428: '[unused423]',\n",
              " 429: '[unused424]',\n",
              " 430: '[unused425]',\n",
              " 431: '[unused426]',\n",
              " 432: '[unused427]',\n",
              " 433: '[unused428]',\n",
              " 434: '[unused429]',\n",
              " 435: '[unused430]',\n",
              " 436: '[unused431]',\n",
              " 437: '[unused432]',\n",
              " 438: '[unused433]',\n",
              " 439: '[unused434]',\n",
              " 440: '[unused435]',\n",
              " 441: '[unused436]',\n",
              " 442: '[unused437]',\n",
              " 443: '[unused438]',\n",
              " 444: '[unused439]',\n",
              " 445: '[unused440]',\n",
              " 446: '[unused441]',\n",
              " 447: '[unused442]',\n",
              " 448: '[unused443]',\n",
              " 449: '[unused444]',\n",
              " 450: '[unused445]',\n",
              " 451: '[unused446]',\n",
              " 452: '[unused447]',\n",
              " 453: '[unused448]',\n",
              " 454: '[unused449]',\n",
              " 455: '[unused450]',\n",
              " 456: '[unused451]',\n",
              " 457: '[unused452]',\n",
              " 458: '[unused453]',\n",
              " 459: '[unused454]',\n",
              " 460: '[unused455]',\n",
              " 461: '[unused456]',\n",
              " 462: '[unused457]',\n",
              " 463: '[unused458]',\n",
              " 464: '[unused459]',\n",
              " 465: '[unused460]',\n",
              " 466: '[unused461]',\n",
              " 467: '[unused462]',\n",
              " 468: '[unused463]',\n",
              " 469: '[unused464]',\n",
              " 470: '[unused465]',\n",
              " 471: '[unused466]',\n",
              " 472: '[unused467]',\n",
              " 473: '[unused468]',\n",
              " 474: '[unused469]',\n",
              " 475: '[unused470]',\n",
              " 476: '[unused471]',\n",
              " 477: '[unused472]',\n",
              " 478: '[unused473]',\n",
              " 479: '[unused474]',\n",
              " 480: '[unused475]',\n",
              " 481: '[unused476]',\n",
              " 482: '[unused477]',\n",
              " 483: '[unused478]',\n",
              " 484: '[unused479]',\n",
              " 485: '[unused480]',\n",
              " 486: '[unused481]',\n",
              " 487: '[unused482]',\n",
              " 488: '[unused483]',\n",
              " 489: '[unused484]',\n",
              " 490: '[unused485]',\n",
              " 491: '[unused486]',\n",
              " 492: '[unused487]',\n",
              " 493: '[unused488]',\n",
              " 494: '[unused489]',\n",
              " 495: '[unused490]',\n",
              " 496: '[unused491]',\n",
              " 497: '[unused492]',\n",
              " 498: '[unused493]',\n",
              " 499: '[unused494]',\n",
              " 500: '[unused495]',\n",
              " 501: '[unused496]',\n",
              " 502: '[unused497]',\n",
              " 503: '[unused498]',\n",
              " 504: '[unused499]',\n",
              " 505: '[unused500]',\n",
              " 506: '[unused501]',\n",
              " 507: '[unused502]',\n",
              " 508: '[unused503]',\n",
              " 509: '[unused504]',\n",
              " 510: '[unused505]',\n",
              " 511: '[unused506]',\n",
              " 512: '[unused507]',\n",
              " 513: '[unused508]',\n",
              " 514: '[unused509]',\n",
              " 515: '[unused510]',\n",
              " 516: '[unused511]',\n",
              " 517: '[unused512]',\n",
              " 518: '[unused513]',\n",
              " 519: '[unused514]',\n",
              " 520: '[unused515]',\n",
              " 521: '[unused516]',\n",
              " 522: '[unused517]',\n",
              " 523: '[unused518]',\n",
              " 524: '[unused519]',\n",
              " 525: '[unused520]',\n",
              " 526: '[unused521]',\n",
              " 527: '[unused522]',\n",
              " 528: '[unused523]',\n",
              " 529: '[unused524]',\n",
              " 530: '[unused525]',\n",
              " 531: '[unused526]',\n",
              " 532: '[unused527]',\n",
              " 533: '[unused528]',\n",
              " 534: '[unused529]',\n",
              " 535: '[unused530]',\n",
              " 536: '[unused531]',\n",
              " 537: '[unused532]',\n",
              " 538: '[unused533]',\n",
              " 539: '[unused534]',\n",
              " 540: '[unused535]',\n",
              " 541: '[unused536]',\n",
              " 542: '[unused537]',\n",
              " 543: '[unused538]',\n",
              " 544: '[unused539]',\n",
              " 545: '[unused540]',\n",
              " 546: '[unused541]',\n",
              " 547: '[unused542]',\n",
              " 548: '[unused543]',\n",
              " 549: '[unused544]',\n",
              " 550: '[unused545]',\n",
              " 551: '[unused546]',\n",
              " 552: '[unused547]',\n",
              " 553: '[unused548]',\n",
              " 554: '[unused549]',\n",
              " 555: '[unused550]',\n",
              " 556: '[unused551]',\n",
              " 557: '[unused552]',\n",
              " 558: '[unused553]',\n",
              " 559: '[unused554]',\n",
              " 560: '[unused555]',\n",
              " 561: '[unused556]',\n",
              " 562: '[unused557]',\n",
              " 563: '[unused558]',\n",
              " 564: '[unused559]',\n",
              " 565: '[unused560]',\n",
              " 566: '[unused561]',\n",
              " 567: '[unused562]',\n",
              " 568: '[unused563]',\n",
              " 569: '[unused564]',\n",
              " 570: '[unused565]',\n",
              " 571: '[unused566]',\n",
              " 572: '[unused567]',\n",
              " 573: '[unused568]',\n",
              " 574: '[unused569]',\n",
              " 575: '[unused570]',\n",
              " 576: '[unused571]',\n",
              " 577: '[unused572]',\n",
              " 578: '[unused573]',\n",
              " 579: '[unused574]',\n",
              " 580: '[unused575]',\n",
              " 581: '[unused576]',\n",
              " 582: '[unused577]',\n",
              " 583: '[unused578]',\n",
              " 584: '[unused579]',\n",
              " 585: '[unused580]',\n",
              " 586: '[unused581]',\n",
              " 587: '[unused582]',\n",
              " 588: '[unused583]',\n",
              " 589: '[unused584]',\n",
              " 590: '[unused585]',\n",
              " 591: '[unused586]',\n",
              " 592: '[unused587]',\n",
              " 593: '[unused588]',\n",
              " 594: '[unused589]',\n",
              " 595: '[unused590]',\n",
              " 596: '[unused591]',\n",
              " 597: '[unused592]',\n",
              " 598: '[unused593]',\n",
              " 599: '[unused594]',\n",
              " 600: '[unused595]',\n",
              " 601: '[unused596]',\n",
              " 602: '[unused597]',\n",
              " 603: '[unused598]',\n",
              " 604: '[unused599]',\n",
              " 605: '[unused600]',\n",
              " 606: '[unused601]',\n",
              " 607: '[unused602]',\n",
              " 608: '[unused603]',\n",
              " 609: '[unused604]',\n",
              " 610: '[unused605]',\n",
              " 611: '[unused606]',\n",
              " 612: '[unused607]',\n",
              " 613: '[unused608]',\n",
              " 614: '[unused609]',\n",
              " 615: '[unused610]',\n",
              " 616: '[unused611]',\n",
              " 617: '[unused612]',\n",
              " 618: '[unused613]',\n",
              " 619: '[unused614]',\n",
              " 620: '[unused615]',\n",
              " 621: '[unused616]',\n",
              " 622: '[unused617]',\n",
              " 623: '[unused618]',\n",
              " 624: '[unused619]',\n",
              " 625: '[unused620]',\n",
              " 626: '[unused621]',\n",
              " 627: '[unused622]',\n",
              " 628: '[unused623]',\n",
              " 629: '[unused624]',\n",
              " 630: '[unused625]',\n",
              " 631: '[unused626]',\n",
              " 632: '[unused627]',\n",
              " 633: '[unused628]',\n",
              " 634: '[unused629]',\n",
              " 635: '[unused630]',\n",
              " 636: '[unused631]',\n",
              " 637: '[unused632]',\n",
              " 638: '[unused633]',\n",
              " 639: '[unused634]',\n",
              " 640: '[unused635]',\n",
              " 641: '[unused636]',\n",
              " 642: '[unused637]',\n",
              " 643: '[unused638]',\n",
              " 644: '[unused639]',\n",
              " 645: '[unused640]',\n",
              " 646: '[unused641]',\n",
              " 647: '[unused642]',\n",
              " 648: '[unused643]',\n",
              " 649: '[unused644]',\n",
              " 650: '[unused645]',\n",
              " 651: '[unused646]',\n",
              " 652: '[unused647]',\n",
              " 653: '[unused648]',\n",
              " 654: '[unused649]',\n",
              " 655: '[unused650]',\n",
              " 656: '[unused651]',\n",
              " 657: '[unused652]',\n",
              " 658: '[unused653]',\n",
              " 659: '[unused654]',\n",
              " 660: '[unused655]',\n",
              " 661: '[unused656]',\n",
              " 662: '[unused657]',\n",
              " 663: '[unused658]',\n",
              " 664: '[unused659]',\n",
              " 665: '[unused660]',\n",
              " 666: '[unused661]',\n",
              " 667: '[unused662]',\n",
              " 668: '[unused663]',\n",
              " 669: '[unused664]',\n",
              " 670: '[unused665]',\n",
              " 671: '[unused666]',\n",
              " 672: '[unused667]',\n",
              " 673: '[unused668]',\n",
              " 674: '[unused669]',\n",
              " 675: '[unused670]',\n",
              " 676: '[unused671]',\n",
              " 677: '[unused672]',\n",
              " 678: '[unused673]',\n",
              " 679: '[unused674]',\n",
              " 680: '[unused675]',\n",
              " 681: '[unused676]',\n",
              " 682: '[unused677]',\n",
              " 683: '[unused678]',\n",
              " 684: '[unused679]',\n",
              " 685: '[unused680]',\n",
              " 686: '[unused681]',\n",
              " 687: '[unused682]',\n",
              " 688: '[unused683]',\n",
              " 689: '[unused684]',\n",
              " 690: '[unused685]',\n",
              " 691: '[unused686]',\n",
              " 692: '[unused687]',\n",
              " 693: '[unused688]',\n",
              " 694: '[unused689]',\n",
              " 695: '[unused690]',\n",
              " 696: '[unused691]',\n",
              " 697: '[unused692]',\n",
              " 698: '[unused693]',\n",
              " 699: '[unused694]',\n",
              " 700: '[unused695]',\n",
              " 701: '[unused696]',\n",
              " 702: '[unused697]',\n",
              " 703: '[unused698]',\n",
              " 704: '[unused699]',\n",
              " 705: '[unused700]',\n",
              " 706: '[unused701]',\n",
              " 707: '[unused702]',\n",
              " 708: '[unused703]',\n",
              " 709: '[unused704]',\n",
              " 710: '[unused705]',\n",
              " 711: '[unused706]',\n",
              " 712: '[unused707]',\n",
              " 713: '[unused708]',\n",
              " 714: '[unused709]',\n",
              " 715: '[unused710]',\n",
              " 716: '[unused711]',\n",
              " 717: '[unused712]',\n",
              " 718: '[unused713]',\n",
              " 719: '[unused714]',\n",
              " 720: '[unused715]',\n",
              " 721: '[unused716]',\n",
              " 722: '[unused717]',\n",
              " 723: '[unused718]',\n",
              " 724: '[unused719]',\n",
              " 725: '[unused720]',\n",
              " 726: '[unused721]',\n",
              " 727: '[unused722]',\n",
              " 728: '[unused723]',\n",
              " 729: '[unused724]',\n",
              " 730: '[unused725]',\n",
              " 731: '[unused726]',\n",
              " 732: '[unused727]',\n",
              " 733: '[unused728]',\n",
              " 734: '[unused729]',\n",
              " 735: '[unused730]',\n",
              " 736: '[unused731]',\n",
              " 737: '[unused732]',\n",
              " 738: '[unused733]',\n",
              " 739: '[unused734]',\n",
              " 740: '[unused735]',\n",
              " 741: '[unused736]',\n",
              " 742: '[unused737]',\n",
              " 743: '[unused738]',\n",
              " 744: '[unused739]',\n",
              " 745: '[unused740]',\n",
              " 746: '[unused741]',\n",
              " 747: '[unused742]',\n",
              " 748: '[unused743]',\n",
              " 749: '[unused744]',\n",
              " 750: '[unused745]',\n",
              " 751: '[unused746]',\n",
              " 752: '[unused747]',\n",
              " 753: '[unused748]',\n",
              " 754: '[unused749]',\n",
              " 755: '[unused750]',\n",
              " 756: '[unused751]',\n",
              " 757: '[unused752]',\n",
              " 758: '[unused753]',\n",
              " 759: '[unused754]',\n",
              " 760: '[unused755]',\n",
              " 761: '[unused756]',\n",
              " 762: '[unused757]',\n",
              " 763: '[unused758]',\n",
              " 764: '[unused759]',\n",
              " 765: '[unused760]',\n",
              " 766: '[unused761]',\n",
              " 767: '[unused762]',\n",
              " 768: '[unused763]',\n",
              " 769: '[unused764]',\n",
              " 770: '[unused765]',\n",
              " 771: '[unused766]',\n",
              " 772: '[unused767]',\n",
              " 773: '[unused768]',\n",
              " 774: '[unused769]',\n",
              " 775: '[unused770]',\n",
              " 776: '[unused771]',\n",
              " 777: '[unused772]',\n",
              " 778: '[unused773]',\n",
              " 779: '[unused774]',\n",
              " 780: '[unused775]',\n",
              " 781: '[unused776]',\n",
              " 782: '[unused777]',\n",
              " 783: '[unused778]',\n",
              " 784: '[unused779]',\n",
              " 785: '[unused780]',\n",
              " 786: '[unused781]',\n",
              " 787: '[unused782]',\n",
              " 788: '[unused783]',\n",
              " 789: '[unused784]',\n",
              " 790: '[unused785]',\n",
              " 791: '[unused786]',\n",
              " 792: '[unused787]',\n",
              " 793: '[unused788]',\n",
              " 794: '[unused789]',\n",
              " 795: '[unused790]',\n",
              " 796: '[unused791]',\n",
              " 797: '[unused792]',\n",
              " 798: '[unused793]',\n",
              " 799: '[unused794]',\n",
              " 800: '[unused795]',\n",
              " 801: '[unused796]',\n",
              " 802: '[unused797]',\n",
              " 803: '[unused798]',\n",
              " 804: '[unused799]',\n",
              " 805: '[unused800]',\n",
              " 806: '[unused801]',\n",
              " 807: '[unused802]',\n",
              " 808: '[unused803]',\n",
              " 809: '[unused804]',\n",
              " 810: '[unused805]',\n",
              " 811: '[unused806]',\n",
              " 812: '[unused807]',\n",
              " 813: '[unused808]',\n",
              " 814: '[unused809]',\n",
              " 815: '[unused810]',\n",
              " 816: '[unused811]',\n",
              " 817: '[unused812]',\n",
              " 818: '[unused813]',\n",
              " 819: '[unused814]',\n",
              " 820: '[unused815]',\n",
              " 821: '[unused816]',\n",
              " 822: '[unused817]',\n",
              " 823: '[unused818]',\n",
              " 824: '[unused819]',\n",
              " 825: '[unused820]',\n",
              " 826: '[unused821]',\n",
              " 827: '[unused822]',\n",
              " 828: '[unused823]',\n",
              " 829: '[unused824]',\n",
              " 830: '[unused825]',\n",
              " 831: '[unused826]',\n",
              " 832: '[unused827]',\n",
              " 833: '[unused828]',\n",
              " 834: '[unused829]',\n",
              " 835: '[unused830]',\n",
              " 836: '[unused831]',\n",
              " 837: '[unused832]',\n",
              " 838: '[unused833]',\n",
              " 839: '[unused834]',\n",
              " 840: '[unused835]',\n",
              " 841: '[unused836]',\n",
              " 842: '[unused837]',\n",
              " 843: '[unused838]',\n",
              " 844: '[unused839]',\n",
              " 845: '[unused840]',\n",
              " 846: '[unused841]',\n",
              " 847: '[unused842]',\n",
              " 848: '[unused843]',\n",
              " 849: '[unused844]',\n",
              " 850: '[unused845]',\n",
              " 851: '[unused846]',\n",
              " 852: '[unused847]',\n",
              " 853: '[unused848]',\n",
              " 854: '[unused849]',\n",
              " 855: '[unused850]',\n",
              " 856: '[unused851]',\n",
              " 857: '[unused852]',\n",
              " 858: '[unused853]',\n",
              " 859: '[unused854]',\n",
              " 860: '[unused855]',\n",
              " 861: '[unused856]',\n",
              " 862: '[unused857]',\n",
              " 863: '[unused858]',\n",
              " 864: '[unused859]',\n",
              " 865: '[unused860]',\n",
              " 866: '[unused861]',\n",
              " 867: '[unused862]',\n",
              " 868: '[unused863]',\n",
              " 869: '[unused864]',\n",
              " 870: '[unused865]',\n",
              " 871: '[unused866]',\n",
              " 872: '[unused867]',\n",
              " 873: '[unused868]',\n",
              " 874: '[unused869]',\n",
              " 875: '[unused870]',\n",
              " 876: '[unused871]',\n",
              " 877: '[unused872]',\n",
              " 878: '[unused873]',\n",
              " 879: '[unused874]',\n",
              " 880: '[unused875]',\n",
              " 881: '[unused876]',\n",
              " 882: '[unused877]',\n",
              " 883: '[unused878]',\n",
              " 884: '[unused879]',\n",
              " 885: '[unused880]',\n",
              " 886: '[unused881]',\n",
              " 887: '[unused882]',\n",
              " 888: '[unused883]',\n",
              " 889: '[unused884]',\n",
              " 890: '[unused885]',\n",
              " 891: '[unused886]',\n",
              " 892: '[unused887]',\n",
              " 893: '[unused888]',\n",
              " 894: '[unused889]',\n",
              " 895: '[unused890]',\n",
              " 896: '[unused891]',\n",
              " 897: '[unused892]',\n",
              " 898: '[unused893]',\n",
              " 899: '[unused894]',\n",
              " 900: '[unused895]',\n",
              " 901: '[unused896]',\n",
              " 902: '[unused897]',\n",
              " 903: '[unused898]',\n",
              " 904: '[unused899]',\n",
              " 905: '[unused900]',\n",
              " 906: '[unused901]',\n",
              " 907: '[unused902]',\n",
              " 908: '[unused903]',\n",
              " 909: '[unused904]',\n",
              " 910: '[unused905]',\n",
              " 911: '[unused906]',\n",
              " 912: '[unused907]',\n",
              " 913: '[unused908]',\n",
              " 914: '[unused909]',\n",
              " 915: '[unused910]',\n",
              " 916: '[unused911]',\n",
              " 917: '[unused912]',\n",
              " 918: '[unused913]',\n",
              " 919: '[unused914]',\n",
              " 920: '[unused915]',\n",
              " 921: '[unused916]',\n",
              " 922: '[unused917]',\n",
              " 923: '[unused918]',\n",
              " 924: '[unused919]',\n",
              " 925: '[unused920]',\n",
              " 926: '[unused921]',\n",
              " 927: '[unused922]',\n",
              " 928: '[unused923]',\n",
              " 929: '[unused924]',\n",
              " 930: '[unused925]',\n",
              " 931: '[unused926]',\n",
              " 932: '[unused927]',\n",
              " 933: '[unused928]',\n",
              " 934: '[unused929]',\n",
              " 935: '[unused930]',\n",
              " 936: '[unused931]',\n",
              " 937: '[unused932]',\n",
              " 938: '[unused933]',\n",
              " 939: '[unused934]',\n",
              " 940: '[unused935]',\n",
              " 941: '[unused936]',\n",
              " 942: '[unused937]',\n",
              " 943: '[unused938]',\n",
              " 944: '[unused939]',\n",
              " 945: '[unused940]',\n",
              " 946: '[unused941]',\n",
              " 947: '[unused942]',\n",
              " 948: '[unused943]',\n",
              " 949: '[unused944]',\n",
              " 950: '[unused945]',\n",
              " 951: '[unused946]',\n",
              " 952: '[unused947]',\n",
              " 953: '[unused948]',\n",
              " 954: '[unused949]',\n",
              " 955: '[unused950]',\n",
              " 956: '[unused951]',\n",
              " 957: '[unused952]',\n",
              " 958: '[unused953]',\n",
              " 959: '[unused954]',\n",
              " 960: '[unused955]',\n",
              " 961: '[unused956]',\n",
              " 962: '[unused957]',\n",
              " 963: '[unused958]',\n",
              " 964: '[unused959]',\n",
              " 965: '[unused960]',\n",
              " 966: '[unused961]',\n",
              " 967: '[unused962]',\n",
              " 968: '[unused963]',\n",
              " 969: '[unused964]',\n",
              " 970: '[unused965]',\n",
              " 971: '[unused966]',\n",
              " 972: '[unused967]',\n",
              " 973: '[unused968]',\n",
              " 974: '[unused969]',\n",
              " 975: '[unused970]',\n",
              " 976: '[unused971]',\n",
              " 977: '[unused972]',\n",
              " 978: '[unused973]',\n",
              " 979: '[unused974]',\n",
              " 980: '[unused975]',\n",
              " 981: '[unused976]',\n",
              " 982: '[unused977]',\n",
              " 983: '[unused978]',\n",
              " 984: '[unused979]',\n",
              " 985: '[unused980]',\n",
              " 986: '[unused981]',\n",
              " 987: '[unused982]',\n",
              " 988: '[unused983]',\n",
              " 989: '[unused984]',\n",
              " 990: '[unused985]',\n",
              " 991: '[unused986]',\n",
              " 992: '[unused987]',\n",
              " 993: '[unused988]',\n",
              " 994: '[unused989]',\n",
              " 995: '[unused990]',\n",
              " 996: '[unused991]',\n",
              " 997: '[unused992]',\n",
              " 998: '[unused993]',\n",
              " 999: '!',\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if the tokenizer works well again in QnA format ([CLS} Question [SEP] Context [SEP] order)\n",
        "print(tokenizer.tokenize(\"Where does Youngsun live in now?\", \"My name is Youngsun Jang, and I live in US now.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgIuKPLIiE4r",
        "outputId": "8d9697d5-b8ca-4c49-a500-086434b50d35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'where', 'does', 'young', '##sun', 'live', 'in', 'now', '?', '[SEP]', 'my', 'name', 'is', 'young', '##sun', 'jang', ',', 'and', 'i', 'live', 'in', 'us', 'now', '.', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Embedding (Numberical representation)\n",
        "print(tokenizer.encode(\"Where does Youngsun live in now?\", \"My name is Youngsun Jang, and I live in US now.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiBBz4TVkNPd",
        "outputId": "3571c171-4df8-4e42-d4b2-00ae83da0a78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([101, 2073, 2515, 2402, 19729, 2444, 1999, 2085, 1029, 102, 2026, 2171, 2003, 2402, 19729, 23769, 1010, 1998, 1045, 2444, 1999, 2149, 2085, 1012, 102], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "545Zv6ZkIrvW",
        "outputId": "55230607-faa9-4296-f914-575f842c9af3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b29e0711-f8c6-4559-a3ec-fc31a4681d3e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>text</th>\n",
              "      <th>c_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5733be284776f41900661182</td>\n",
              "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>515</td>\n",
              "      <td>Saint Bernadette Soubirous</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5733be284776f4190066117f</td>\n",
              "      <td>What is in front of the Notre Dame Main Building?</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>188</td>\n",
              "      <td>a copper statue of Christ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5733be284776f41900661180</td>\n",
              "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>279</td>\n",
              "      <td>the Main Building</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5733be284776f41900661181</td>\n",
              "      <td>What is the Grotto at Notre Dame?</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>381</td>\n",
              "      <td>a Marian place of prayer and reflection</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5733be284776f4190066117e</td>\n",
              "      <td>What sits on top of the Main Building at Notre...</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>92</td>\n",
              "      <td>a golden statue of the Virgin Mary</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b29e0711-f8c6-4559-a3ec-fc31a4681d3e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b29e0711-f8c6-4559-a3ec-fc31a4681d3e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b29e0711-f8c6-4559-a3ec-fc31a4681d3e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                      index  ... c_id\n",
              "0  5733be284776f41900661182  ...    0\n",
              "1  5733be284776f4190066117f  ...    0\n",
              "2  5733be284776f41900661180  ...    0\n",
              "3  5733be284776f41900661181  ...    0\n",
              "4  5733be284776f4190066117e  ...    0\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Question: \", train.loc[0, 'question'])\n",
        "print(\"Context: \", train.loc[0, 'context'])\n",
        "print(\"Answer: \", train.loc[0, 'text'])"
      ],
      "metadata": {
        "id": "widGhKPakwqL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db4ae907-a9b2-41e8-b298-2b9a801924db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:  To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
            "Context:  Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
            "Answer:  Saint Bernadette Soubirous\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QA is to confirm the position of the Answer token in Context \n",
        "# position of Saint Bernadette Soubirous !\n",
        "# That is, from 3002 - 2271\n",
        "\n",
        "print(\"Question: \", tokenizer.tokenize(train.loc[0, 'question']))\n",
        "print(\"Context: \", tokenizer.tokenize(train.loc[0, 'context']))\n",
        "print(\"Answer: \", tokenizer.tokenize(train.loc[0, 'text']))\n",
        "\n",
        "print(\"Question: \", tokenizer.encode(train.loc[0, 'question']))\n",
        "print(\"Context: \", tokenizer.encode(train.loc[0, 'context']))\n",
        "print(\"Answer: \", tokenizer.encode(train.loc[0, 'text']))"
      ],
      "metadata": {
        "id": "6NnrgxvRGGN4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5097c13a-44e8-432b-d3f9-294fbfc01605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:  ['[CLS]', 'to', 'whom', 'did', 'the', 'virgin', 'mary', 'allegedly', 'appear', 'in', '1858', 'in', 'lou', '##rdes', 'france', '?', '[SEP]']\n",
            "Context:  ['[CLS]', 'architectural', '##ly', ',', 'the', 'school', 'has', 'a', 'catholic', 'character', '.', 'atop', 'the', 'main', 'building', \"'\", 's', 'gold', 'dome', 'is', 'a', 'golden', 'statue', 'of', 'the', 'virgin', 'mary', '.', 'immediately', 'in', 'front', 'of', 'the', 'main', 'building', 'and', 'facing', 'it', ',', 'is', 'a', 'copper', 'statue', 'of', 'christ', 'with', 'arms', 'up', '##rai', '##sed', 'with', 'the', 'legend', '\"', 've', '##ni', '##te', 'ad', 'me', 'om', '##nes', '\"', '.', 'next', 'to', 'the', 'main', 'building', 'is', 'the', 'basilica', 'of', 'the', 'sacred', 'heart', '.', 'immediately', 'behind', 'the', 'basilica', 'is', 'the', 'gr', '##otto', ',', 'a', 'marian', 'place', 'of', 'prayer', 'and', 'reflection', '.', 'it', 'is', 'a', 'replica', 'of', 'the', 'gr', '##otto', 'at', 'lou', '##rdes', ',', 'france', 'where', 'the', 'virgin', 'mary', 'reputed', '##ly', 'appeared', 'to', 'saint', 'bern', '##ade', '##tte', 'so', '##ub', '##iro', '##us', 'in', '1858', '.', 'at', 'the', 'end', 'of', 'the', 'main', 'drive', '(', 'and', 'in', 'a', 'direct', 'line', 'that', 'connects', 'through', '3', 'statues', 'and', 'the', 'gold', 'dome', ')', ',', 'is', 'a', 'simple', ',', 'modern', 'stone', 'statue', 'of', 'mary', '.', '[SEP]']\n",
            "Answer:  ['[CLS]', 'saint', 'bern', '##ade', '##tte', 'so', '##ub', '##iro', '##us', '[SEP]']\n",
            "Question:  ([101, 2000, 3183, 2106, 1996, 6261, 2984, 9382, 3711, 1999, 8517, 1999, 10223, 26371, 2605, 1029, 102], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Context:  ([101, 6549, 2135, 1010, 1996, 2082, 2038, 1037, 3234, 2839, 1012, 10234, 1996, 2364, 2311, 1005, 1055, 2751, 8514, 2003, 1037, 3585, 6231, 1997, 1996, 6261, 2984, 1012, 3202, 1999, 2392, 1997, 1996, 2364, 2311, 1998, 5307, 2009, 1010, 2003, 1037, 6967, 6231, 1997, 4828, 2007, 2608, 2039, 14995, 6924, 2007, 1996, 5722, 1000, 2310, 3490, 2618, 4748, 2033, 18168, 5267, 1000, 1012, 2279, 2000, 1996, 2364, 2311, 2003, 1996, 13546, 1997, 1996, 6730, 2540, 1012, 3202, 2369, 1996, 13546, 2003, 1996, 24665, 23052, 1010, 1037, 14042, 2173, 1997, 7083, 1998, 9185, 1012, 2009, 2003, 1037, 15059, 1997, 1996, 24665, 23052, 2012, 10223, 26371, 1010, 2605, 2073, 1996, 6261, 2984, 22353, 2135, 2596, 2000, 3002, 16595, 9648, 4674, 2061, 12083, 9711, 2271, 1999, 8517, 1012, 2012, 1996, 2203, 1997, 1996, 2364, 3298, 1006, 1998, 1999, 1037, 3622, 2240, 2008, 8539, 2083, 1017, 11342, 1998, 1996, 2751, 8514, 1007, 1010, 2003, 1037, 3722, 1010, 2715, 2962, 6231, 1997, 2984, 1012, 102], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Answer:  ([101, 3002, 16595, 9648, 4674, 2061, 12083, 9711, 2271, 102], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting only token part excepting the segment part\n",
        "context = tokenizer.encode(train.loc[0, 'context'])[0]\n",
        "text = tokenizer.encode(train.loc[0, 'text'])[0]"
      ],
      "metadata": {
        "id": "vMDSi0GOKHlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLS, SEP token delete\n",
        "text.pop(0)\n",
        "text.pop(-1)\n",
        "print(text)\n",
        "print(len(text))\n",
        "len(context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWOeVdn4b2DC",
        "outputId": "cf44d0f9-30c6-43e3-d1d1-747b94c1fcd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3002, 16595, 9648, 4674, 2061, 12083, 9711, 2271]\n",
            "8\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "160"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To calculate the length of text (answer)\n",
        "# Method : Doing 'Sliding' the context with the length of the answer\n",
        "text_slide_len = len(text) # text_slide_len = 8\n",
        "\n",
        "for j in range(0, (len(context))):\n",
        "  # exist_flag : showing whether it is answerable or not (similar with is_unanswerable in SimpleTransforemr)\n",
        "  # 0 : No answer / 1 : Having answer\n",
        "  exist_flag = 0 \n",
        "  if text == context[j:j+text_slide_len]: # [0:8]->[1:9]->[2:10]->..->[159:160]\n",
        "    # Assign the location of answer (start, end)\n",
        "    ans_start = j\n",
        "    ans_end = j + text_slide_len - 1\n",
        "    # If matched, exist_flag changed to 1\n",
        "    exist_flag = 1\n",
        "    break\n",
        "\n",
        "print(\"ans_start : {}, ans_end : {}\".format(ans_start, ans_end))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51XFX7aeb2qq",
        "outputId": "54bab0b7-f6fa-48c1-ed6c-39a4b242f0e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ans_start : 114, ans_end : 121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check\n",
        "# context[ans_start:ans_end]\n",
        "print(context[ans_start:ans_end+1], text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JAOk87qdx8m",
        "outputId": "948c99fe-502b-4921-8245-724c0b048cb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3002, 16595, 9648, 4674, 2061, 12083, 9711, 2271] [3002, 16595, 9648, 4674, 2061, 12083, 9711, 2271]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function calculating the length of answer in context dataset all at once\n",
        "def convert_data(data_df):\n",
        "  global tokenizer\n",
        "  indices, segments, target_start, target_end = [], [], [], []\n",
        "\n",
        "  for i in tqdm(range(len(data_df))):\n",
        "    # que : List of tokenized question\n",
        "    que, _ = tokenizer.encode(data_df[QUESTION_COLUMN][i])\n",
        "    # doc : List of tokenized context\n",
        "    doc, _ = tokenizer.encode(data_df[DATA_COLUMN][i])\n",
        "\n",
        "    # [CLS] token deleted in context\n",
        "    doc.pop(0)\n",
        "\n",
        "    # Length of question & context\n",
        "    que_len = len(que)\n",
        "    doc_len = len(doc)\n",
        "    # 1. Length of question\n",
        "    # The question is cut by the length of 64\n",
        "    if que_len > 64:\n",
        "      que = que[:63]\n",
        "      que.append(102) # [SEP] token added to make it clear the question block\n",
        "    # 2. Total length of question and context\n",
        "    # The total input is cut by the length of 384\n",
        "    if len(que+doc) > SEQ_LEN:\n",
        "      while len(que+doc) != SEQ_LEN:\n",
        "        doc.pop(-1)\n",
        "      doc.pop(-1)\n",
        "      doc.append(102)\n",
        "\n",
        "    # Segment embedding\n",
        "    # Question : 0 / Context 1 / Padding : 0 (remaining part for short sentences)\n",
        "        \n",
        "    ############################\n",
        "    ###### Segment 예시 ########\n",
        "    ############################\n",
        "    \n",
        "    # question, context, padding\n",
        "    # 00000000, 1111111, 0000000\n",
        "    \n",
        "    segment = [0]*len(que) + [1]*len(doc) + [0]*(SEQ_LEN-len(que)-len(doc))\n",
        "\n",
        "    # Padding\n",
        "    if len(que+doc) <= SEQ_LEN:\n",
        "      while len(que+doc) != SEQ_LEN:\n",
        "        doc.append(0)\n",
        "\n",
        "    # Final Input 'ids' (Question + Context)\n",
        "    ids = que + doc\n",
        "\n",
        "    # Sliding Part\n",
        "    text = tokenizer.encode(data_df[TEXT][i])[0]\n",
        "    text_slide_len = len(text[1:-1]) # text_slide_len = 8\n",
        "\n",
        "    for j in range(0, (len(doc))):\n",
        "      # exist_flag : showing whether it is answerable or not (similar with is_unanswerable in SimpleTransforemr)\n",
        "      # 0 : No answer / 1 : Having answer\n",
        "      exist_flag = 0 \n",
        "      if text[1:-1] == doc[j:j+text_slide_len]: # [0:8]->[1:9]->[2:10]->..->[159:160]\n",
        "        # Assign the location of answer (start, end)\n",
        "        ans_start = j + len(que)\n",
        "        ans_end = j + text_slide_len - 1 + len(que)\n",
        "        # If matched, exist_flag changed to 1\n",
        "        exist_flag = 1\n",
        "        break\n",
        "\n",
        "    # When no answer case (exist_flag = 0), starting & ending value become SEQ_LEN\n",
        "    # All the data of starting, ending = 384 (SEQ_LEN) will be deleted from the list\n",
        "    if exist_flag == 0:\n",
        "      ans_start = SEQ_LEN\n",
        "      ans_end = SEQ_LEN\n",
        "\n",
        "    # Input(ids), Segment saving into list type (indices, segments)\n",
        "    indices.append(ids)\n",
        "    segments.append(segment)\n",
        "    # Starting and ending info saving into list type (target_start, target_end)\n",
        "    target_start.append(ans_start)\n",
        "    target_end.append(ans_end)\n",
        "\n",
        "  # Converting the 4 lists into numpy array\n",
        "  indices_x = np.array(indices)\n",
        "  segments = np.array(segments)\n",
        "  target_start = np.array(target_start)\n",
        "  target_end = np.array(target_end)\n",
        "\n",
        "  # The cut part saved in del_list and deleted from data\n",
        "  del_list = np.where(target_start != SEQ_LEN)[0]\n",
        "  not_del_list = np.where(target_start == SEQ_LEN)[0]\n",
        "  indices_x = indices_x[del_list]\n",
        "  segments = segments[del_list]\n",
        "  target_start = target_start[del_list]\n",
        "  target_end = target_end[del_list]\n",
        "\n",
        "  return [indices_x, segments], [target_start, target_end], not_del_list"
      ],
      "metadata": {
        "id": "G3XbA6REm74x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load of the Calculator Function\n",
        "def load_data(pandas_dataframe):\n",
        "  data_df = pandas_dataframe\n",
        "  data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n",
        "  data_df[QUESTION_COLUMN] = data_df[QUESTION_COLUMN].astype(str)\n",
        "  data_x, data_y, del_list = convert_data(data_df)\n",
        "\n",
        "  return data_x, data_y, del_list"
      ],
      "metadata": {
        "id": "iDfJWDt0jLV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Data Coneversion Starting\n",
        "train_x, train_y, z = load_data(train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-FqyeJ9e5tm",
        "outputId": "4cd8a44f-4089-46d9-92e7-eb0764e959b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 87589/87589 [03:18<00:00, 442.00it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_x[0].shape, train_y[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xb4e9GlNfGrO",
        "outputId": "9ea5a11b-82f2-4d1c-9593-9ad722ecad8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(87353, 384) (87353,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oMDoTCNfvrD",
        "outputId": "bdd21dc0-6b2c-4790-ba79-4f54eedf2545"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  366,   451,   453,   455,   460,   477,   672,   677,  1315,\n",
              "        1590,  1595,  1600,  1922,  2360,  2361,  2478,  2491,  2683,\n",
              "        2862,  3017,  3019,  3117,  3142,  3275,  3276,  3427,  3490,\n",
              "        4312,  4786,  5121,  5919,  5924,  5928,  5932,  5990,  5992,\n",
              "        5994,  5995,  6155,  6158,  6248,  6296,  6311,  6314,  6316,\n",
              "        6329,  6330,  6354,  6392,  6985,  7096,  7331,  7528,  7529,\n",
              "        7622,  7625,  7626,  7676,  7678,  7948,  8017,  8021,  8273,\n",
              "        8735,  8835,  9741, 10000, 10578, 10653, 10675, 10699, 10700,\n",
              "       11308, 11780, 11846, 11987, 12136, 12147, 12189, 12777, 12874,\n",
              "       12969, 13470, 13692, 13702, 13729, 13783, 13787, 13791, 13799,\n",
              "       13863, 14260, 14317, 14377, 14404, 14434, 14448, 14571, 14582,\n",
              "       14600, 16070, 16116, 17168, 17344, 17410, 18100, 18369, 18707,\n",
              "       19350, 19374, 19830, 20249, 20459, 20508, 20661, 22422, 23848,\n",
              "       25521, 25931, 26648, 27627, 27687, 27972, 28010, 28246, 28420,\n",
              "       28589, 29872, 30712, 30825, 30921, 31817, 32952, 33079, 33865,\n",
              "       33888, 33907, 34452, 34600, 34700, 35692, 36071, 37093, 38526,\n",
              "       38536, 38666, 38694, 39351, 40189, 42639, 42947, 43383, 43671,\n",
              "       45020, 45078, 45079, 45180, 45181, 45708, 45758, 46979, 47476,\n",
              "       47604, 47653, 47697, 48296, 48397, 49059, 49060, 49084, 49692,\n",
              "       50455, 50456, 50753, 51737, 52720, 53486, 53522, 54325, 54330,\n",
              "       55207, 56308, 57102, 57103, 57391, 57576, 57577, 57578, 57579,\n",
              "       57580, 57714, 58137, 58259, 58319, 58365, 58756, 58815, 58845,\n",
              "       60014, 60017, 61895, 62102, 62817, 63290, 63986, 64569, 65265,\n",
              "       65712, 65723, 65881, 66136, 66863, 68314, 68487, 68494, 70547,\n",
              "       71359, 73377, 73881, 73964, 75504, 75710, 76471, 77597, 77693,\n",
              "       79965, 81363, 82537, 83109, 83394, 83902, 84812, 86286, 86572,\n",
              "       87196, 87210])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Pretrained Model\n",
        "model = load_trained_model_from_checkpoint (\n",
        "    config_path,\n",
        "    checkpoint_path,\n",
        "    training = False,\n",
        "    trainable = True,\n",
        "    seq_len = SEQ_LEN\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSBvWN7g9nZU",
        "outputId": "3123f282-34f4-4f5c-e53b-65b5ad91ab7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Input-Token (InputLayer)       [(None, 384)]        0           []                               \n",
            "                                                                                                  \n",
            " Input-Segment (InputLayer)     [(None, 384)]        0           []                               \n",
            "                                                                                                  \n",
            " Embedding-Token (TokenEmbeddin  [(None, 384, 768),  23440896    ['Input-Token[0][0]']            \n",
            " g)                              (30522, 768)]                                                    \n",
            "                                                                                                  \n",
            " Embedding-Segment (Embedding)  (None, 384, 768)     1536        ['Input-Segment[0][0]']          \n",
            "                                                                                                  \n",
            " Embedding-Token-Segment (Add)  (None, 384, 768)     0           ['Embedding-Token[0][0]',        \n",
            "                                                                  'Embedding-Segment[0][0]']      \n",
            "                                                                                                  \n",
            " Embedding-Position (PositionEm  (None, 384, 768)    294912      ['Embedding-Token-Segment[0][0]']\n",
            " bedding)                                                                                         \n",
            "                                                                                                  \n",
            " Embedding-Dropout (Dropout)    (None, 384, 768)     0           ['Embedding-Position[0][0]']     \n",
            "                                                                                                  \n",
            " Embedding-Norm (LayerNormaliza  (None, 384, 768)    1536        ['Embedding-Dropout[0][0]']      \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " Encoder-1-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Embedding-Norm[0][0]']         \n",
            " on (MultiHeadAttention)                                                                          \n",
            "                                                                                                  \n",
            " Encoder-1-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-1-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-1-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Embedding-Norm[0][0]',         \n",
            " on-Add (Add)                                                     'Encoder-1-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-1-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-1-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-1-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-1-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-1-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-1-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-1-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-1-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-1-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-1-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-1-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-2-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-1-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-2-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-2-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-2-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-1-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-2-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-2-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-2-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-2-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-2-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-2-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-2-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-2-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-2-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-2-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-2-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-2-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-3-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-2-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-3-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-3-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-3-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-2-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-3-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-3-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-3-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-3-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-3-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-3-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-3-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-3-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-3-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-3-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-3-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-3-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-4-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-3-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-4-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-4-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-4-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-3-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-4-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-4-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-4-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-4-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-4-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-4-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-4-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-4-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-4-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-4-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-4-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-4-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-5-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-4-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-5-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-5-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-5-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-4-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-5-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-5-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-5-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-5-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-5-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-5-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-5-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-5-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-5-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-5-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-5-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-5-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-6-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-5-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-6-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-6-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-6-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-5-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-6-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-6-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-6-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-6-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-6-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-6-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-6-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-6-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-6-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-6-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-6-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-6-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-7-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-6-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-7-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-7-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-7-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-6-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-7-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-7-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-7-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-7-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-7-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-7-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-7-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-7-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-7-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-7-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-7-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-7-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-8-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-7-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-8-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-8-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-8-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-7-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-8-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-8-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-8-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-8-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-8-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-8-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-8-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-8-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-8-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-8-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-8-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-8-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-9-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-8-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-9-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-9-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-9-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-8-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-9-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-9-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-9-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-9-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-9-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-9-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-9-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-9-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-9-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-9-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-9-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-9-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-10-MultiHeadSelfAttent  (None, 384, 768)    2362368     ['Encoder-9-FeedForward-Norm[0][0\n",
            " ion (MultiHeadAttention)                                        ]']                              \n",
            "                                                                                                  \n",
            " Encoder-10-MultiHeadSelfAttent  (None, 384, 768)    0           ['Encoder-10-MultiHeadSelfAttenti\n",
            " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
            "                                                                                                  \n",
            " Encoder-10-MultiHeadSelfAttent  (None, 384, 768)    0           ['Encoder-9-FeedForward-Norm[0][0\n",
            " ion-Add (Add)                                                   ]',                              \n",
            "                                                                  'Encoder-10-MultiHeadSelfAttenti\n",
            "                                                                 on-Dropout[0][0]']               \n",
            "                                                                                                  \n",
            " Encoder-10-MultiHeadSelfAttent  (None, 384, 768)    1536        ['Encoder-10-MultiHeadSelfAttenti\n",
            " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-10-FeedForward (FeedFo  (None, 384, 768)    4722432     ['Encoder-10-MultiHeadSelfAttenti\n",
            " rward)                                                          on-Norm[0][0]']                  \n",
            "                                                                                                  \n",
            " Encoder-10-FeedForward-Dropout  (None, 384, 768)    0           ['Encoder-10-FeedForward[0][0]'] \n",
            "  (Dropout)                                                                                       \n",
            "                                                                                                  \n",
            " Encoder-10-FeedForward-Add (Ad  (None, 384, 768)    0           ['Encoder-10-MultiHeadSelfAttenti\n",
            " d)                                                              on-Norm[0][0]',                  \n",
            "                                                                  'Encoder-10-FeedForward-Dropout[\n",
            "                                                                 0][0]']                          \n",
            "                                                                                                  \n",
            " Encoder-10-FeedForward-Norm (L  (None, 384, 768)    1536        ['Encoder-10-FeedForward-Add[0][0\n",
            " ayerNormalization)                                              ]']                              \n",
            "                                                                                                  \n",
            " Encoder-11-MultiHeadSelfAttent  (None, 384, 768)    2362368     ['Encoder-10-FeedForward-Norm[0][\n",
            " ion (MultiHeadAttention)                                        0]']                             \n",
            "                                                                                                  \n",
            " Encoder-11-MultiHeadSelfAttent  (None, 384, 768)    0           ['Encoder-11-MultiHeadSelfAttenti\n",
            " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
            "                                                                                                  \n",
            " Encoder-11-MultiHeadSelfAttent  (None, 384, 768)    0           ['Encoder-10-FeedForward-Norm[0][\n",
            " ion-Add (Add)                                                   0]',                             \n",
            "                                                                  'Encoder-11-MultiHeadSelfAttenti\n",
            "                                                                 on-Dropout[0][0]']               \n",
            "                                                                                                  \n",
            " Encoder-11-MultiHeadSelfAttent  (None, 384, 768)    1536        ['Encoder-11-MultiHeadSelfAttenti\n",
            " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-11-FeedForward (FeedFo  (None, 384, 768)    4722432     ['Encoder-11-MultiHeadSelfAttenti\n",
            " rward)                                                          on-Norm[0][0]']                  \n",
            "                                                                                                  \n",
            " Encoder-11-FeedForward-Dropout  (None, 384, 768)    0           ['Encoder-11-FeedForward[0][0]'] \n",
            "  (Dropout)                                                                                       \n",
            "                                                                                                  \n",
            " Encoder-11-FeedForward-Add (Ad  (None, 384, 768)    0           ['Encoder-11-MultiHeadSelfAttenti\n",
            " d)                                                              on-Norm[0][0]',                  \n",
            "                                                                  'Encoder-11-FeedForward-Dropout[\n",
            "                                                                 0][0]']                          \n",
            "                                                                                                  \n",
            " Encoder-11-FeedForward-Norm (L  (None, 384, 768)    1536        ['Encoder-11-FeedForward-Add[0][0\n",
            " ayerNormalization)                                              ]']                              \n",
            "                                                                                                  \n",
            " Encoder-12-MultiHeadSelfAttent  (None, 384, 768)    2362368     ['Encoder-11-FeedForward-Norm[0][\n",
            " ion (MultiHeadAttention)                                        0]']                             \n",
            "                                                                                                  \n",
            " Encoder-12-MultiHeadSelfAttent  (None, 384, 768)    0           ['Encoder-12-MultiHeadSelfAttenti\n",
            " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
            "                                                                                                  \n",
            " Encoder-12-MultiHeadSelfAttent  (None, 384, 768)    0           ['Encoder-11-FeedForward-Norm[0][\n",
            " ion-Add (Add)                                                   0]',                             \n",
            "                                                                  'Encoder-12-MultiHeadSelfAttenti\n",
            "                                                                 on-Dropout[0][0]']               \n",
            "                                                                                                  \n",
            " Encoder-12-MultiHeadSelfAttent  (None, 384, 768)    1536        ['Encoder-12-MultiHeadSelfAttenti\n",
            " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-12-FeedForward (FeedFo  (None, 384, 768)    4722432     ['Encoder-12-MultiHeadSelfAttenti\n",
            " rward)                                                          on-Norm[0][0]']                  \n",
            "                                                                                                  \n",
            " Encoder-12-FeedForward-Dropout  (None, 384, 768)    0           ['Encoder-12-FeedForward[0][0]'] \n",
            "  (Dropout)                                                                                       \n",
            "                                                                                                  \n",
            " Encoder-12-FeedForward-Add (Ad  (None, 384, 768)    0           ['Encoder-12-MultiHeadSelfAttenti\n",
            " d)                                                              on-Norm[0][0]',                  \n",
            "                                                                  'Encoder-12-FeedForward-Dropout[\n",
            "                                                                 0][0]']                          \n",
            "                                                                                                  \n",
            " Encoder-12-FeedForward-Norm (L  (None, 384, 768)    1536        ['Encoder-12-FeedForward-Add[0][0\n",
            " ayerNormalization)                                              ]']                              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,793,344\n",
            "Trainable params: 108,793,344\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For Transfer Learning, Customized Layer needs to be added after 12 Encoder\n",
        "# by defining 'Non-masking' function, BERT Model's masked tensors disclosed\n",
        "\n",
        "class NonMasking(Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    self.supports_masking = True\n",
        "    super(NonMasking, self).__init__(**kwargs)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    input_shape = input_shape\n",
        "  \n",
        "  def compute_mask(self, input, input_mask = None):\n",
        "    return None\n",
        "\n",
        "  def call(self, x, mask = None):\n",
        "    return x\n",
        "\n",
        "  def get_output_shape_for(self, input_shape):\n",
        "    return input_shape"
      ],
      "metadata": {
        "id": "p8OuSxoJYLBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation function definition\n",
        "def bert_gelu(x):\n",
        "  \"\"\"Gaussian Error Linear Unit.\n",
        "  This is a smoother version of the RELU.\n",
        "  Original paper: https://arxiv.org/abs/1606.08415\n",
        "  Args:\n",
        "    x : float Tensor to perform activation\n",
        "  Returns:\n",
        "    'x' with the GELU activation applied.\n",
        "  \"\"\"\n",
        "  cdf = 0.5*(1.0+ K.tanh(\n",
        "      (np.sqrt(2/np.pi)*(x+0.044715 * K.pow(x,3)))))\n",
        "  return x*cdf"
      ],
      "metadata": {
        "id": "91iL3th6aiU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation Function Layer attached to Transformer\n",
        "class Start_End_Prediction(Layer):\n",
        "  def __init__(self, seq_len, **kwargs):\n",
        "    self.seq_len = SEQ_LEN\n",
        "    self.supports_masking = True\n",
        "    super(Start_End_Prediction, self).__init__(**kwargs)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    # A tensor ('self.W') multiplied with the final layer (12 encoder, batch_size, 384, 768)\n",
        "    # Making Output tensor as (384, 2 dimension (768->2))\n",
        "    self.W = self.add_weight(name='kernel',\n",
        "                             shape = (input_shape[2],2),\n",
        "                             initializer = 'uniform',\n",
        "                             trainable = True)\n",
        "    super(Start_End_Prediction, self).build(input_shape)\n",
        "\n",
        "  def call(self, x):\n",
        "    # Redifine Output dimension as (384 * 768)\n",
        "    x = K.reshape(x, shape=(-1, self.seq_len, K.shape(x)[2]))\n",
        "    # Dot production between self.W and x\n",
        "    # (batch_size, 384, 768) * (384, 2) = (batch_size, 384, 2)\n",
        "    x = K.dot(x, self.W)\n",
        "    # (batch_size, 384, 2) -> (2, batch_size, 384)\n",
        "    x = K.permute_dimensions(x, (2, 0, 1))\n",
        "\n",
        "    # Split the (2, batch_size, 384) into 2 (batch_size, 384)  --> start_logits & end_logits\n",
        "    # start_logits = (batch_size, 384)\n",
        "    # end_logits = (batch_size, 384)\n",
        "    self.start_logits, self.end_logits = x[0], x[1]\n",
        "\n",
        "    # Fed into, Passed by the GELU layer\n",
        "    self.start_logits = bert_gelu(self.start_logits)\n",
        "    self.end_logits = bert_gelu(self.end_logits)\n",
        "\n",
        "    # Fed into, Passed by the Softmax layer\n",
        "    # Getting probability for 384 tokens\n",
        "    self.start_logits = K.softmax(self.start_logits, axis=-1)\n",
        "    self.end_logits = K.softmax(self.end_logits, axis=-1)\n",
        "\n",
        "    return [self.start_logits, self.end_logits]\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    # In Keras, when defining custom layer, \n",
        "    # the output dimension must be defined in compute_output_shape function\n",
        "    return [(input_shape[0], self.seq_len), (input_shape[0], self.seq_len)]"
      ],
      "metadata": {
        "id": "ykF0O6J4cEnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BERT model\n",
        "from keras.layers import merge, dot, concatenate\n",
        "from keras import metrics\n",
        "import numpy as np\n",
        "\n",
        "# Define a functoin loading BERT model\n",
        "def get_bert_finetuning_model(model):\n",
        "  # Input data is token-embedding and segment-embedding\n",
        "  inputs = model.inputs[:2]\n",
        "\n",
        "  # Output of Transformer : (batch_size, 384, 768)\n",
        "  bert_transformer = model.layers[-1].output\n",
        "\n",
        "  # Unmask the maksed tensors using NonMasking function\n",
        "  x = NonMasking()(bert_transformer)\n",
        "\n",
        "  # Show the start and end token of answer at last\n",
        "  outputs_start, outputs_end = Start_End_Prediction(SEQ_LEN)(x)\n",
        "\n",
        "  bert_model = keras.models.Model(inputs, [outputs_start, outputs_end])\n",
        "\n",
        "  # Optimizer defined growing Learning_rate from 0 to 1.5e-5 gradually\n",
        "  # Original RAdam changed to RAdamOptimizer for some issue\n",
        "  optimizer_warmup = RAdamOptimizer(learning_rate = 1.5e-5, warmup_proportion=0.2, epsilon=1e-6, weight_decay = 0.01)\n",
        "\n",
        "  # Final Model compile\n",
        "  bert_model.compile(\n",
        "      optimizer = optimizer_warmup,\n",
        "      loss = 'sparse_categorical_crossentropy',\n",
        "      metrics = ['accuracy']\n",
        "  )\n",
        "\n",
        "  return bert_model"
      ],
      "metadata": {
        "id": "jV46fdelhJSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display format customize in wrap for Google Colab (Youngsun)\n",
        "from IPython.display import HTML, display\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "2RhVBoRou8ZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model FLow Check\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "SVG(model_to_dot(get_bert_finetuning_model(model), dpi=65).create(prog='dot', format='svg'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DqiN0F5bm7HG",
        "outputId": "18e026c2-1f69-4e2b-969f-6b1abb95f9bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"6829pt\" viewBox=\"0.00 0.00 574.50 7564.00\" width=\"519pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(.9028 .9028) rotate(0) translate(4 7560)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-7560 570.5,-7560 570.5,4 -4,4\" stroke=\"transparent\"/>\n<!-- 139708938560272 -->\n<g class=\"node\" id=\"node1\">\n<title>139708938560272</title>\n<polygon fill=\"none\" points=\"117.5,-7519.5 117.5,-7555.5 286.5,-7555.5 286.5,-7519.5 117.5,-7519.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"162\" y=\"-7533.8\">Input-Token</text>\n<polyline fill=\"none\" points=\"206.5,-7519.5 206.5,-7555.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"246.5\" y=\"-7533.8\">InputLayer</text>\n</g>\n<!-- 139706858213712 -->\n<g class=\"node\" id=\"node3\">\n<title>139706858213712</title>\n<polygon fill=\"none\" points=\"79,-7446.5 79,-7482.5 325,-7482.5 325,-7446.5 79,-7446.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"141.5\" y=\"-7460.8\">Embedding-Token</text>\n<polyline fill=\"none\" points=\"204,-7446.5 204,-7482.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"264.5\" y=\"-7460.8\">TokenEmbedding</text>\n</g>\n<!-- 139708938560272&#45;&gt;139706858213712 -->\n<g class=\"edge\" id=\"edge1\">\n<title>139708938560272-&gt;139706858213712</title>\n<path d=\"M202,-7519.4551C202,-7511.3828 202,-7501.6764 202,-7492.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"205.5001,-7492.5903 202,-7482.5904 198.5001,-7492.5904 205.5001,-7492.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139706858683920 -->\n<g class=\"node\" id=\"node2\">\n<title>139706858683920</title>\n<polygon fill=\"none\" points=\"364,-7519.5 364,-7555.5 546,-7555.5 546,-7519.5 364,-7519.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"415\" y=\"-7533.8\">Input-Segment</text>\n<polyline fill=\"none\" points=\"466,-7519.5 466,-7555.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"506\" y=\"-7533.8\">InputLayer</text>\n</g>\n<!-- 139706858626128 -->\n<g class=\"node\" id=\"node4\">\n<title>139706858626128</title>\n<polygon fill=\"none\" points=\"343.5,-7446.5 343.5,-7482.5 566.5,-7482.5 566.5,-7446.5 343.5,-7446.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"413\" y=\"-7460.8\">Embedding-Segment</text>\n<polyline fill=\"none\" points=\"482.5,-7446.5 482.5,-7482.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"524.5\" y=\"-7460.8\">Embedding</text>\n</g>\n<!-- 139706858683920&#45;&gt;139706858626128 -->\n<g class=\"edge\" id=\"edge2\">\n<title>139706858683920-&gt;139706858626128</title>\n<path d=\"M455,-7519.4551C455,-7511.3828 455,-7501.6764 455,-7492.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"458.5001,-7492.5903 455,-7482.5904 451.5001,-7492.5904 458.5001,-7492.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139706858111696 -->\n<g class=\"node\" id=\"node5\">\n<title>139706858111696</title>\n<polygon fill=\"none\" points=\"217,-7373.5 217,-7409.5 439,-7409.5 439,-7373.5 217,-7373.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"307\" y=\"-7387.8\">Embedding-Token-Segment</text>\n<polyline fill=\"none\" points=\"397,-7373.5 397,-7409.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"418\" y=\"-7387.8\">Add</text>\n</g>\n<!-- 139706858213712&#45;&gt;139706858111696 -->\n<g class=\"edge\" id=\"edge3\">\n<title>139706858213712-&gt;139706858111696</title>\n<path d=\"M233.1461,-7446.4551C249.6535,-7436.8912 270.1204,-7425.0334 287.8124,-7414.7833\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"289.8774,-7417.6319 296.7755,-7409.5904 286.3682,-7411.575 289.8774,-7417.6319\" stroke=\"#000000\"/>\n</g>\n<!-- 139706858626128&#45;&gt;139706858111696 -->\n<g class=\"edge\" id=\"edge4\">\n<title>139706858626128-&gt;139706858111696</title>\n<path d=\"M423.6067,-7446.4551C406.9683,-7436.8912 386.3389,-7425.0334 368.5066,-7414.7833\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"369.8863,-7411.5394 359.4723,-7409.5904 366.3979,-7417.6083 369.8863,-7411.5394\" stroke=\"#000000\"/>\n</g>\n<!-- 139706696727952 -->\n<g class=\"node\" id=\"node6\">\n<title>139706696727952</title>\n<polygon fill=\"none\" points=\"194.5,-7300.5 194.5,-7336.5 461.5,-7336.5 461.5,-7300.5 194.5,-7300.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"262.5\" y=\"-7314.8\">Embedding-Position</text>\n<polyline fill=\"none\" points=\"330.5,-7300.5 330.5,-7336.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"396\" y=\"-7314.8\">PositionEmbedding</text>\n</g>\n<!-- 139706858111696&#45;&gt;139706696727952 -->\n<g class=\"edge\" id=\"edge5\">\n<title>139706858111696-&gt;139706696727952</title>\n<path d=\"M328,-7373.4551C328,-7365.3828 328,-7355.6764 328,-7346.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"331.5001,-7346.5903 328,-7336.5904 324.5001,-7346.5904 331.5001,-7346.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139704785032400 -->\n<g class=\"node\" id=\"node7\">\n<title>139704785032400</title>\n<polygon fill=\"none\" points=\"227,-7227.5 227,-7263.5 429,-7263.5 429,-7227.5 227,-7227.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"295.5\" y=\"-7241.8\">Embedding-Dropout</text>\n<polyline fill=\"none\" points=\"364,-7227.5 364,-7263.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"396.5\" y=\"-7241.8\">Dropout</text>\n</g>\n<!-- 139706696727952&#45;&gt;139704785032400 -->\n<g class=\"edge\" id=\"edge6\">\n<title>139706696727952-&gt;139704785032400</title>\n<path d=\"M328,-7300.4551C328,-7292.3828 328,-7282.6764 328,-7273.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"331.5001,-7273.5903 328,-7263.5904 324.5001,-7273.5904 331.5001,-7273.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139704785033808 -->\n<g class=\"node\" id=\"node8\">\n<title>139704785033808</title>\n<polygon fill=\"none\" points=\"201.5,-7154.5 201.5,-7190.5 454.5,-7190.5 454.5,-7154.5 201.5,-7154.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"262.5\" y=\"-7168.8\">Embedding-Norm</text>\n<polyline fill=\"none\" points=\"323.5,-7154.5 323.5,-7190.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"389\" y=\"-7168.8\">LayerNormalization</text>\n</g>\n<!-- 139704785032400&#45;&gt;139704785033808 -->\n<g class=\"edge\" id=\"edge7\">\n<title>139704785032400-&gt;139704785033808</title>\n<path d=\"M328,-7227.4551C328,-7219.3828 328,-7209.6764 328,-7200.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"331.5001,-7200.5903 328,-7190.5904 324.5001,-7200.5904 331.5001,-7200.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139704784910672 -->\n<g class=\"node\" id=\"node9\">\n<title>139704784910672</title>\n<polygon fill=\"none\" points=\"50.5,-7081.5 50.5,-7117.5 401.5,-7117.5 401.5,-7081.5 50.5,-7081.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"160\" y=\"-7095.8\">Encoder-1-MultiHeadSelfAttention</text>\n<polyline fill=\"none\" points=\"269.5,-7081.5 269.5,-7117.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"335.5\" y=\"-7095.8\">MultiHeadAttention</text>\n</g>\n<!-- 139704785033808&#45;&gt;139704784910672 -->\n<g class=\"edge\" id=\"edge8\">\n<title>139704785033808-&gt;139704784910672</title>\n<path d=\"M302.7865,-7154.4551C289.7912,-7145.1545 273.7645,-7133.6844 259.7204,-7123.6332\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"261.4459,-7120.5641 251.2769,-7117.5904 257.3719,-7126.2565 261.4459,-7120.5641\" stroke=\"#000000\"/>\n</g>\n<!-- 139704784708432 -->\n<g class=\"node\" id=\"node11\">\n<title>139704784708432</title>\n<polygon fill=\"none\" points=\"182.5,-6935.5 182.5,-6971.5 473.5,-6971.5 473.5,-6935.5 182.5,-6935.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"307\" y=\"-6949.8\">Encoder-1-MultiHeadSelfAttention-Add</text>\n<polyline fill=\"none\" points=\"431.5,-6935.5 431.5,-6971.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"452.5\" y=\"-6949.8\">Add</text>\n</g>\n<!-- 139704785033808&#45;&gt;139704784708432 -->\n<g class=\"edge\" id=\"edge10\">\n<title>139704785033808-&gt;139704784708432</title>\n<path d=\"M372.0299,-7154.3618C387.1906,-7145.7299 402.4854,-7133.7771 411,-7118 418.7883,-7103.5687 418.7039,-7027.2752 408,-7008 401.0407,-6995.468 389.8377,-6985.2368 377.9915,-6977.1643\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"379.6251,-6974.0561 369.3034,-6971.6658 375.8816,-6979.9711 379.6251,-6974.0561\" stroke=\"#000000\"/>\n</g>\n<!-- 139704784474384 -->\n<g class=\"node\" id=\"node10\">\n<title>139704784474384</title>\n<polygon fill=\"none\" points=\"61.5,-7008.5 61.5,-7044.5 398.5,-7044.5 398.5,-7008.5 61.5,-7008.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"197.5\" y=\"-7022.8\">Encoder-1-MultiHeadSelfAttention-Dropout</text>\n<polyline fill=\"none\" points=\"333.5,-7008.5 333.5,-7044.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"366\" y=\"-7022.8\">Dropout</text>\n</g>\n<!-- 139704784910672&#45;&gt;139704784474384 -->\n<g class=\"edge\" id=\"edge9\">\n<title>139704784910672-&gt;139704784474384</title>\n<path d=\"M226.9888,-7081.4551C227.4311,-7073.3828 227.9629,-7063.6764 228.4558,-7054.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"231.9563,-7054.7669 229.0087,-7044.5904 224.9668,-7054.3839 231.9563,-7054.7669\" stroke=\"#000000\"/>\n</g>\n<!-- 139704784474384&#45;&gt;139704784708432 -->\n<g class=\"edge\" id=\"edge11\">\n<title>139704784474384-&gt;139704784708432</title>\n<path d=\"M254.2247,-7008.4551C266.7105,-6999.1545 282.1086,-6987.6844 295.602,-6977.6332\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"297.7855,-6980.371 303.7143,-6971.5904 293.6039,-6974.7573 297.7855,-6980.371\" stroke=\"#000000\"/>\n</g>\n<!-- 139706696631568 -->\n<g class=\"node\" id=\"node12\">\n<title>139706696631568</title>\n<polygon fill=\"none\" points=\"134,-6862.5 134,-6898.5 522,-6898.5 522,-6862.5 134,-6862.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"262.5\" y=\"-6876.8\">Encoder-1-MultiHeadSelfAttention-Norm</text>\n<polyline fill=\"none\" points=\"391,-6862.5 391,-6898.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"456.5\" y=\"-6876.8\">LayerNormalization</text>\n</g>\n<!-- 139704784708432&#45;&gt;139706696631568 -->\n<g class=\"edge\" id=\"edge12\">\n<title>139704784708432-&gt;139706696631568</title>\n<path d=\"M328,-6935.4551C328,-6927.3828 328,-6917.6764 328,-6908.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"331.5001,-6908.5903 328,-6898.5904 324.5001,-6908.5904 331.5001,-6908.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139704405168336 -->\n<g class=\"node\" id=\"node13\">\n<title>139704405168336</title>\n<polygon fill=\"none\" points=\"125.5,-6789.5 125.5,-6825.5 376.5,-6825.5 376.5,-6789.5 125.5,-6789.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"204.5\" y=\"-6803.8\">Encoder-1-FeedForward</text>\n<polyline fill=\"none\" points=\"283.5,-6789.5 283.5,-6825.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"330\" y=\"-6803.8\">FeedForward</text>\n</g>\n<!-- 139706696631568&#45;&gt;139704405168336 -->\n<g class=\"edge\" id=\"edge13\">\n<title>139706696631568-&gt;139704405168336</title>\n<path d=\"M308.9663,-6862.4551C299.4337,-6853.4177 287.7405,-6842.3319 277.3592,-6832.4899\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"279.7467,-6829.9305 270.0816,-6825.5904 274.9307,-6835.0104 279.7467,-6829.9305\" stroke=\"#000000\"/>\n</g>\n<!-- 139704405541840 -->\n<g class=\"node\" id=\"node15\">\n<title>139704405541840</title>\n<polygon fill=\"none\" points=\"213,-6643.5 213,-6679.5 443,-6679.5 443,-6643.5 213,-6643.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"307\" y=\"-6657.8\">Encoder-1-FeedForward-Add</text>\n<polyline fill=\"none\" points=\"401,-6643.5 401,-6679.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"422\" y=\"-6657.8\">Add</text>\n</g>\n<!-- 139706696631568&#45;&gt;139704405541840 -->\n<g class=\"edge\" id=\"edge15\">\n<title>139706696631568-&gt;139704405541840</title>\n<path d=\"M353.9805,-6862.4928C365.5008,-6853.0346 378.0077,-6840.4122 385,-6826 406.3833,-6781.9255 412.5416,-6760.473 392,-6716 386.5436,-6704.1867 377.1242,-6693.9911 367.1643,-6685.7193\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"369.178,-6682.8521 359.1262,-6679.5038 364.896,-6688.3897 369.178,-6682.8521\" stroke=\"#000000\"/>\n</g>\n<!-- 139704784631632 -->\n<g class=\"node\" id=\"node14\">\n<title>139704784631632</title>\n<polygon fill=\"none\" points=\"107,-6716.5 107,-6752.5 383,-6752.5 383,-6716.5 107,-6716.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"212.5\" y=\"-6730.8\">Encoder-1-FeedForward-Dropout</text>\n<polyline fill=\"none\" points=\"318,-6716.5 318,-6752.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"350.5\" y=\"-6730.8\">Dropout</text>\n</g>\n<!-- 139704405168336&#45;&gt;139704784631632 -->\n<g class=\"edge\" id=\"edge14\">\n<title>139704405168336-&gt;139704784631632</title>\n<path d=\"M249.5169,-6789.4551C248.8534,-6781.3828 248.0556,-6771.6764 247.3163,-6762.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"250.7944,-6762.27 246.4869,-6752.5904 243.8179,-6762.8435 250.7944,-6762.27\" stroke=\"#000000\"/>\n</g>\n<!-- 139704784631632&#45;&gt;139704405541840 -->\n<g class=\"edge\" id=\"edge16\">\n<title>139704784631632-&gt;139704405541840</title>\n<path d=\"M265.5169,-6716.4551C275.892,-6707.3299 288.6416,-6696.1165 299.9123,-6686.2036\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"302.2341,-6688.8228 307.4315,-6679.5904 297.6111,-6683.5665 302.2341,-6688.8228\" stroke=\"#000000\"/>\n</g>\n<!-- 139704405536272 -->\n<g class=\"node\" id=\"node16\">\n<title>139704405536272</title>\n<polygon fill=\"none\" points=\"164.5,-6570.5 164.5,-6606.5 491.5,-6606.5 491.5,-6570.5 164.5,-6570.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"262.5\" y=\"-6584.8\">Encoder-1-FeedForward-Norm</text>\n<polyline fill=\"none\" points=\"360.5,-6570.5 360.5,-6606.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"426\" y=\"-6584.8\">LayerNormalization</text>\n</g>\n<!-- 139704405541840&#45;&gt;139704405536272 -->\n<g class=\"edge\" id=\"edge17\">\n<title>139704405541840-&gt;139704405536272</title>\n<path d=\"M328,-6643.4551C328,-6635.3828 328,-6625.6764 328,-6616.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"331.5001,-6616.5903 328,-6606.5904 324.5001,-6616.5904 331.5001,-6616.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139704405416720 -->\n<g class=\"node\" id=\"node17\">\n<title>139704405416720</title>\n<polygon fill=\"none\" points=\"50.5,-6497.5 50.5,-6533.5 401.5,-6533.5 401.5,-6497.5 50.5,-6497.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"160\" y=\"-6511.8\">Encoder-2-MultiHeadSelfAttention</text>\n<polyline fill=\"none\" points=\"269.5,-6497.5 269.5,-6533.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"335.5\" y=\"-6511.8\">MultiHeadAttention</text>\n</g>\n<!-- 139704405536272&#45;&gt;139704405416720 -->\n<g class=\"edge\" id=\"edge18\">\n<title>139704405536272-&gt;139704405416720</title>\n<path d=\"M302.7865,-6570.4551C289.7912,-6561.1545 273.7645,-6549.6844 259.7204,-6539.6332\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"261.4459,-6536.5641 251.2769,-6533.5904 257.3719,-6542.2565 261.4459,-6536.5641\" stroke=\"#000000\"/>\n</g>\n<!-- 139704784557392 -->\n<g class=\"node\" id=\"node19\">\n<title>139704784557392</title>\n<polygon fill=\"none\" points=\"183.5,-6351.5 183.5,-6387.5 474.5,-6387.5 474.5,-6351.5 183.5,-6351.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"308\" y=\"-6365.8\">Encoder-2-MultiHeadSelfAttention-Add</text>\n<polyline fill=\"none\" points=\"432.5,-6351.5 432.5,-6387.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"453.5\" y=\"-6365.8\">Add</text>\n</g>\n<!-- 139704405536272&#45;&gt;139704784557392 -->\n<g class=\"edge\" id=\"edge20\">\n<title>139704405536272-&gt;139704784557392</title>\n<path d=\"M372.0299,-6570.3618C387.1906,-6561.7299 402.4854,-6549.7771 411,-6534 419.1991,-6518.8075 416.9679,-6441.9111 407,-6424 400.0832,-6411.5713 388.9956,-6401.3379 377.3254,-6393.229\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"379.0734,-6390.1914 368.7759,-6387.699 375.2716,-6396.069 379.0734,-6390.1914\" stroke=\"#000000\"/>\n</g>\n<!-- 139704783832400 -->\n<g class=\"node\" id=\"node18\">\n<title>139704783832400</title>\n<polygon fill=\"none\" points=\"60.5,-6424.5 60.5,-6460.5 397.5,-6460.5 397.5,-6424.5 60.5,-6424.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"196.5\" y=\"-6438.8\">Encoder-2-MultiHeadSelfAttention-Dropout</text>\n<polyline fill=\"none\" points=\"332.5,-6424.5 332.5,-6460.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"365\" y=\"-6438.8\">Dropout</text>\n</g>\n<!-- 139704405416720&#45;&gt;139704783832400 -->\n<g class=\"edge\" id=\"edge19\">\n<title>139704405416720-&gt;139704783832400</title>\n<path d=\"M226.7416,-6497.4551C227.0733,-6489.3828 227.4722,-6479.6764 227.8418,-6470.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"231.3429,-6470.7257 228.2566,-6460.5904 224.3488,-6470.4382 231.3429,-6470.7257\" stroke=\"#000000\"/>\n</g>\n<!-- 139704783832400&#45;&gt;139704784557392 -->\n<g class=\"edge\" id=\"edge21\">\n<title>139704783832400-&gt;139704784557392</title>\n<path d=\"M253.7191,-6424.4551C266.4597,-6415.1545 282.1721,-6403.6844 295.9408,-6393.6332\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"298.2055,-6396.3134 304.2187,-6387.5904 294.0781,-6390.6596 298.2055,-6396.3134\" stroke=\"#000000\"/>\n</g>\n<!-- 139704784965520 -->\n<g class=\"node\" id=\"node20\">\n<title>139704784965520</title>\n<polygon fill=\"none\" points=\"135,-6278.5 135,-6314.5 523,-6314.5 523,-6278.5 135,-6278.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263.5\" y=\"-6292.8\">Encoder-2-MultiHeadSelfAttention-Norm</text>\n<polyline fill=\"none\" points=\"392,-6278.5 392,-6314.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"457.5\" y=\"-6292.8\">LayerNormalization</text>\n</g>\n<!-- 139704784557392&#45;&gt;139704784965520 -->\n<g class=\"edge\" id=\"edge22\">\n<title>139704784557392-&gt;139704784965520</title>\n<path d=\"M329,-6351.4551C329,-6343.3828 329,-6333.6764 329,-6324.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"332.5001,-6324.5903 329,-6314.5904 325.5001,-6324.5904 332.5001,-6324.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139704405414480 -->\n<g class=\"node\" id=\"node21\">\n<title>139704405414480</title>\n<polygon fill=\"none\" points=\"120.5,-6205.5 120.5,-6241.5 371.5,-6241.5 371.5,-6205.5 120.5,-6205.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"199.5\" y=\"-6219.8\">Encoder-2-FeedForward</text>\n<polyline fill=\"none\" points=\"278.5,-6205.5 278.5,-6241.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"325\" y=\"-6219.8\">FeedForward</text>\n</g>\n<!-- 139704784965520&#45;&gt;139704405414480 -->\n<g class=\"edge\" id=\"edge23\">\n<title>139704784965520-&gt;139704405414480</title>\n<path d=\"M308.4831,-6278.4551C298.108,-6269.3299 285.3584,-6258.1165 274.0877,-6248.2036\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"276.3889,-6245.5665 266.5685,-6241.5904 271.7659,-6250.8228 276.3889,-6245.5665\" stroke=\"#000000\"/>\n</g>\n<!-- 139704783834704 -->\n<g class=\"node\" id=\"node23\">\n<title>139704783834704</title>\n<polygon fill=\"none\" points=\"208,-6059.5 208,-6095.5 438,-6095.5 438,-6059.5 208,-6059.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"302\" y=\"-6073.8\">Encoder-2-FeedForward-Add</text>\n<polyline fill=\"none\" points=\"396,-6059.5 396,-6095.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"417\" y=\"-6073.8\">Add</text>\n</g>\n<!-- 139704784965520&#45;&gt;139704783834704 -->\n<g class=\"edge\" id=\"edge25\">\n<title>139704784965520-&gt;139704783834704</title>\n<path d=\"M351.7353,-6278.457C362.1909,-6268.8203 373.657,-6256.0556 380,-6242 400.1503,-6197.3484 407.5416,-6176.473 387,-6132 381.5436,-6120.1867 372.1242,-6109.9911 362.1643,-6101.7193\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"364.178,-6098.8521 354.1262,-6095.5038 359.896,-6104.3897 364.178,-6098.8521\" stroke=\"#000000\"/>\n</g>\n<!-- 139704784984272 -->\n<g class=\"node\" id=\"node22\">\n<title>139704784984272</title>\n<polygon fill=\"none\" points=\"102,-6132.5 102,-6168.5 378,-6168.5 378,-6132.5 102,-6132.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"207.5\" y=\"-6146.8\">Encoder-2-FeedForward-Dropout</text>\n<polyline fill=\"none\" points=\"313,-6132.5 313,-6168.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"345.5\" y=\"-6146.8\">Dropout</text>\n</g>\n<!-- 139704405414480&#45;&gt;139704784984272 -->\n<g class=\"edge\" id=\"edge24\">\n<title>139704405414480-&gt;139704784984272</title>\n<path d=\"M244.5169,-6205.4551C243.8534,-6197.3828 243.0556,-6187.6764 242.3163,-6178.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"245.7944,-6178.27 241.4869,-6168.5904 238.8179,-6178.8435 245.7944,-6178.27\" stroke=\"#000000\"/>\n</g>\n<!-- 139704784984272&#45;&gt;139704783834704 -->\n<g class=\"edge\" id=\"edge26\">\n<title>139704784984272-&gt;139704783834704</title>\n<path d=\"M260.5169,-6132.4551C270.892,-6123.3299 283.6416,-6112.1165 294.9123,-6102.2036\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"297.2341,-6104.8228 302.4315,-6095.5904 292.6111,-6099.5665 297.2341,-6104.8228\" stroke=\"#000000\"/>\n</g>\n<!-- 139704783831696 -->\n<g class=\"node\" id=\"node24\">\n<title>139704783831696</title>\n<polygon fill=\"none\" points=\"159.5,-5986.5 159.5,-6022.5 486.5,-6022.5 486.5,-5986.5 159.5,-5986.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"257.5\" y=\"-6000.8\">Encoder-2-FeedForward-Norm</text>\n<polyline fill=\"none\" points=\"355.5,-5986.5 355.5,-6022.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"421\" y=\"-6000.8\">LayerNormalization</text>\n</g>\n<!-- 139704783834704&#45;&gt;139704783831696 -->\n<g class=\"edge\" id=\"edge27\">\n<title>139704783834704-&gt;139704783831696</title>\n<path d=\"M323,-6059.4551C323,-6051.3828 323,-6041.6764 323,-6032.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"326.5001,-6032.5903 323,-6022.5904 319.5001,-6032.5904 326.5001,-6032.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139704785440848 -->\n<g class=\"node\" id=\"node25\">\n<title>139704785440848</title>\n<polygon fill=\"none\" points=\"45.5,-5913.5 45.5,-5949.5 396.5,-5949.5 396.5,-5913.5 45.5,-5913.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155\" y=\"-5927.8\">Encoder-3-MultiHeadSelfAttention</text>\n<polyline fill=\"none\" points=\"264.5,-5913.5 264.5,-5949.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"330.5\" y=\"-5927.8\">MultiHeadAttention</text>\n</g>\n<!-- 139704783831696&#45;&gt;139704785440848 -->\n<g class=\"edge\" id=\"edge28\">\n<title>139704783831696-&gt;139704785440848</title>\n<path d=\"M297.7865,-5986.4551C284.7912,-5977.1545 268.7645,-5965.6844 254.7204,-5955.6332\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"256.4459,-5952.5641 246.2769,-5949.5904 252.3719,-5958.2565 256.4459,-5952.5641\" stroke=\"#000000\"/>\n</g>\n<!-- 139704784559312 -->\n<g class=\"node\" id=\"node27\">\n<title>139704784559312</title>\n<polygon fill=\"none\" points=\"178.5,-5767.5 178.5,-5803.5 469.5,-5803.5 469.5,-5767.5 178.5,-5767.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"303\" y=\"-5781.8\">Encoder-3-MultiHeadSelfAttention-Add</text>\n<polyline fill=\"none\" points=\"427.5,-5767.5 427.5,-5803.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"448.5\" y=\"-5781.8\">Add</text>\n</g>\n<!-- 139704783831696&#45;&gt;139704784559312 -->\n<g class=\"edge\" id=\"edge30\">\n<title>139704783831696-&gt;139704784559312</title>\n<path d=\"M367.0299,-5986.3618C382.1906,-5977.7299 397.4854,-5965.7771 406,-5950 414.1991,-5934.8075 411.9679,-5857.9111 402,-5840 395.0832,-5827.5713 383.9956,-5817.3379 372.3254,-5809.229\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"374.0734,-5806.1914 363.7759,-5803.699 370.2716,-5812.069 374.0734,-5806.1914\" stroke=\"#000000\"/>\n</g>\n<!-- 139704405443792 -->\n<g class=\"node\" id=\"node26\">\n<title>139704405443792</title>\n<polygon fill=\"none\" points=\"55.5,-5840.5 55.5,-5876.5 392.5,-5876.5 392.5,-5840.5 55.5,-5840.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"191.5\" y=\"-5854.8\">Encoder-3-MultiHeadSelfAttention-Dropout</text>\n<polyline fill=\"none\" points=\"327.5,-5840.5 327.5,-5876.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"360\" y=\"-5854.8\">Dropout</text>\n</g>\n<!-- 139704785440848&#45;&gt;139704405443792 -->\n<g class=\"edge\" id=\"edge29\">\n<title>139704785440848-&gt;139704405443792</title>\n<path d=\"M221.7416,-5913.4551C222.0733,-5905.3828 222.4722,-5895.6764 222.8418,-5886.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"226.3429,-5886.7257 223.2566,-5876.5904 219.3488,-5886.4382 226.3429,-5886.7257\" stroke=\"#000000\"/>\n</g>\n<!-- 139704405443792&#45;&gt;139704784559312 -->\n<g class=\"edge\" id=\"edge31\">\n<title>139704405443792-&gt;139704784559312</title>\n<path d=\"M248.7191,-5840.4551C261.4597,-5831.1545 277.1721,-5819.6844 290.9408,-5809.6332\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"293.2055,-5812.3134 299.2187,-5803.5904 289.0781,-5806.6596 293.2055,-5812.3134\" stroke=\"#000000\"/>\n</g>\n<!-- 139704783716752 -->\n<g class=\"node\" id=\"node28\">\n<title>139704783716752</title>\n<polygon fill=\"none\" points=\"130,-5694.5 130,-5730.5 518,-5730.5 518,-5694.5 130,-5694.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"258.5\" y=\"-5708.8\">Encoder-3-MultiHeadSelfAttention-Norm</text>\n<polyline fill=\"none\" points=\"387,-5694.5 387,-5730.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"452.5\" y=\"-5708.8\">LayerNormalization</text>\n</g>\n<!-- 139704784559312&#45;&gt;139704783716752 -->\n<g class=\"edge\" id=\"edge32\">\n<title>139704784559312-&gt;139704783716752</title>\n<path d=\"M324,-5767.4551C324,-5759.3828 324,-5749.6764 324,-5740.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"327.5001,-5740.5903 324,-5730.5904 320.5001,-5740.5904 327.5001,-5740.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139704404286160 -->\n<g class=\"node\" id=\"node29\">\n<title>139704404286160</title>\n<polygon fill=\"none\" points=\"115.5,-5621.5 115.5,-5657.5 366.5,-5657.5 366.5,-5621.5 115.5,-5621.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"194.5\" y=\"-5635.8\">Encoder-3-FeedForward</text>\n<polyline fill=\"none\" points=\"273.5,-5621.5 273.5,-5657.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"320\" y=\"-5635.8\">FeedForward</text>\n</g>\n<!-- 139704783716752&#45;&gt;139704404286160 -->\n<g class=\"edge\" id=\"edge33\">\n<title>139704783716752-&gt;139704404286160</title>\n<path d=\"M303.4831,-5694.4551C293.108,-5685.3299 280.3584,-5674.1165 269.0877,-5664.2036\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"271.3889,-5661.5665 261.5685,-5657.5904 266.7659,-5666.8228 271.3889,-5661.5665\" stroke=\"#000000\"/>\n</g>\n<!-- 139704784737488 -->\n<g class=\"node\" id=\"node31\">\n<title>139704784737488</title>\n<polygon fill=\"none\" points=\"203,-5475.5 203,-5511.5 433,-5511.5 433,-5475.5 203,-5475.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"297\" y=\"-5489.8\">Encoder-3-FeedForward-Add</text>\n<polyline fill=\"none\" points=\"391,-5475.5 391,-5511.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"412\" y=\"-5489.8\">Add</text>\n</g>\n<!-- 139704783716752&#45;&gt;139704784737488 -->\n<g class=\"edge\" id=\"edge35\">\n<title>139704783716752-&gt;139704784737488</title>\n<path d=\"M346.7353,-5694.457C357.1909,-5684.8203 368.657,-5672.0556 375,-5658 395.1503,-5613.3484 402.5416,-5592.473 382,-5548 376.5436,-5536.1867 367.1242,-5525.9911 357.1643,-5517.7193\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"359.178,-5514.8521 349.1262,-5511.5038 354.896,-5520.3897 359.178,-5514.8521\" stroke=\"#000000\"/>\n</g>\n<!-- 139704404333648 -->\n<g class=\"node\" id=\"node30\">\n<title>139704404333648</title>\n<polygon fill=\"none\" points=\"97,-5548.5 97,-5584.5 373,-5584.5 373,-5548.5 97,-5548.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"202.5\" y=\"-5562.8\">Encoder-3-FeedForward-Dropout</text>\n<polyline fill=\"none\" points=\"308,-5548.5 308,-5584.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"340.5\" y=\"-5562.8\">Dropout</text>\n</g>\n<!-- 139704404286160&#45;&gt;139704404333648 -->\n<g class=\"edge\" id=\"edge34\">\n<title>139704404286160-&gt;139704404333648</title>\n<path d=\"M239.5169,-5621.4551C238.8534,-5613.3828 238.0556,-5603.6764 237.3163,-5594.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"240.7944,-5594.27 236.4869,-5584.5904 233.8179,-5594.8435 240.7944,-5594.27\" stroke=\"#000000\"/>\n</g>\n<!-- 139704404333648&#45;&gt;139704784737488 -->\n<g class=\"edge\" id=\"edge36\">\n<title>139704404333648-&gt;139704784737488</title>\n<path d=\"M255.5169,-5548.4551C265.892,-5539.3299 278.6416,-5528.1165 289.9123,-5518.2036\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"292.2341,-5520.8228 297.4315,-5511.5904 287.6111,-5515.5665 292.2341,-5520.8228\" stroke=\"#000000\"/>\n</g>\n<!-- 139704784585744 -->\n<g class=\"node\" id=\"node32\">\n<title>139704784585744</title>\n<polygon fill=\"none\" points=\"154.5,-5402.5 154.5,-5438.5 481.5,-5438.5 481.5,-5402.5 154.5,-5402.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"252.5\" y=\"-5416.8\">Encoder-3-FeedForward-Norm</text>\n<polyline fill=\"none\" points=\"350.5,-5402.5 350.5,-5438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"416\" y=\"-5416.8\">LayerNormalization</text>\n</g>\n<!-- 139704784737488&#45;&gt;139704784585744 -->\n<g class=\"edge\" id=\"edge37\">\n<title>139704784737488-&gt;139704784585744</title>\n<path d=\"M318,-5475.4551C318,-5467.3828 318,-5457.6764 318,-5448.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"321.5001,-5448.5903 318,-5438.5904 314.5001,-5448.5904 321.5001,-5448.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139704783841424 -->\n<g class=\"node\" id=\"node33\">\n<title>139704783841424</title>\n<polygon fill=\"none\" points=\"40.5,-5329.5 40.5,-5365.5 391.5,-5365.5 391.5,-5329.5 40.5,-5329.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"150\" y=\"-5343.8\">Encoder-4-MultiHeadSelfAttention</text>\n<polyline fill=\"none\" points=\"259.5,-5329.5 259.5,-5365.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"325.5\" y=\"-5343.8\">MultiHeadAttention</text>\n</g>\n<!-- 139704784585744&#45;&gt;139704783841424 -->\n<g class=\"edge\" id=\"edge38\">\n<title>139704784585744-&gt;139704783841424</title>\n<path d=\"M292.7865,-5402.4551C279.7912,-5393.1545 263.7645,-5381.6844 249.7204,-5371.6332\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"251.4459,-5368.5641 241.2769,-5365.5904 247.3719,-5374.2565 251.4459,-5368.5641\" stroke=\"#000000\"/>\n</g>\n<!-- 139706696709840 -->\n<g class=\"node\" id=\"node35\">\n<title>139706696709840</title>\n<polygon fill=\"none\" points=\"173.5,-5183.5 173.5,-5219.5 464.5,-5219.5 464.5,-5183.5 173.5,-5183.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"298\" y=\"-5197.8\">Encoder-4-MultiHeadSelfAttention-Add</text>\n<polyline fill=\"none\" points=\"422.5,-5183.5 422.5,-5219.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"443.5\" y=\"-5197.8\">Add</text>\n</g>\n<!-- 139704784585744&#45;&gt;139706696709840 -->\n<g class=\"edge\" id=\"edge40\">\n<title>139704784585744-&gt;139706696709840</title>\n<path d=\"M362.0299,-5402.3618C377.1906,-5393.7299 392.4854,-5381.7771 401,-5366 409.1991,-5350.8075 406.9679,-5273.9111 397,-5256 390.0832,-5243.5713 378.9956,-5233.3379 367.3254,-5225.229\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"369.0734,-5222.1914 358.7759,-5219.699 365.2716,-5228.069 369.0734,-5222.1914\" stroke=\"#000000\"/>\n</g>\n<!-- 139704783842704 -->\n<g class=\"node\" id=\"node34\">\n<title>139704783842704</title>\n<polygon fill=\"none\" points=\"50.5,-5256.5 50.5,-5292.5 387.5,-5292.5 387.5,-5256.5 50.5,-5256.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"186.5\" y=\"-5270.8\">Encoder-4-MultiHeadSelfAttention-Dropout</text>\n<polyline fill=\"none\" points=\"322.5,-5256.5 322.5,-5292.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"355\" y=\"-5270.8\">Dropout</text>\n</g>\n<!-- 139704783841424&#45;&gt;139704783842704 -->\n<g class=\"edge\" id=\"edge39\">\n<title>139704783841424-&gt;139704783842704</title>\n<path d=\"M216.7416,-5329.4551C217.0733,-5321.3828 217.4722,-5311.6764 217.8418,-5302.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"221.3429,-5302.7257 218.2566,-5292.5904 214.3488,-5302.4382 221.3429,-5302.7257\" stroke=\"#000000\"/>\n</g>\n<!-- 139704783842704&#45;&gt;139706696709840 -->\n<g class=\"edge\" id=\"edge41\">\n<title>139704783842704-&gt;139706696709840</title>\n<path d=\"M243.7191,-5256.4551C256.4597,-5247.1545 272.1721,-5235.6844 285.9408,-5225.6332\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"288.2055,-5228.3134 294.2187,-5219.5904 284.0781,-5222.6596 288.2055,-5228.3134\" stroke=\"#000000\"/>\n</g>\n<!-- 139704784511632 -->\n<g class=\"node\" id=\"node36\">\n<title>139704784511632</title>\n<polygon fill=\"none\" points=\"125,-5110.5 125,-5146.5 513,-5146.5 513,-5110.5 125,-5110.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"253.5\" y=\"-5124.8\">Encoder-4-MultiHeadSelfAttention-Norm</text>\n<polyline fill=\"none\" points=\"382,-5110.5 382,-5146.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"447.5\" y=\"-5124.8\">LayerNormalization</text>\n</g>\n<!-- 139706696709840&#45;&gt;139704784511632 -->\n<g class=\"edge\" id=\"edge42\">\n<title>139706696709840-&gt;139704784511632</title>\n<path d=\"M319,-5183.4551C319,-5175.3828 319,-5165.6764 319,-5156.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"322.5001,-5156.5903 319,-5146.5904 315.5001,-5156.5904 322.5001,-5156.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139704404285328 -->\n<g class=\"node\" id=\"node37\">\n<title>139704404285328</title>\n<polygon fill=\"none\" points=\"110.5,-5037.5 110.5,-5073.5 361.5,-5073.5 361.5,-5037.5 110.5,-5037.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"189.5\" y=\"-5051.8\">Encoder-4-FeedForward</text>\n<polyline fill=\"none\" points=\"268.5,-5037.5 268.5,-5073.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"315\" y=\"-5051.8\">FeedForward</text>\n</g>\n<!-- 139704784511632&#45;&gt;139704404285328 -->\n<g class=\"edge\" id=\"edge43\">\n<title>139704784511632-&gt;139704404285328</title>\n<path d=\"M298.4831,-5110.4551C288.108,-5101.3299 275.3584,-5090.1165 264.0877,-5080.2036\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"266.3889,-5077.5665 256.5685,-5073.5904 261.7659,-5082.8228 266.3889,-5077.5665\" stroke=\"#000000\"/>\n</g>\n<!-- 139708836106448 -->\n<g class=\"node\" id=\"node39\">\n<title>139708836106448</title>\n<polygon fill=\"none\" points=\"198,-4891.5 198,-4927.5 428,-4927.5 428,-4891.5 198,-4891.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"292\" y=\"-4905.8\">Encoder-4-FeedForward-Add</text>\n<polyline fill=\"none\" points=\"386,-4891.5 386,-4927.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"407\" y=\"-4905.8\">Add</text>\n</g>\n<!-- 139704784511632&#45;&gt;139708836106448 -->\n<g class=\"edge\" id=\"edge45\">\n<title>139704784511632-&gt;139708836106448</title>\n<path d=\"M341.7353,-5110.457C352.1909,-5100.8203 363.657,-5088.0556 370,-5074 390.1503,-5029.3484 397.5416,-5008.473 377,-4964 371.5436,-4952.1867 362.1242,-4941.9911 352.1643,-4933.7193\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"354.178,-4930.8521 344.1262,-4927.5038 349.896,-4936.3897 354.178,-4930.8521\" stroke=\"#000000\"/>\n</g>\n<!-- 139704785372816 -->\n<g class=\"node\" id=\"node38\">\n<title>139704785372816</title>\n<polygon fill=\"none\" points=\"92,-4964.5 92,-5000.5 368,-5000.5 368,-4964.5 92,-4964.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"197.5\" y=\"-4978.8\">Encoder-4-FeedForward-Dropout</text>\n<polyline fill=\"none\" points=\"303,-4964.5 303,-5000.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"335.5\" y=\"-4978.8\">Dropout</text>\n</g>\n<!-- 139704404285328&#45;&gt;139704785372816 -->\n<g class=\"edge\" id=\"edge44\">\n<title>139704404285328-&gt;139704785372816</title>\n<path d=\"M234.5169,-5037.4551C233.8534,-5029.3828 233.0556,-5019.6764 232.3163,-5010.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"235.7944,-5010.27 231.4869,-5000.5904 228.8179,-5010.8435 235.7944,-5010.27\" stroke=\"#000000\"/>\n</g>\n<!-- 139704785372816&#45;&gt;139708836106448 -->\n<g class=\"edge\" id=\"edge46\">\n<title>139704785372816-&gt;139708836106448</title>\n<path d=\"M250.5169,-4964.4551C260.892,-4955.3299 273.6416,-4944.1165 284.9123,-4934.2036\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"287.2341,-4936.8228 292.4315,-4927.5904 282.6111,-4931.5665 287.2341,-4936.8228\" stroke=\"#000000\"/>\n</g>\n<!-- 139704405443408 -->\n<g class=\"node\" id=\"node40\">\n<title>139704405443408</title>\n<polygon fill=\"none\" points=\"149.5,-4818.5 149.5,-4854.5 476.5,-4854.5 476.5,-4818.5 149.5,-4818.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"247.5\" y=\"-4832.8\">Encoder-4-FeedForward-Norm</text>\n<polyline fill=\"none\" points=\"345.5,-4818.5 345.5,-4854.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"411\" y=\"-4832.8\">LayerNormalization</text>\n</g>\n<!-- 139708836106448&#45;&gt;139704405443408 -->\n<g class=\"edge\" id=\"edge47\">\n<title>139708836106448-&gt;139704405443408</title>\n<path d=\"M313,-4891.4551C313,-4883.3828 313,-4873.6764 313,-4864.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"316.5001,-4864.5903 313,-4854.5904 309.5001,-4864.5904 316.5001,-4864.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139704403802384 -->\n<g class=\"node\" id=\"node41\">\n<title>139704403802384</title>\n<polygon fill=\"none\" points=\"35.5,-4745.5 35.5,-4781.5 386.5,-4781.5 386.5,-4745.5 35.5,-4745.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"145\" y=\"-4759.8\">Encoder-5-MultiHeadSelfAttention</text>\n<polyline fill=\"none\" points=\"254.5,-4745.5 254.5,-4781.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"320.5\" y=\"-4759.8\">MultiHeadAttention</text>\n</g>\n<!-- 139704405443408&#45;&gt;139704403802384 -->\n<g class=\"edge\" id=\"edge48\">\n<title>139704405443408-&gt;139704403802384</title>\n<path d=\"M287.7865,-4818.4551C274.7912,-4809.1545 258.7645,-4797.6844 244.7204,-4787.6332\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"246.4459,-4784.5641 236.2769,-4781.5904 242.3719,-4790.2565 246.4459,-4784.5641\" stroke=\"#000000\"/>\n</g>\n<!-- 139704404611216 -->\n<g class=\"node\" id=\"node43\">\n<title>139704404611216</title>\n<polygon fill=\"none\" points=\"168.5,-4599.5 168.5,-4635.5 459.5,-4635.5 459.5,-4599.5 168.5,-4599.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"293\" y=\"-4613.8\">Encoder-5-MultiHeadSelfAttention-Add</text>\n<polyline fill=\"none\" points=\"417.5,-4599.5 417.5,-4635.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"438.5\" y=\"-4613.8\">Add</text>\n</g>\n<!-- 139704405443408&#45;&gt;139704404611216 -->\n<g class=\"edge\" id=\"edge50\">\n<title>139704405443408-&gt;139704404611216</title>\n<path d=\"M357.0299,-4818.3618C372.1906,-4809.7299 387.4854,-4797.7771 396,-4782 404.1991,-4766.8075 401.9679,-4689.9111 392,-4672 385.0832,-4659.5713 373.9956,-4649.3379 362.3254,-4641.229\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"364.0734,-4638.1914 353.7759,-4635.699 360.2716,-4644.069 364.0734,-4638.1914\" stroke=\"#000000\"/>\n</g>\n<!-- 139704785335760 -->\n<g class=\"node\" id=\"node42\">\n<title>139704785335760</title>\n<polygon fill=\"none\" points=\"45.5,-4672.5 45.5,-4708.5 382.5,-4708.5 382.5,-4672.5 45.5,-4672.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"181.5\" y=\"-4686.8\">Encoder-5-MultiHeadSelfAttention-Dropout</text>\n<polyline fill=\"none\" points=\"317.5,-4672.5 317.5,-4708.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"350\" y=\"-4686.8\">Dropout</text>\n</g>\n<!-- 139704403802384&#45;&gt;139704785335760 -->\n<g class=\"edge\" id=\"edge49\">\n<title>139704403802384-&gt;139704785335760</title>\n<path d=\"M211.7416,-4745.4551C212.0733,-4737.3828 212.4722,-4727.6764 212.8418,-4718.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"216.3429,-4718.7257 213.2566,-4708.5904 209.3488,-4718.4382 216.3429,-4718.7257\" stroke=\"#000000\"/>\n</g>\n<!-- 139704785335760&#45;&gt;139704404611216 -->\n<g class=\"edge\" id=\"edge51\">\n<title>139704785335760-&gt;139704404611216</title>\n<path d=\"M238.7191,-4672.4551C251.4597,-4663.1545 267.1721,-4651.6844 280.9408,-4641.6332\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"283.2055,-4644.3134 289.2187,-4635.5904 279.0781,-4638.6596 283.2055,-4644.3134\" stroke=\"#000000\"/>\n</g>\n<!-- 139704405132944 -->\n<g class=\"node\" id=\"node44\">\n<title>139704405132944</title>\n<polygon fill=\"none\" points=\"120,-4526.5 120,-4562.5 508,-4562.5 508,-4526.5 120,-4526.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"248.5\" y=\"-4540.8\">Encoder-5-MultiHeadSelfAttention-Norm</text>\n<polyline fill=\"none\" points=\"377,-4526.5 377,-4562.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"442.5\" y=\"-4540.8\">LayerNormalization</text>\n</g>\n<!-- 139704404611216&#45;&gt;139704405132944 -->\n<g class=\"edge\" id=\"edge52\">\n<title>139704404611216-&gt;139704405132944</title>\n<path d=\"M314,-4599.4551C314,-4591.3828 314,-4581.6764 314,-4572.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"317.5001,-4572.5903 314,-4562.5904 310.5001,-4572.5904 317.5001,-4572.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139704404335504 -->\n<g class=\"node\" id=\"node45\">\n<title>139704404335504</title>\n<polygon fill=\"none\" points=\"105.5,-4453.5 105.5,-4489.5 356.5,-4489.5 356.5,-4453.5 105.5,-4453.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"184.5\" y=\"-4467.8\">Encoder-5-FeedForward</text>\n<polyline fill=\"none\" points=\"263.5,-4453.5 263.5,-4489.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"310\" y=\"-4467.8\">FeedForward</text>\n</g>\n<!-- 139704405132944&#45;&gt;139704404335504 -->\n<g class=\"edge\" id=\"edge53\">\n<title>139704405132944-&gt;139704404335504</title>\n<path d=\"M293.4831,-4526.4551C283.108,-4517.3299 270.3584,-4506.1165 259.0877,-4496.2036\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"261.3889,-4493.5665 251.5685,-4489.5904 256.7659,-4498.8228 261.3889,-4493.5665\" stroke=\"#000000\"/>\n</g>\n<!-- 139704404283472 -->\n<g class=\"node\" id=\"node47\">\n<title>139704404283472</title>\n<polygon fill=\"none\" points=\"193,-4307.5 193,-4343.5 423,-4343.5 423,-4307.5 193,-4307.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"287\" y=\"-4321.8\">Encoder-5-FeedForward-Add</text>\n<polyline fill=\"none\" points=\"381,-4307.5 381,-4343.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"402\" y=\"-4321.8\">Add</text>\n</g>\n<!-- 139704405132944&#45;&gt;139704404283472 -->\n<g class=\"edge\" id=\"edge55\">\n<title>139704405132944-&gt;139704404283472</title>\n<path d=\"M336.7353,-4526.457C347.1909,-4516.8203 358.657,-4504.0556 365,-4490 385.1503,-4445.3484 392.5416,-4424.473 372,-4380 366.5436,-4368.1867 357.1242,-4357.9911 347.1643,-4349.7193\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"349.178,-4346.8521 339.1262,-4343.5038 344.896,-4352.3897 349.178,-4346.8521\" stroke=\"#000000\"/>\n</g>\n<!-- 139704784585872 -->\n<g class=\"node\" id=\"node46\">\n<title>139704784585872</title>\n<polygon fill=\"none\" points=\"87,-4380.5 87,-4416.5 363,-4416.5 363,-4380.5 87,-4380.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"192.5\" y=\"-4394.8\">Encoder-5-FeedForward-Dropout</text>\n<polyline fill=\"none\" points=\"298,-4380.5 298,-4416.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"330.5\" y=\"-4394.8\">Dropout</text>\n</g>\n<!-- 139704404335504&#45;&gt;139704784585872 -->\n<g class=\"edge\" id=\"edge54\">\n<title>139704404335504-&gt;139704784585872</title>\n<path d=\"M229.5169,-4453.4551C228.8534,-4445.3828 228.0556,-4435.6764 227.3163,-4426.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"230.7944,-4426.27 226.4869,-4416.5904 223.8179,-4426.8435 230.7944,-4426.27\" stroke=\"#000000\"/>\n</g>\n<!-- 139704784585872&#45;&gt;139704404283472 -->\n<g class=\"edge\" id=\"edge56\">\n<title>139704784585872-&gt;139704404283472</title>\n<path d=\"M245.5169,-4380.4551C255.892,-4371.3299 268.6416,-4360.1165 279.9123,-4350.2036\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"282.2341,-4352.8228 287.4315,-4343.5904 277.6111,-4347.5665 282.2341,-4352.8228\" stroke=\"#000000\"/>\n</g>\n<!-- 139704784786576 -->\n<g class=\"node\" id=\"node48\">\n<title>139704784786576</title>\n<polygon fill=\"none\" points=\"144.5,-4234.5 144.5,-4270.5 471.5,-4270.5 471.5,-4234.5 144.5,-4234.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"242.5\" y=\"-4248.8\">Encoder-5-FeedForward-Norm</text>\n<polyline fill=\"none\" points=\"340.5,-4234.5 340.5,-4270.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"406\" y=\"-4248.8\">LayerNormalization</text>\n</g>\n<!-- 139704404283472&#45;&gt;139704784786576 -->\n<g class=\"edge\" id=\"edge57\">\n<title>139704404283472-&gt;139704784786576</title>\n<path d=\"M308,-4307.4551C308,-4299.3828 308,-4289.6764 308,-4280.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"311.5001,-4280.5903 308,-4270.5904 304.5001,-4280.5904 311.5001,-4280.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139704403275664 -->\n<g class=\"node\" id=\"node49\">\n<title>139704403275664</title>\n<polygon fill=\"none\" points=\"30.5,-4161.5 30.5,-4197.5 381.5,-4197.5 381.5,-4161.5 30.5,-4161.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"140\" y=\"-4175.8\">Encoder-6-MultiHeadSelfAttention</text>\n<polyline fill=\"none\" points=\"249.5,-4161.5 249.5,-4197.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"315.5\" y=\"-4175.8\">MultiHeadAttention</text>\n</g>\n<!-- 139704784786576&#45;&gt;139704403275664 -->\n<g class=\"edge\" id=\"edge58\">\n<title>139704784786576-&gt;139704403275664</title>\n<path d=\"M282.7865,-4234.4551C269.7912,-4225.1545 253.7645,-4213.6844 239.7204,-4203.6332\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"241.4459,-4200.5641 231.2769,-4197.5904 237.3719,-4206.2565 241.4459,-4200.5641\" stroke=\"#000000\"/>\n</g>\n<!-- 139704405181968 -->\n<g class=\"node\" id=\"node51\">\n<title>139704405181968</title>\n<polygon fill=\"none\" points=\"162.5,-4015.5 162.5,-4051.5 453.5,-4051.5 453.5,-4015.5 162.5,-4015.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"287\" y=\"-4029.8\">Encoder-6-MultiHeadSelfAttention-Add</text>\n<polyline fill=\"none\" points=\"411.5,-4015.5 411.5,-4051.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"432.5\" y=\"-4029.8\">Add</text>\n</g>\n<!-- 139704784786576&#45;&gt;139704405181968 -->\n<g class=\"edge\" id=\"edge60\">\n<title>139704784786576-&gt;139704405181968</title>\n<path d=\"M352.0299,-4234.3618C367.1906,-4225.7299 382.4854,-4213.7771 391,-4198 398.7883,-4183.5687 398.7039,-4107.2752 388,-4088 381.0407,-4075.468 369.8377,-4065.2368 357.9915,-4057.1643\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"359.6251,-4054.0561 349.3034,-4051.6658 355.8816,-4059.9711 359.6251,-4054.0561\" stroke=\"#000000\"/>\n</g>\n<!-- 139704783834512 -->\n<g class=\"node\" id=\"node50\">\n<title>139704783834512</title>\n<polygon fill=\"none\" points=\"41.5,-4088.5 41.5,-4124.5 378.5,-4124.5 378.5,-4088.5 41.5,-4088.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"177.5\" y=\"-4102.8\">Encoder-6-MultiHeadSelfAttention-Dropout</text>\n<polyline fill=\"none\" points=\"313.5,-4088.5 313.5,-4124.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"346\" y=\"-4102.8\">Dropout</text>\n</g>\n<!-- 139704403275664&#45;&gt;139704783834512 -->\n<g class=\"edge\" id=\"edge59\">\n<title>139704403275664-&gt;139704783834512</title>\n<path d=\"M206.9888,-4161.4551C207.4311,-4153.3828 207.9629,-4143.6764 208.4558,-4134.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.9563,-4134.7669 209.0087,-4124.5904 204.9668,-4134.3839 211.9563,-4134.7669\" stroke=\"#000000\"/>\n</g>\n<!-- 139704783834512&#45;&gt;139704405181968 -->\n<g class=\"edge\" id=\"edge61\">\n<title>139704783834512-&gt;139704405181968</title>\n<path d=\"M234.2247,-4088.4551C246.7105,-4079.1545 262.1086,-4067.6844 275.602,-4057.6332\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"277.7855,-4060.371 283.7143,-4051.5904 273.6039,-4054.7573 277.7855,-4060.371\" stroke=\"#000000\"/>\n</g>\n<!-- 139704403338512 -->\n<g class=\"node\" id=\"node52\">\n<title>139704403338512</title>\n<polygon fill=\"none\" points=\"114,-3942.5 114,-3978.5 502,-3978.5 502,-3942.5 114,-3942.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"242.5\" y=\"-3956.8\">Encoder-6-MultiHeadSelfAttention-Norm</text>\n<polyline fill=\"none\" points=\"371,-3942.5 371,-3978.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"436.5\" y=\"-3956.8\">LayerNormalization</text>\n</g>\n<!-- 139704405181968&#45;&gt;139704403338512 -->\n<g class=\"edge\" id=\"edge62\">\n<title>139704405181968-&gt;139704403338512</title>\n<path d=\"M308,-4015.4551C308,-4007.3828 308,-3997.6764 308,-3988.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"311.5001,-3988.5903 308,-3978.5904 304.5001,-3988.5904 311.5001,-3988.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139706696711952 -->\n<g class=\"node\" id=\"node53\">\n<title>139706696711952</title>\n<polygon fill=\"none\" points=\"99.5,-3869.5 99.5,-3905.5 350.5,-3905.5 350.5,-3869.5 99.5,-3869.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"178.5\" y=\"-3883.8\">Encoder-6-FeedForward</text>\n<polyline fill=\"none\" points=\"257.5,-3869.5 257.5,-3905.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"304\" y=\"-3883.8\">FeedForward</text>\n</g>\n<!-- 139704403338512&#45;&gt;139706696711952 -->\n<g class=\"edge\" id=\"edge63\">\n<title>139704403338512-&gt;139706696711952</title>\n<path d=\"M287.4831,-3942.4551C277.108,-3933.3299 264.3584,-3922.1165 253.0877,-3912.2036\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"255.3889,-3909.5665 245.5685,-3905.5904 250.7659,-3914.8228 255.3889,-3909.5665\" stroke=\"#000000\"/>\n</g>\n<!-- 139704785337360 -->\n<g class=\"node\" id=\"node55\">\n<title>139704785337360</title>\n<polygon fill=\"none\" points=\"187,-3723.5 187,-3759.5 417,-3759.5 417,-3723.5 187,-3723.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"281\" y=\"-3737.8\">Encoder-6-FeedForward-Add</text>\n<polyline fill=\"none\" points=\"375,-3723.5 375,-3759.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"396\" y=\"-3737.8\">Add</text>\n</g>\n<!-- 139704403338512&#45;&gt;139704785337360 -->\n<g class=\"edge\" id=\"edge65\">\n<title>139704403338512-&gt;139704785337360</title>\n<path d=\"M330.7353,-3942.457C341.1909,-3932.8203 352.657,-3920.0556 359,-3906 379.1503,-3861.3484 386.5416,-3840.473 366,-3796 360.5436,-3784.1867 351.1242,-3773.9911 341.1643,-3765.7193\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"343.178,-3762.8521 333.1262,-3759.5038 338.896,-3768.3897 343.178,-3762.8521\" stroke=\"#000000\"/>\n</g>\n<!-- 139704784983248 -->\n<g class=\"node\" id=\"node54\">\n<title>139704784983248</title>\n<polygon fill=\"none\" points=\"81,-3796.5 81,-3832.5 357,-3832.5 357,-3796.5 81,-3796.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"186.5\" y=\"-3810.8\">Encoder-6-FeedForward-Dropout</text>\n<polyline fill=\"none\" points=\"292,-3796.5 292,-3832.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"324.5\" y=\"-3810.8\">Dropout</text>\n</g>\n<!-- 139706696711952&#45;&gt;139704784983248 -->\n<g class=\"edge\" id=\"edge64\">\n<title>139706696711952-&gt;139704784983248</title>\n<path d=\"M223.5169,-3869.4551C222.8534,-3861.3828 222.0556,-3851.6764 221.3163,-3842.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"224.7944,-3842.27 220.4869,-3832.5904 217.8179,-3842.8435 224.7944,-3842.27\" stroke=\"#000000\"/>\n</g>\n<!-- 139704784983248&#45;&gt;139704785337360 -->\n<g class=\"edge\" id=\"edge66\">\n<title>139704784983248-&gt;139704785337360</title>\n<path d=\"M239.5169,-3796.4551C249.892,-3787.3299 262.6416,-3776.1165 273.9123,-3766.2036\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"276.2341,-3768.8228 281.4315,-3759.5904 271.6111,-3763.5665 276.2341,-3768.8228\" stroke=\"#000000\"/>\n</g>\n<!-- 139704405183440 -->\n<g class=\"node\" id=\"node56\">\n<title>139704405183440</title>\n<polygon fill=\"none\" points=\"138.5,-3650.5 138.5,-3686.5 465.5,-3686.5 465.5,-3650.5 138.5,-3650.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"236.5\" y=\"-3664.8\">Encoder-6-FeedForward-Norm</text>\n<polyline fill=\"none\" points=\"334.5,-3650.5 334.5,-3686.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"400\" y=\"-3664.8\">LayerNormalization</text>\n</g>\n<!-- 139704785337360&#45;&gt;139704405183440 -->\n<g class=\"edge\" id=\"edge67\">\n<title>139704785337360-&gt;139704405183440</title>\n<path d=\"M302,-3723.4551C302,-3715.3828 302,-3705.6764 302,-3696.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"305.5001,-3696.5903 302,-3686.5904 298.5001,-3696.5904 305.5001,-3696.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139704402569232 -->\n<g class=\"node\" id=\"node57\">\n<title>139704402569232</title>\n<polygon fill=\"none\" points=\"24.5,-3577.5 24.5,-3613.5 375.5,-3613.5 375.5,-3577.5 24.5,-3577.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"134\" y=\"-3591.8\">Encoder-7-MultiHeadSelfAttention</text>\n<polyline fill=\"none\" points=\"243.5,-3577.5 243.5,-3613.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"309.5\" y=\"-3591.8\">MultiHeadAttention</text>\n</g>\n<!-- 139704405183440&#45;&gt;139704402569232 -->\n<g class=\"edge\" id=\"edge68\">\n<title>139704405183440-&gt;139704402569232</title>\n<path d=\"M276.7865,-3650.4551C263.7912,-3641.1545 247.7645,-3629.6844 233.7204,-3619.6332\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"235.4459,-3616.5641 225.2769,-3613.5904 231.3719,-3622.2565 235.4459,-3616.5641\" stroke=\"#000000\"/>\n</g>\n<!-- 139704402569168 -->\n<g class=\"node\" id=\"node59\">\n<title>139704402569168</title>\n<polygon fill=\"none\" points=\"156.5,-3431.5 156.5,-3467.5 447.5,-3467.5 447.5,-3431.5 156.5,-3431.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"281\" y=\"-3445.8\">Encoder-7-MultiHeadSelfAttention-Add</text>\n<polyline fill=\"none\" points=\"405.5,-3431.5 405.5,-3467.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"426.5\" y=\"-3445.8\">Add</text>\n</g>\n<!-- 139704405183440&#45;&gt;139704402569168 -->\n<g class=\"edge\" id=\"edge70\">\n<title>139704405183440-&gt;139704402569168</title>\n<path d=\"M346.0299,-3650.3618C361.1906,-3641.7299 376.4854,-3629.7771 385,-3614 392.7883,-3599.5687 392.7039,-3523.2752 382,-3504 375.0407,-3491.468 363.8377,-3481.2368 351.9915,-3473.1643\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"353.6251,-3470.0561 343.3034,-3467.6658 349.8816,-3475.9711 353.6251,-3470.0561\" stroke=\"#000000\"/>\n</g>\n<!-- 139704405446032 -->\n<g class=\"node\" id=\"node58\">\n<title>139704405446032</title>\n<polygon fill=\"none\" points=\"35.5,-3504.5 35.5,-3540.5 372.5,-3540.5 372.5,-3504.5 35.5,-3504.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"171.5\" y=\"-3518.8\">Encoder-7-MultiHeadSelfAttention-Dropout</text>\n<polyline fill=\"none\" points=\"307.5,-3504.5 307.5,-3540.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"340\" y=\"-3518.8\">Dropout</text>\n</g>\n<!-- 139704402569232&#45;&gt;139704405446032 -->\n<g class=\"edge\" id=\"edge69\">\n<title>139704402569232-&gt;139704405446032</title>\n<path d=\"M200.9888,-3577.4551C201.4311,-3569.3828 201.9629,-3559.6764 202.4558,-3550.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"205.9563,-3550.7669 203.0087,-3540.5904 198.9668,-3550.3839 205.9563,-3550.7669\" stroke=\"#000000\"/>\n</g>\n<!-- 139704405446032&#45;&gt;139704402569168 -->\n<g class=\"edge\" id=\"edge71\">\n<title>139704405446032-&gt;139704402569168</title>\n<path d=\"M228.2247,-3504.4551C240.7105,-3495.1545 256.1086,-3483.6844 269.602,-3473.6332\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"271.7855,-3476.371 277.7143,-3467.5904 267.6039,-3470.7573 271.7855,-3476.371\" stroke=\"#000000\"/>\n</g>\n<!-- 139706696765968 -->\n<g class=\"node\" id=\"node60\">\n<title>139706696765968</title>\n<polygon fill=\"none\" points=\"108,-3358.5 108,-3394.5 496,-3394.5 496,-3358.5 108,-3358.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"236.5\" y=\"-3372.8\">Encoder-7-MultiHeadSelfAttention-Norm</text>\n<polyline fill=\"none\" points=\"365,-3358.5 365,-3394.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"430.5\" y=\"-3372.8\">LayerNormalization</text>\n</g>\n<!-- 139704402569168&#45;&gt;139706696765968 -->\n<g class=\"edge\" id=\"edge72\">\n<title>139704402569168-&gt;139706696765968</title>\n<path d=\"M302,-3431.4551C302,-3423.3828 302,-3413.6764 302,-3404.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"305.5001,-3404.5903 302,-3394.5904 298.5001,-3404.5904 305.5001,-3404.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139704402570192 -->\n<g class=\"node\" id=\"node61\">\n<title>139704402570192</title>\n<polygon fill=\"none\" points=\"93.5,-3285.5 93.5,-3321.5 344.5,-3321.5 344.5,-3285.5 93.5,-3285.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"172.5\" y=\"-3299.8\">Encoder-7-FeedForward</text>\n<polyline fill=\"none\" points=\"251.5,-3285.5 251.5,-3321.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"298\" y=\"-3299.8\">FeedForward</text>\n</g>\n<!-- 139706696765968&#45;&gt;139704402570192 -->\n<g class=\"edge\" id=\"edge73\">\n<title>139706696765968-&gt;139704402570192</title>\n<path d=\"M281.4831,-3358.4551C271.108,-3349.3299 258.3584,-3338.1165 247.0877,-3328.2036\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"249.3889,-3325.5665 239.5685,-3321.5904 244.7659,-3330.8228 249.3889,-3325.5665\" stroke=\"#000000\"/>\n</g>\n<!-- 139704784629968 -->\n<g class=\"node\" id=\"node63\">\n<title>139704784629968</title>\n<polygon fill=\"none\" points=\"181,-3139.5 181,-3175.5 411,-3175.5 411,-3139.5 181,-3139.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"275\" y=\"-3153.8\">Encoder-7-FeedForward-Add</text>\n<polyline fill=\"none\" points=\"369,-3139.5 369,-3175.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"390\" y=\"-3153.8\">Add</text>\n</g>\n<!-- 139706696765968&#45;&gt;139704784629968 -->\n<g class=\"edge\" id=\"edge75\">\n<title>139706696765968-&gt;139704784629968</title>\n<path d=\"M324.7353,-3358.457C335.1909,-3348.8203 346.657,-3336.0556 353,-3322 373.1503,-3277.3484 380.5416,-3256.473 360,-3212 354.5436,-3200.1867 345.1242,-3189.9911 335.1643,-3181.7193\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"337.178,-3178.8521 327.1262,-3175.5038 332.896,-3184.3897 337.178,-3178.8521\" stroke=\"#000000\"/>\n</g>\n<!-- 139704784982224 -->\n<g class=\"node\" id=\"node62\">\n<title>139704784982224</title>\n<polygon fill=\"none\" points=\"75,-3212.5 75,-3248.5 351,-3248.5 351,-3212.5 75,-3212.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"180.5\" y=\"-3226.8\">Encoder-7-FeedForward-Dropout</text>\n<polyline fill=\"none\" points=\"286,-3212.5 286,-3248.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"318.5\" y=\"-3226.8\">Dropout</text>\n</g>\n<!-- 139704402570192&#45;&gt;139704784982224 -->\n<g class=\"edge\" id=\"edge74\">\n<title>139704402570192-&gt;139704784982224</title>\n<path d=\"M217.5169,-3285.4551C216.8534,-3277.3828 216.0556,-3267.6764 215.3163,-3258.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"218.7944,-3258.27 214.4869,-3248.5904 211.8179,-3258.8435 218.7944,-3258.27\" stroke=\"#000000\"/>\n</g>\n<!-- 139704784982224&#45;&gt;139704784629968 -->\n<g class=\"edge\" id=\"edge76\">\n<title>139704784982224-&gt;139704784629968</title>\n<path d=\"M233.5169,-3212.4551C243.892,-3203.3299 256.6416,-3192.1165 267.9123,-3182.2036\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"270.2341,-3184.8228 275.4315,-3175.5904 265.6111,-3179.5665 270.2341,-3184.8228\" stroke=\"#000000\"/>\n</g>\n<!-- 139704401952592 -->\n<g class=\"node\" id=\"node64\">\n<title>139704401952592</title>\n<polygon fill=\"none\" points=\"132.5,-3066.5 132.5,-3102.5 459.5,-3102.5 459.5,-3066.5 132.5,-3066.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"230.5\" y=\"-3080.8\">Encoder-7-FeedForward-Norm</text>\n<polyline fill=\"none\" points=\"328.5,-3066.5 328.5,-3102.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"394\" y=\"-3080.8\">LayerNormalization</text>\n</g>\n<!-- 139704784629968&#45;&gt;139704401952592 -->\n<g class=\"edge\" id=\"edge77\">\n<title>139704784629968-&gt;139704401952592</title>\n<path d=\"M296,-3139.4551C296,-3131.3828 296,-3121.6764 296,-3112.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"299.5001,-3112.5903 296,-3102.5904 292.5001,-3112.5904 299.5001,-3112.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139704402027472 -->\n<g class=\"node\" id=\"node65\">\n<title>139704402027472</title>\n<polygon fill=\"none\" points=\"18.5,-2993.5 18.5,-3029.5 369.5,-3029.5 369.5,-2993.5 18.5,-2993.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"128\" y=\"-3007.8\">Encoder-8-MultiHeadSelfAttention</text>\n<polyline fill=\"none\" points=\"237.5,-2993.5 237.5,-3029.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"303.5\" y=\"-3007.8\">MultiHeadAttention</text>\n</g>\n<!-- 139704401952592&#45;&gt;139704402027472 -->\n<g class=\"edge\" id=\"edge78\">\n<title>139704401952592-&gt;139704402027472</title>\n<path d=\"M270.7865,-3066.4551C257.7912,-3057.1545 241.7645,-3045.6844 227.7204,-3035.6332\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"229.4459,-3032.5641 219.2769,-3029.5904 225.3719,-3038.2565 229.4459,-3032.5641\" stroke=\"#000000\"/>\n</g>\n<!-- 139704785390672 -->\n<g class=\"node\" id=\"node67\">\n<title>139704785390672</title>\n<polygon fill=\"none\" points=\"151.5,-2847.5 151.5,-2883.5 442.5,-2883.5 442.5,-2847.5 151.5,-2847.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"276\" y=\"-2861.8\">Encoder-8-MultiHeadSelfAttention-Add</text>\n<polyline fill=\"none\" points=\"400.5,-2847.5 400.5,-2883.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"421.5\" y=\"-2861.8\">Add</text>\n</g>\n<!-- 139704401952592&#45;&gt;139704785390672 -->\n<g class=\"edge\" id=\"edge80\">\n<title>139704401952592-&gt;139704785390672</title>\n<path d=\"M340.0299,-3066.3618C355.1906,-3057.7299 370.4854,-3045.7771 379,-3030 387.1991,-3014.8075 384.9679,-2937.9111 375,-2920 368.0832,-2907.5713 356.9956,-2897.3379 345.3254,-2889.229\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"347.0734,-2886.1914 336.7759,-2883.699 343.2716,-2892.069 347.0734,-2886.1914\" stroke=\"#000000\"/>\n</g>\n<!-- 139704784992208 -->\n<g class=\"node\" id=\"node66\">\n<title>139704784992208</title>\n<polygon fill=\"none\" points=\"28.5,-2920.5 28.5,-2956.5 365.5,-2956.5 365.5,-2920.5 28.5,-2920.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"164.5\" y=\"-2934.8\">Encoder-8-MultiHeadSelfAttention-Dropout</text>\n<polyline fill=\"none\" points=\"300.5,-2920.5 300.5,-2956.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"333\" y=\"-2934.8\">Dropout</text>\n</g>\n<!-- 139704402027472&#45;&gt;139704784992208 -->\n<g class=\"edge\" id=\"edge79\">\n<title>139704402027472-&gt;139704784992208</title>\n<path d=\"M194.7416,-2993.4551C195.0733,-2985.3828 195.4722,-2975.6764 195.8418,-2966.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"199.3429,-2966.7257 196.2566,-2956.5904 192.3488,-2966.4382 199.3429,-2966.7257\" stroke=\"#000000\"/>\n</g>\n<!-- 139704784992208&#45;&gt;139704785390672 -->\n<g class=\"edge\" id=\"edge81\">\n<title>139704784992208-&gt;139704785390672</title>\n<path d=\"M221.7191,-2920.4551C234.4597,-2911.1545 250.1721,-2899.6844 263.9408,-2889.6332\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"266.2055,-2892.3134 272.2187,-2883.5904 262.0781,-2886.6596 266.2055,-2892.3134\" stroke=\"#000000\"/>\n</g>\n<!-- 139704784983504 -->\n<g class=\"node\" id=\"node68\">\n<title>139704784983504</title>\n<polygon fill=\"none\" points=\"103,-2774.5 103,-2810.5 491,-2810.5 491,-2774.5 103,-2774.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"231.5\" y=\"-2788.8\">Encoder-8-MultiHeadSelfAttention-Norm</text>\n<polyline fill=\"none\" points=\"360,-2774.5 360,-2810.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"425.5\" y=\"-2788.8\">LayerNormalization</text>\n</g>\n<!-- 139704785390672&#45;&gt;139704784983504 -->\n<g class=\"edge\" id=\"edge82\">\n<title>139704785390672-&gt;139704784983504</title>\n<path d=\"M297,-2847.4551C297,-2839.3828 297,-2829.6764 297,-2820.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"300.5001,-2820.5903 297,-2810.5904 293.5001,-2820.5904 300.5001,-2820.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139706858114320 -->\n<g class=\"node\" id=\"node69\">\n<title>139706858114320</title>\n<polygon fill=\"none\" points=\"88.5,-2701.5 88.5,-2737.5 339.5,-2737.5 339.5,-2701.5 88.5,-2701.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"167.5\" y=\"-2715.8\">Encoder-8-FeedForward</text>\n<polyline fill=\"none\" points=\"246.5,-2701.5 246.5,-2737.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"293\" y=\"-2715.8\">FeedForward</text>\n</g>\n<!-- 139704784983504&#45;&gt;139706858114320 -->\n<g class=\"edge\" id=\"edge83\">\n<title>139704784983504-&gt;139706858114320</title>\n<path d=\"M276.4831,-2774.4551C266.108,-2765.3299 253.3584,-2754.1165 242.0877,-2744.2036\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"244.3889,-2741.5665 234.5685,-2737.5904 239.7659,-2746.8228 244.3889,-2741.5665\" stroke=\"#000000\"/>\n</g>\n<!-- 139704401143568 -->\n<g class=\"node\" id=\"node71\">\n<title>139704401143568</title>\n<polygon fill=\"none\" points=\"176,-2555.5 176,-2591.5 406,-2591.5 406,-2555.5 176,-2555.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"270\" y=\"-2569.8\">Encoder-8-FeedForward-Add</text>\n<polyline fill=\"none\" points=\"364,-2555.5 364,-2591.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"385\" y=\"-2569.8\">Add</text>\n</g>\n<!-- 139704784983504&#45;&gt;139704401143568 -->\n<g class=\"edge\" id=\"edge85\">\n<title>139704784983504-&gt;139704401143568</title>\n<path d=\"M319.7353,-2774.457C330.1909,-2764.8203 341.657,-2752.0556 348,-2738 368.1503,-2693.3484 375.5416,-2672.473 355,-2628 349.5436,-2616.1867 340.1242,-2605.9911 330.1643,-2597.7193\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"332.178,-2594.8521 322.1262,-2591.5038 327.896,-2600.3897 332.178,-2594.8521\" stroke=\"#000000\"/>\n</g>\n<!-- 139704401587088 -->\n<g class=\"node\" id=\"node70\">\n<title>139704401587088</title>\n<polygon fill=\"none\" points=\"70,-2628.5 70,-2664.5 346,-2664.5 346,-2628.5 70,-2628.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"175.5\" y=\"-2642.8\">Encoder-8-FeedForward-Dropout</text>\n<polyline fill=\"none\" points=\"281,-2628.5 281,-2664.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"313.5\" y=\"-2642.8\">Dropout</text>\n</g>\n<!-- 139706858114320&#45;&gt;139704401587088 -->\n<g class=\"edge\" id=\"edge84\">\n<title>139706858114320-&gt;139704401587088</title>\n<path d=\"M212.5169,-2701.4551C211.8534,-2693.3828 211.0556,-2683.6764 210.3163,-2674.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"213.7944,-2674.27 209.4869,-2664.5904 206.8179,-2674.8435 213.7944,-2674.27\" stroke=\"#000000\"/>\n</g>\n<!-- 139704401587088&#45;&gt;139704401143568 -->\n<g class=\"edge\" id=\"edge86\">\n<title>139704401587088-&gt;139704401143568</title>\n<path d=\"M228.5169,-2628.4551C238.892,-2619.3299 251.6416,-2608.1165 262.9123,-2598.2036\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"265.2341,-2600.8228 270.4315,-2591.5904 260.6111,-2595.5665 265.2341,-2600.8228\" stroke=\"#000000\"/>\n</g>\n<!-- 139704401122512 -->\n<g class=\"node\" id=\"node72\">\n<title>139704401122512</title>\n<polygon fill=\"none\" points=\"127.5,-2482.5 127.5,-2518.5 454.5,-2518.5 454.5,-2482.5 127.5,-2482.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"225.5\" y=\"-2496.8\">Encoder-8-FeedForward-Norm</text>\n<polyline fill=\"none\" points=\"323.5,-2482.5 323.5,-2518.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"389\" y=\"-2496.8\">LayerNormalization</text>\n</g>\n<!-- 139704401143568&#45;&gt;139704401122512 -->\n<g class=\"edge\" id=\"edge87\">\n<title>139704401143568-&gt;139704401122512</title>\n<path d=\"M291,-2555.4551C291,-2547.3828 291,-2537.6764 291,-2528.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"294.5001,-2528.5903 291,-2518.5904 287.5001,-2528.5904 294.5001,-2528.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139704402421200 -->\n<g class=\"node\" id=\"node73\">\n<title>139704402421200</title>\n<polygon fill=\"none\" points=\"13.5,-2409.5 13.5,-2445.5 364.5,-2445.5 364.5,-2409.5 13.5,-2409.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"123\" y=\"-2423.8\">Encoder-9-MultiHeadSelfAttention</text>\n<polyline fill=\"none\" points=\"232.5,-2409.5 232.5,-2445.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"298.5\" y=\"-2423.8\">MultiHeadAttention</text>\n</g>\n<!-- 139704401122512&#45;&gt;139704402421200 -->\n<g class=\"edge\" id=\"edge88\">\n<title>139704401122512-&gt;139704402421200</title>\n<path d=\"M265.7865,-2482.4551C252.7912,-2473.1545 236.7645,-2461.6844 222.7204,-2451.6332\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"224.4459,-2448.5641 214.2769,-2445.5904 220.3719,-2454.2565 224.4459,-2448.5641\" stroke=\"#000000\"/>\n</g>\n<!-- 139704402353872 -->\n<g class=\"node\" id=\"node75\">\n<title>139704402353872</title>\n<polygon fill=\"none\" points=\"146.5,-2263.5 146.5,-2299.5 437.5,-2299.5 437.5,-2263.5 146.5,-2263.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"271\" y=\"-2277.8\">Encoder-9-MultiHeadSelfAttention-Add</text>\n<polyline fill=\"none\" points=\"395.5,-2263.5 395.5,-2299.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"416.5\" y=\"-2277.8\">Add</text>\n</g>\n<!-- 139704401122512&#45;&gt;139704402353872 -->\n<g class=\"edge\" id=\"edge90\">\n<title>139704401122512-&gt;139704402353872</title>\n<path d=\"M335.0299,-2482.3618C350.1906,-2473.7299 365.4854,-2461.7771 374,-2446 382.1991,-2430.8075 379.9679,-2353.9111 370,-2336 363.0832,-2323.5713 351.9956,-2313.3379 340.3254,-2305.229\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"342.0734,-2302.1914 331.7759,-2299.699 338.2716,-2308.069 342.0734,-2302.1914\" stroke=\"#000000\"/>\n</g>\n<!-- 139704402354064 -->\n<g class=\"node\" id=\"node74\">\n<title>139704402354064</title>\n<polygon fill=\"none\" points=\"23.5,-2336.5 23.5,-2372.5 360.5,-2372.5 360.5,-2336.5 23.5,-2336.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"159.5\" y=\"-2350.8\">Encoder-9-MultiHeadSelfAttention-Dropout</text>\n<polyline fill=\"none\" points=\"295.5,-2336.5 295.5,-2372.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"328\" y=\"-2350.8\">Dropout</text>\n</g>\n<!-- 139704402421200&#45;&gt;139704402354064 -->\n<g class=\"edge\" id=\"edge89\">\n<title>139704402421200-&gt;139704402354064</title>\n<path d=\"M189.7416,-2409.4551C190.0733,-2401.3828 190.4722,-2391.6764 190.8418,-2382.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"194.3429,-2382.7257 191.2566,-2372.5904 187.3488,-2382.4382 194.3429,-2382.7257\" stroke=\"#000000\"/>\n</g>\n<!-- 139704402354064&#45;&gt;139704402353872 -->\n<g class=\"edge\" id=\"edge91\">\n<title>139704402354064-&gt;139704402353872</title>\n<path d=\"M216.7191,-2336.4551C229.4597,-2327.1545 245.1721,-2315.6844 258.9408,-2305.6332\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"261.2055,-2308.3134 267.2187,-2299.5904 257.0781,-2302.6596 261.2055,-2308.3134\" stroke=\"#000000\"/>\n</g>\n<!-- 139704401204688 -->\n<g class=\"node\" id=\"node76\">\n<title>139704401204688</title>\n<polygon fill=\"none\" points=\"98,-2190.5 98,-2226.5 486,-2226.5 486,-2190.5 98,-2190.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"226.5\" y=\"-2204.8\">Encoder-9-MultiHeadSelfAttention-Norm</text>\n<polyline fill=\"none\" points=\"355,-2190.5 355,-2226.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"420.5\" y=\"-2204.8\">LayerNormalization</text>\n</g>\n<!-- 139704402353872&#45;&gt;139704401204688 -->\n<g class=\"edge\" id=\"edge92\">\n<title>139704402353872-&gt;139704401204688</title>\n<path d=\"M292,-2263.4551C292,-2255.3828 292,-2245.6764 292,-2236.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"295.5001,-2236.5903 292,-2226.5904 288.5001,-2236.5904 295.5001,-2236.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139704402317264 -->\n<g class=\"node\" id=\"node77\">\n<title>139704402317264</title>\n<polygon fill=\"none\" points=\"83.5,-2117.5 83.5,-2153.5 334.5,-2153.5 334.5,-2117.5 83.5,-2117.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"162.5\" y=\"-2131.8\">Encoder-9-FeedForward</text>\n<polyline fill=\"none\" points=\"241.5,-2117.5 241.5,-2153.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"288\" y=\"-2131.8\">FeedForward</text>\n</g>\n<!-- 139704401204688&#45;&gt;139704402317264 -->\n<g class=\"edge\" id=\"edge93\">\n<title>139704401204688-&gt;139704402317264</title>\n<path d=\"M271.4831,-2190.4551C261.108,-2181.3299 248.3584,-2170.1165 237.0877,-2160.2036\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"239.3889,-2157.5665 229.5685,-2153.5904 234.7659,-2162.8228 239.3889,-2157.5665\" stroke=\"#000000\"/>\n</g>\n<!-- 139704400741712 -->\n<g class=\"node\" id=\"node79\">\n<title>139704400741712</title>\n<polygon fill=\"none\" points=\"171,-1971.5 171,-2007.5 401,-2007.5 401,-1971.5 171,-1971.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"265\" y=\"-1985.8\">Encoder-9-FeedForward-Add</text>\n<polyline fill=\"none\" points=\"359,-1971.5 359,-2007.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"380\" y=\"-1985.8\">Add</text>\n</g>\n<!-- 139704401204688&#45;&gt;139704400741712 -->\n<g class=\"edge\" id=\"edge95\">\n<title>139704401204688-&gt;139704400741712</title>\n<path d=\"M314.7353,-2190.457C325.1909,-2180.8203 336.657,-2168.0556 343,-2154 363.1503,-2109.3484 370.5416,-2088.473 350,-2044 344.5436,-2032.1867 335.1242,-2021.9911 325.1643,-2013.7193\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"327.178,-2010.8521 317.1262,-2007.5038 322.896,-2016.3897 327.178,-2010.8521\" stroke=\"#000000\"/>\n</g>\n<!-- 139704400661072 -->\n<g class=\"node\" id=\"node78\">\n<title>139704400661072</title>\n<polygon fill=\"none\" points=\"65,-2044.5 65,-2080.5 341,-2080.5 341,-2044.5 65,-2044.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"170.5\" y=\"-2058.8\">Encoder-9-FeedForward-Dropout</text>\n<polyline fill=\"none\" points=\"276,-2044.5 276,-2080.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"308.5\" y=\"-2058.8\">Dropout</text>\n</g>\n<!-- 139704402317264&#45;&gt;139704400661072 -->\n<g class=\"edge\" id=\"edge94\">\n<title>139704402317264-&gt;139704400661072</title>\n<path d=\"M207.5169,-2117.4551C206.8534,-2109.3828 206.0556,-2099.6764 205.3163,-2090.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"208.7944,-2090.27 204.4869,-2080.5904 201.8179,-2090.8435 208.7944,-2090.27\" stroke=\"#000000\"/>\n</g>\n<!-- 139704400661072&#45;&gt;139704400741712 -->\n<g class=\"edge\" id=\"edge96\">\n<title>139704400661072-&gt;139704400741712</title>\n<path d=\"M223.5169,-2044.4551C233.892,-2035.3299 246.6416,-2024.1165 257.9123,-2014.2036\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"260.2341,-2016.8228 265.4315,-2007.5904 255.6111,-2011.5665 260.2341,-2016.8228\" stroke=\"#000000\"/>\n</g>\n<!-- 139704400346768 -->\n<g class=\"node\" id=\"node80\">\n<title>139704400346768</title>\n<polygon fill=\"none\" points=\"122.5,-1898.5 122.5,-1934.5 449.5,-1934.5 449.5,-1898.5 122.5,-1898.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"220.5\" y=\"-1912.8\">Encoder-9-FeedForward-Norm</text>\n<polyline fill=\"none\" points=\"318.5,-1898.5 318.5,-1934.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"384\" y=\"-1912.8\">LayerNormalization</text>\n</g>\n<!-- 139704400741712&#45;&gt;139704400346768 -->\n<g class=\"edge\" id=\"edge97\">\n<title>139704400741712-&gt;139704400346768</title>\n<path d=\"M286,-1971.4551C286,-1963.3828 286,-1953.6764 286,-1944.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"289.5001,-1944.5903 286,-1934.5904 282.5001,-1944.5904 289.5001,-1944.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139704400366160 -->\n<g class=\"node\" id=\"node81\">\n<title>139704400366160</title>\n<polygon fill=\"none\" points=\"2.5,-1825.5 2.5,-1861.5 361.5,-1861.5 361.5,-1825.5 2.5,-1825.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"116\" y=\"-1839.8\">Encoder-10-MultiHeadSelfAttention</text>\n<polyline fill=\"none\" points=\"229.5,-1825.5 229.5,-1861.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"295.5\" y=\"-1839.8\">MultiHeadAttention</text>\n</g>\n<!-- 139704400346768&#45;&gt;139704400366160 -->\n<g class=\"edge\" id=\"edge98\">\n<title>139704400346768-&gt;139704400366160</title>\n<path d=\"M260.2921,-1898.4551C246.917,-1889.0667 230.3925,-1877.4678 215.9768,-1867.3491\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"217.9683,-1864.4709 207.7726,-1861.5904 213.9467,-1870.2003 217.9683,-1864.4709\" stroke=\"#000000\"/>\n</g>\n<!-- 139704400446480 -->\n<g class=\"node\" id=\"node83\">\n<title>139704400446480</title>\n<polygon fill=\"none\" points=\"138.5,-1679.5 138.5,-1715.5 437.5,-1715.5 437.5,-1679.5 138.5,-1679.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"267\" y=\"-1693.8\">Encoder-10-MultiHeadSelfAttention-Add</text>\n<polyline fill=\"none\" points=\"395.5,-1679.5 395.5,-1715.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"416.5\" y=\"-1693.8\">Add</text>\n</g>\n<!-- 139704400346768&#45;&gt;139704400446480 -->\n<g class=\"edge\" id=\"edge100\">\n<title>139704400346768-&gt;139704400446480</title>\n<path d=\"M331.5114,-1898.3446C346.887,-1889.7593 362.3385,-1877.8425 371,-1862 379.2708,-1846.8721 377.1095,-1770.0386 367,-1752 359.9861,-1739.4849 348.7537,-1729.2232 336.9338,-1721.1149\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"338.588,-1718.0185 328.2752,-1715.5899 334.8226,-1723.9196 338.588,-1718.0185\" stroke=\"#000000\"/>\n</g>\n<!-- 139704400417424 -->\n<g class=\"node\" id=\"node82\">\n<title>139704400417424</title>\n<polygon fill=\"none\" points=\"13.5,-1752.5 13.5,-1788.5 358.5,-1788.5 358.5,-1752.5 13.5,-1752.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"153.5\" y=\"-1766.8\">Encoder-10-MultiHeadSelfAttention-Dropout</text>\n<polyline fill=\"none\" points=\"293.5,-1752.5 293.5,-1788.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"326\" y=\"-1766.8\">Dropout</text>\n</g>\n<!-- 139704400366160&#45;&gt;139704400417424 -->\n<g class=\"edge\" id=\"edge99\">\n<title>139704400366160-&gt;139704400417424</title>\n<path d=\"M182.9888,-1825.4551C183.4311,-1817.3828 183.9629,-1807.6764 184.4558,-1798.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"187.9563,-1798.7669 185.0087,-1788.5904 180.9668,-1798.3839 187.9563,-1798.7669\" stroke=\"#000000\"/>\n</g>\n<!-- 139704400417424&#45;&gt;139704400446480 -->\n<g class=\"edge\" id=\"edge101\">\n<title>139704400417424-&gt;139704400446480</title>\n<path d=\"M211.2135,-1752.4551C224.2088,-1743.1545 240.2355,-1731.6844 254.2796,-1721.6332\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"256.6281,-1724.2565 262.7231,-1715.5904 252.5541,-1718.5641 256.6281,-1724.2565\" stroke=\"#000000\"/>\n</g>\n<!-- 139704400460240 -->\n<g class=\"node\" id=\"node84\">\n<title>139704400460240</title>\n<polygon fill=\"none\" points=\"90,-1606.5 90,-1642.5 486,-1642.5 486,-1606.5 90,-1606.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"222.5\" y=\"-1620.8\">Encoder-10-MultiHeadSelfAttention-Norm</text>\n<polyline fill=\"none\" points=\"355,-1606.5 355,-1642.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"420.5\" y=\"-1620.8\">LayerNormalization</text>\n</g>\n<!-- 139704400446480&#45;&gt;139704400460240 -->\n<g class=\"edge\" id=\"edge102\">\n<title>139704400446480-&gt;139704400460240</title>\n<path d=\"M288,-1679.4551C288,-1671.3828 288,-1661.6764 288,-1652.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"291.5001,-1652.5903 288,-1642.5904 284.5001,-1652.5904 291.5001,-1652.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139704401206032 -->\n<g class=\"node\" id=\"node85\">\n<title>139704401206032</title>\n<polygon fill=\"none\" points=\"74,-1533.5 74,-1569.5 332,-1569.5 332,-1533.5 74,-1533.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"156.5\" y=\"-1547.8\">Encoder-10-FeedForward</text>\n<polyline fill=\"none\" points=\"239,-1533.5 239,-1569.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"285.5\" y=\"-1547.8\">FeedForward</text>\n</g>\n<!-- 139704400460240&#45;&gt;139704401206032 -->\n<g class=\"edge\" id=\"edge103\">\n<title>139704400460240-&gt;139704401206032</title>\n<path d=\"M266.9888,-1606.4551C256.3636,-1597.3299 243.3068,-1586.1165 231.7645,-1576.2036\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"233.9308,-1573.4505 224.0641,-1569.5904 229.3701,-1578.7609 233.9308,-1573.4505\" stroke=\"#000000\"/>\n</g>\n<!-- 139704400723024 -->\n<g class=\"node\" id=\"node87\">\n<title>139704400723024</title>\n<polygon fill=\"none\" points=\"163.5,-1387.5 163.5,-1423.5 400.5,-1423.5 400.5,-1387.5 163.5,-1387.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"261\" y=\"-1401.8\">Encoder-10-FeedForward-Add</text>\n<polyline fill=\"none\" points=\"358.5,-1387.5 358.5,-1423.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"379.5\" y=\"-1401.8\">Add</text>\n</g>\n<!-- 139704400460240&#45;&gt;139704400723024 -->\n<g class=\"edge\" id=\"edge105\">\n<title>139704400460240-&gt;139704400723024</title>\n<path d=\"M311.896,-1606.3839C322.6863,-1596.8165 334.4579,-1584.1331 341,-1570 361.5783,-1525.544 368.8779,-1504.3161 348,-1460 342.4031,-1448.1199 332.805,-1437.9472 322.6353,-1429.7189\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"324.5179,-1426.7554 314.4224,-1423.5413 320.3101,-1432.3495 324.5179,-1426.7554\" stroke=\"#000000\"/>\n</g>\n<!-- 139704400788304 -->\n<g class=\"node\" id=\"node86\">\n<title>139704400788304</title>\n<polygon fill=\"none\" points=\"55.5,-1460.5 55.5,-1496.5 338.5,-1496.5 338.5,-1460.5 55.5,-1460.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"164.5\" y=\"-1474.8\">Encoder-10-FeedForward-Dropout</text>\n<polyline fill=\"none\" points=\"273.5,-1460.5 273.5,-1496.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"306\" y=\"-1474.8\">Dropout</text>\n</g>\n<!-- 139704401206032&#45;&gt;139704400788304 -->\n<g class=\"edge\" id=\"edge104\">\n<title>139704401206032-&gt;139704400788304</title>\n<path d=\"M201.5169,-1533.4551C200.8534,-1525.3828 200.0556,-1515.6764 199.3163,-1506.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"202.7944,-1506.27 198.4869,-1496.5904 195.8179,-1506.8435 202.7944,-1506.27\" stroke=\"#000000\"/>\n</g>\n<!-- 139704400788304&#45;&gt;139704400723024 -->\n<g class=\"edge\" id=\"edge106\">\n<title>139704400788304-&gt;139704400723024</title>\n<path d=\"M218.0112,-1460.4551C228.6364,-1451.3299 241.6932,-1440.1165 253.2355,-1430.2036\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"255.6299,-1432.7609 260.9359,-1423.5904 251.0692,-1427.4505 255.6299,-1432.7609\" stroke=\"#000000\"/>\n</g>\n<!-- 139704399927376 -->\n<g class=\"node\" id=\"node88\">\n<title>139704399927376</title>\n<polygon fill=\"none\" points=\"115,-1314.5 115,-1350.5 449,-1350.5 449,-1314.5 115,-1314.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"216.5\" y=\"-1328.8\">Encoder-10-FeedForward-Norm</text>\n<polyline fill=\"none\" points=\"318,-1314.5 318,-1350.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"383.5\" y=\"-1328.8\">LayerNormalization</text>\n</g>\n<!-- 139704400723024&#45;&gt;139704399927376 -->\n<g class=\"edge\" id=\"edge107\">\n<title>139704400723024-&gt;139704399927376</title>\n<path d=\"M282,-1387.4551C282,-1379.3828 282,-1369.6764 282,-1360.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"285.5001,-1360.5903 282,-1350.5904 278.5001,-1360.5904 285.5001,-1360.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139704399994448 -->\n<g class=\"node\" id=\"node89\">\n<title>139704399994448</title>\n<polygon fill=\"none\" points=\"0,-1241.5 0,-1277.5 358,-1277.5 358,-1241.5 0,-1241.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"113\" y=\"-1255.8\">Encoder-11-MultiHeadSelfAttention</text>\n<polyline fill=\"none\" points=\"226,-1241.5 226,-1277.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"292\" y=\"-1255.8\">MultiHeadAttention</text>\n</g>\n<!-- 139704399927376&#45;&gt;139704399994448 -->\n<g class=\"edge\" id=\"edge108\">\n<title>139704399927376-&gt;139704399994448</title>\n<path d=\"M256.5393,-1314.4551C243.4166,-1305.1545 227.2328,-1293.6844 213.051,-1283.6332\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"214.7073,-1280.5172 204.5248,-1277.5904 210.6596,-1286.2283 214.7073,-1280.5172\" stroke=\"#000000\"/>\n</g>\n<!-- 139704399994576 -->\n<g class=\"node\" id=\"node91\">\n<title>139704399994576</title>\n<polygon fill=\"none\" points=\"135,-1095.5 135,-1131.5 433,-1131.5 433,-1095.5 135,-1095.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263\" y=\"-1109.8\">Encoder-11-MultiHeadSelfAttention-Add</text>\n<polyline fill=\"none\" points=\"391,-1095.5 391,-1131.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"412\" y=\"-1109.8\">Add</text>\n</g>\n<!-- 139704399927376&#45;&gt;139704399994576 -->\n<g class=\"edge\" id=\"edge110\">\n<title>139704399927376-&gt;139704399994576</title>\n<path d=\"M327.5114,-1314.3446C342.887,-1305.7593 358.3385,-1293.8425 367,-1278 375.2708,-1262.8721 373.1095,-1186.0386 363,-1168 355.9861,-1155.4849 344.7537,-1145.2232 332.9338,-1137.1149\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"334.588,-1134.0185 324.2752,-1131.5899 330.8226,-1139.9196 334.588,-1134.0185\" stroke=\"#000000\"/>\n</g>\n<!-- 139704399959952 -->\n<g class=\"node\" id=\"node90\">\n<title>139704399959952</title>\n<polygon fill=\"none\" points=\"10,-1168.5 10,-1204.5 354,-1204.5 354,-1168.5 10,-1168.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"149.5\" y=\"-1182.8\">Encoder-11-MultiHeadSelfAttention-Dropout</text>\n<polyline fill=\"none\" points=\"289,-1168.5 289,-1204.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"321.5\" y=\"-1182.8\">Dropout</text>\n</g>\n<!-- 139704399994448&#45;&gt;139704399959952 -->\n<g class=\"edge\" id=\"edge109\">\n<title>139704399994448-&gt;139704399959952</title>\n<path d=\"M179.7416,-1241.4551C180.0733,-1233.3828 180.4722,-1223.6764 180.8418,-1214.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"184.3429,-1214.7257 181.2566,-1204.5904 177.3488,-1214.4382 184.3429,-1214.7257\" stroke=\"#000000\"/>\n</g>\n<!-- 139704399959952&#45;&gt;139704399994576 -->\n<g class=\"edge\" id=\"edge111\">\n<title>139704399959952-&gt;139704399994576</title>\n<path d=\"M207.2135,-1168.4551C220.2088,-1159.1545 236.2355,-1147.6844 250.2796,-1137.6332\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"252.6281,-1140.2565 258.7231,-1131.5904 248.5541,-1134.5641 252.6281,-1140.2565\" stroke=\"#000000\"/>\n</g>\n<!-- 139704400004048 -->\n<g class=\"node\" id=\"node92\">\n<title>139704400004048</title>\n<polygon fill=\"none\" points=\"86.5,-1022.5 86.5,-1058.5 481.5,-1058.5 481.5,-1022.5 86.5,-1022.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218.5\" y=\"-1036.8\">Encoder-11-MultiHeadSelfAttention-Norm</text>\n<polyline fill=\"none\" points=\"350.5,-1022.5 350.5,-1058.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"416\" y=\"-1036.8\">LayerNormalization</text>\n</g>\n<!-- 139704399994576&#45;&gt;139704400004048 -->\n<g class=\"edge\" id=\"edge112\">\n<title>139704399994576-&gt;139704400004048</title>\n<path d=\"M284,-1095.4551C284,-1087.3828 284,-1077.6764 284,-1068.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"287.5001,-1068.5903 284,-1058.5904 280.5001,-1068.5904 287.5001,-1068.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139704400686800 -->\n<g class=\"node\" id=\"node93\">\n<title>139704400686800</title>\n<polygon fill=\"none\" points=\"77.5,-949.5 77.5,-985.5 334.5,-985.5 334.5,-949.5 77.5,-949.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"159.5\" y=\"-963.8\">Encoder-11-FeedForward</text>\n<polyline fill=\"none\" points=\"241.5,-949.5 241.5,-985.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"288\" y=\"-963.8\">FeedForward</text>\n</g>\n<!-- 139704400004048&#45;&gt;139704400686800 -->\n<g class=\"edge\" id=\"edge113\">\n<title>139704400004048-&gt;139704400686800</title>\n<path d=\"M264.7191,-1022.4551C255.0627,-1013.4177 243.2176,-1002.3319 232.7016,-992.4899\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"235.0223,-989.8681 225.3294,-985.5904 230.239,-994.979 235.0223,-989.8681\" stroke=\"#000000\"/>\n</g>\n<!-- 139704399119056 -->\n<g class=\"node\" id=\"node95\">\n<title>139704399119056</title>\n<polygon fill=\"none\" points=\"166,-803.5 166,-839.5 402,-839.5 402,-803.5 166,-803.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263\" y=\"-817.8\">Encoder-11-FeedForward-Add</text>\n<polyline fill=\"none\" points=\"360,-803.5 360,-839.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"381\" y=\"-817.8\">Add</text>\n</g>\n<!-- 139704400004048&#45;&gt;139704399119056 -->\n<g class=\"edge\" id=\"edge115\">\n<title>139704400004048-&gt;139704399119056</title>\n<path d=\"M312.0464,-1022.325C324.0147,-1012.9899 336.8488,-1000.5175 344,-986 365.6355,-942.078 370.8667,-920.2924 350,-876 344.4031,-864.1199 334.805,-853.9472 324.6353,-845.7189\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"326.5179,-842.7554 316.4224,-839.5413 322.3101,-848.3495 326.5179,-842.7554\" stroke=\"#000000\"/>\n</g>\n<!-- 139704399036560 -->\n<g class=\"node\" id=\"node94\">\n<title>139704399036560</title>\n<polygon fill=\"none\" points=\"57.5,-876.5 57.5,-912.5 340.5,-912.5 340.5,-876.5 57.5,-876.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"166.5\" y=\"-890.8\">Encoder-11-FeedForward-Dropout</text>\n<polyline fill=\"none\" points=\"275.5,-876.5 275.5,-912.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"308\" y=\"-890.8\">Dropout</text>\n</g>\n<!-- 139704400686800&#45;&gt;139704399036560 -->\n<g class=\"edge\" id=\"edge114\">\n<title>139704400686800-&gt;139704399036560</title>\n<path d=\"M204.2697,-949.4551C203.4956,-941.3828 202.5649,-931.6764 201.7024,-922.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"205.1733,-922.2106 200.7347,-912.5904 198.2053,-922.8788 205.1733,-922.2106\" stroke=\"#000000\"/>\n</g>\n<!-- 139704399036560&#45;&gt;139704399119056 -->\n<g class=\"edge\" id=\"edge116\">\n<title>139704399036560-&gt;139704399119056</title>\n<path d=\"M220.0112,-876.4551C230.6364,-867.3299 243.6932,-856.1165 255.2355,-846.2036\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"257.6299,-848.7609 262.9359,-839.5904 253.0692,-843.4505 257.6299,-848.7609\" stroke=\"#000000\"/>\n</g>\n<!-- 139704399200592 -->\n<g class=\"node\" id=\"node96\">\n<title>139704399200592</title>\n<polygon fill=\"none\" points=\"117,-730.5 117,-766.5 451,-766.5 451,-730.5 117,-730.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218.5\" y=\"-744.8\">Encoder-11-FeedForward-Norm</text>\n<polyline fill=\"none\" points=\"320,-730.5 320,-766.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"385.5\" y=\"-744.8\">LayerNormalization</text>\n</g>\n<!-- 139704399119056&#45;&gt;139704399200592 -->\n<g class=\"edge\" id=\"edge117\">\n<title>139704399119056-&gt;139704399200592</title>\n<path d=\"M284,-803.4551C284,-795.3828 284,-785.6764 284,-776.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"287.5001,-776.5903 284,-766.5904 280.5001,-776.5904 287.5001,-776.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139704399270800 -->\n<g class=\"node\" id=\"node97\">\n<title>139704399270800</title>\n<polygon fill=\"none\" points=\".5,-657.5 .5,-693.5 359.5,-693.5 359.5,-657.5 .5,-657.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"114\" y=\"-671.8\">Encoder-12-MultiHeadSelfAttention</text>\n<polyline fill=\"none\" points=\"227.5,-657.5 227.5,-693.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"293.5\" y=\"-671.8\">MultiHeadAttention</text>\n</g>\n<!-- 139704399200592&#45;&gt;139704399270800 -->\n<g class=\"edge\" id=\"edge118\">\n<title>139704399200592-&gt;139704399270800</title>\n<path d=\"M258.2921,-730.4551C244.917,-721.0667 228.3925,-709.4678 213.9768,-699.3491\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"215.9683,-696.4709 205.7726,-693.5904 211.9467,-702.2003 215.9683,-696.4709\" stroke=\"#000000\"/>\n</g>\n<!-- 139704404977744 -->\n<g class=\"node\" id=\"node99\">\n<title>139704404977744</title>\n<polygon fill=\"none\" points=\"136.5,-511.5 136.5,-547.5 435.5,-547.5 435.5,-511.5 136.5,-511.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"265\" y=\"-525.8\">Encoder-12-MultiHeadSelfAttention-Add</text>\n<polyline fill=\"none\" points=\"393.5,-511.5 393.5,-547.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"414.5\" y=\"-525.8\">Add</text>\n</g>\n<!-- 139704399200592&#45;&gt;139704404977744 -->\n<g class=\"edge\" id=\"edge120\">\n<title>139704399200592-&gt;139704404977744</title>\n<path d=\"M329.5114,-730.3446C344.887,-721.7593 360.3385,-709.8425 369,-694 377.2708,-678.8721 375.1095,-602.0386 365,-584 357.9861,-571.4849 346.7537,-561.2232 334.9338,-553.1149\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"336.588,-550.0185 326.2752,-547.5899 332.8226,-555.9196 336.588,-550.0185\" stroke=\"#000000\"/>\n</g>\n<!-- 139704399254416 -->\n<g class=\"node\" id=\"node98\">\n<title>139704399254416</title>\n<polygon fill=\"none\" points=\"11.5,-584.5 11.5,-620.5 356.5,-620.5 356.5,-584.5 11.5,-584.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"151.5\" y=\"-598.8\">Encoder-12-MultiHeadSelfAttention-Dropout</text>\n<polyline fill=\"none\" points=\"291.5,-584.5 291.5,-620.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"324\" y=\"-598.8\">Dropout</text>\n</g>\n<!-- 139704399270800&#45;&gt;139704399254416 -->\n<g class=\"edge\" id=\"edge119\">\n<title>139704399270800-&gt;139704399254416</title>\n<path d=\"M180.9888,-657.4551C181.4311,-649.3828 181.9629,-639.6764 182.4558,-630.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"185.9563,-630.7669 183.0087,-620.5904 178.9668,-630.3839 185.9563,-630.7669\" stroke=\"#000000\"/>\n</g>\n<!-- 139704399254416&#45;&gt;139704404977744 -->\n<g class=\"edge\" id=\"edge121\">\n<title>139704399254416-&gt;139704404977744</title>\n<path d=\"M209.2135,-584.4551C222.2088,-575.1545 238.2355,-563.6844 252.2796,-553.6332\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"254.6281,-556.2565 260.7231,-547.5904 250.5541,-550.5641 254.6281,-556.2565\" stroke=\"#000000\"/>\n</g>\n<!-- 139704398795472 -->\n<g class=\"node\" id=\"node100\">\n<title>139704398795472</title>\n<polygon fill=\"none\" points=\"88,-438.5 88,-474.5 484,-474.5 484,-438.5 88,-438.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"220.5\" y=\"-452.8\">Encoder-12-MultiHeadSelfAttention-Norm</text>\n<polyline fill=\"none\" points=\"353,-438.5 353,-474.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"418.5\" y=\"-452.8\">LayerNormalization</text>\n</g>\n<!-- 139704404977744&#45;&gt;139704398795472 -->\n<g class=\"edge\" id=\"edge122\">\n<title>139704404977744-&gt;139704398795472</title>\n<path d=\"M286,-511.4551C286,-503.3828 286,-493.6764 286,-484.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"289.5001,-484.5903 286,-474.5904 282.5001,-484.5904 289.5001,-484.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139704399270672 -->\n<g class=\"node\" id=\"node101\">\n<title>139704399270672</title>\n<polygon fill=\"none\" points=\"72,-365.5 72,-401.5 330,-401.5 330,-365.5 72,-365.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"154.5\" y=\"-379.8\">Encoder-12-FeedForward</text>\n<polyline fill=\"none\" points=\"237,-365.5 237,-401.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"283.5\" y=\"-379.8\">FeedForward</text>\n</g>\n<!-- 139704398795472&#45;&gt;139704399270672 -->\n<g class=\"edge\" id=\"edge123\">\n<title>139704398795472-&gt;139704399270672</title>\n<path d=\"M264.9888,-438.4551C254.3636,-429.3299 241.3068,-418.1165 229.7645,-408.2036\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"231.9308,-405.4505 222.0641,-401.5904 227.3701,-410.7609 231.9308,-405.4505\" stroke=\"#000000\"/>\n</g>\n<!-- 139704398586512 -->\n<g class=\"node\" id=\"node103\">\n<title>139704398586512</title>\n<polygon fill=\"none\" points=\"161.5,-219.5 161.5,-255.5 398.5,-255.5 398.5,-219.5 161.5,-219.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"259\" y=\"-233.8\">Encoder-12-FeedForward-Add</text>\n<polyline fill=\"none\" points=\"356.5,-219.5 356.5,-255.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"377.5\" y=\"-233.8\">Add</text>\n</g>\n<!-- 139704398795472&#45;&gt;139704398586512 -->\n<g class=\"edge\" id=\"edge125\">\n<title>139704398795472-&gt;139704398586512</title>\n<path d=\"M309.896,-438.3839C320.6863,-428.8165 332.4579,-416.1331 339,-402 359.5783,-357.544 366.8779,-336.3161 346,-292 340.4031,-280.1199 330.805,-269.9472 320.6353,-261.7189\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"322.5179,-258.7554 312.4224,-255.5413 318.3101,-264.3495 322.5179,-258.7554\" stroke=\"#000000\"/>\n</g>\n<!-- 139704398552848 -->\n<g class=\"node\" id=\"node102\">\n<title>139704398552848</title>\n<polygon fill=\"none\" points=\"53.5,-292.5 53.5,-328.5 336.5,-328.5 336.5,-292.5 53.5,-292.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"162.5\" y=\"-306.8\">Encoder-12-FeedForward-Dropout</text>\n<polyline fill=\"none\" points=\"271.5,-292.5 271.5,-328.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"304\" y=\"-306.8\">Dropout</text>\n</g>\n<!-- 139704399270672&#45;&gt;139704398552848 -->\n<g class=\"edge\" id=\"edge124\">\n<title>139704399270672-&gt;139704398552848</title>\n<path d=\"M199.5169,-365.4551C198.8534,-357.3828 198.0556,-347.6764 197.3163,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"200.7944,-338.27 196.4869,-328.5904 193.8179,-338.8435 200.7944,-338.27\" stroke=\"#000000\"/>\n</g>\n<!-- 139704398552848&#45;&gt;139704398586512 -->\n<g class=\"edge\" id=\"edge126\">\n<title>139704398552848-&gt;139704398586512</title>\n<path d=\"M216.0112,-292.4551C226.6364,-283.3299 239.6932,-272.1165 251.2355,-262.2036\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"253.6299,-264.7609 258.9359,-255.5904 249.0692,-259.4505 253.6299,-264.7609\" stroke=\"#000000\"/>\n</g>\n<!-- 139706859130704 -->\n<g class=\"node\" id=\"node104\">\n<title>139706859130704</title>\n<polygon fill=\"none\" points=\"113,-146.5 113,-182.5 447,-182.5 447,-146.5 113,-146.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"214.5\" y=\"-160.8\">Encoder-12-FeedForward-Norm</text>\n<polyline fill=\"none\" points=\"316,-146.5 316,-182.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"381.5\" y=\"-160.8\">LayerNormalization</text>\n</g>\n<!-- 139704398586512&#45;&gt;139706859130704 -->\n<g class=\"edge\" id=\"edge127\">\n<title>139704398586512-&gt;139706859130704</title>\n<path d=\"M280,-219.4551C280,-211.3828 280,-201.6764 280,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"283.5001,-192.5903 280,-182.5904 276.5001,-192.5904 283.5001,-192.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139703281766672 -->\n<g class=\"node\" id=\"node105\">\n<title>139703281766672</title>\n<polygon fill=\"none\" points=\"179,-73.5 179,-109.5 381,-109.5 381,-73.5 179,-73.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"234\" y=\"-87.8\">non_masking_3</text>\n<polyline fill=\"none\" points=\"289,-73.5 289,-109.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"335\" y=\"-87.8\">NonMasking</text>\n</g>\n<!-- 139706859130704&#45;&gt;139703281766672 -->\n<g class=\"edge\" id=\"edge128\">\n<title>139706859130704-&gt;139703281766672</title>\n<path d=\"M280,-146.4551C280,-138.3828 280,-128.6764 280,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"283.5001,-119.5903 280,-109.5904 276.5001,-119.5904 283.5001,-119.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139703281766480 -->\n<g class=\"node\" id=\"node106\">\n<title>139703281766480</title>\n<polygon fill=\"none\" points=\"128,-.5 128,-36.5 432,-36.5 432,-.5 128,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"210\" y=\"-14.8\">start__end__prediction_2</text>\n<polyline fill=\"none\" points=\"292,-.5 292,-36.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"362\" y=\"-14.8\">Start_End_Prediction</text>\n</g>\n<!-- 139703281766672&#45;&gt;139703281766480 -->\n<g class=\"edge\" id=\"edge129\">\n<title>139703281766672-&gt;139703281766480</title>\n<path d=\"M280,-73.4551C280,-65.3828 280,-55.6764 280,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"283.5001,-46.5903 280,-36.5904 276.5001,-46.5904 283.5001,-46.5903\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU check\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7-u0_25rf3-",
        "outputId": "fcf535f5-261a-4814-8a28-e7d5e7ebc486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jan 20 04:06:49 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0    37W / 250W |   1401MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training -- Error\n",
        "# sess = K.get_session()\n",
        "# uninitialized_variables = set([i.decode('ascii') for i in sess.run(tf.report_uninitialized_variables())])\n",
        "# init = tf.variables_initializer([v for v in tf.global_variables() if v.name.split(':')[0] in uninitialized_variables])\n",
        "# sess.run(init)\n",
        "\n",
        "bert_model = get_bert_finetuning_model(model)\n",
        "bert_model.summary()\n",
        "\n",
        "# Training Start!\n",
        "history = bert_model.fit(train_x, train_y, batch_size=12, shuffle=True, verbose=1, epochs=2)"
      ],
      "metadata": {
        "id": "dh9TK0cUxbKT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95411385-7423-4235-9f94-c38f8c0c7432"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Input-Token (InputLayer)       [(None, 384)]        0           []                               \n",
            "                                                                                                  \n",
            " Input-Segment (InputLayer)     [(None, 384)]        0           []                               \n",
            "                                                                                                  \n",
            " Embedding-Token (TokenEmbeddin  [(None, 384, 768),  23440896    ['Input-Token[0][0]']            \n",
            " g)                              (30522, 768)]                                                    \n",
            "                                                                                                  \n",
            " Embedding-Segment (Embedding)  (None, 384, 768)     1536        ['Input-Segment[0][0]']          \n",
            "                                                                                                  \n",
            " Embedding-Token-Segment (Add)  (None, 384, 768)     0           ['Embedding-Token[0][0]',        \n",
            "                                                                  'Embedding-Segment[0][0]']      \n",
            "                                                                                                  \n",
            " Embedding-Position (PositionEm  (None, 384, 768)    294912      ['Embedding-Token-Segment[0][0]']\n",
            " bedding)                                                                                         \n",
            "                                                                                                  \n",
            " Embedding-Dropout (Dropout)    (None, 384, 768)     0           ['Embedding-Position[0][0]']     \n",
            "                                                                                                  \n",
            " Embedding-Norm (LayerNormaliza  (None, 384, 768)    1536        ['Embedding-Dropout[0][0]']      \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " Encoder-1-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Embedding-Norm[0][0]']         \n",
            " on (MultiHeadAttention)                                                                          \n",
            "                                                                                                  \n",
            " Encoder-1-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-1-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-1-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Embedding-Norm[0][0]',         \n",
            " on-Add (Add)                                                     'Encoder-1-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-1-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-1-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-1-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-1-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-1-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-1-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-1-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-1-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-1-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-1-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-1-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-2-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-1-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-2-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-2-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-2-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-1-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-2-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-2-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-2-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-2-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-2-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-2-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-2-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-2-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-2-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-2-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-2-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-2-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-3-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-2-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-3-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-3-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-3-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-2-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-3-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-3-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-3-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-3-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-3-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-3-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-3-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-3-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-3-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-3-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-3-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-3-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-4-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-3-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-4-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-4-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-4-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-3-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-4-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-4-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-4-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-4-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-4-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-4-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-4-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-4-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-4-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-4-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-4-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-4-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-5-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-4-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-5-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-5-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-5-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-4-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-5-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-5-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-5-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-5-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-5-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-5-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-5-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-5-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-5-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-5-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-5-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-5-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-6-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-5-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-6-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-6-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-6-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-5-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-6-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-6-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-6-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-6-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-6-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-6-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-6-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-6-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-6-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-6-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-6-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-6-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-7-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-6-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-7-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-7-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-7-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-6-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-7-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-7-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-7-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-7-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-7-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-7-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-7-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-7-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-7-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-7-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-7-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-7-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-8-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-7-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-8-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-8-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-8-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-7-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-8-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-8-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-8-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-8-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-8-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-8-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-8-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-8-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-8-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-8-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-8-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-8-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-9-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-8-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-9-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-9-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-9-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-8-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-9-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-9-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-9-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-9-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-9-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-9-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-9-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-9-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-9-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-9-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-9-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-9-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-10-MultiHeadSelfAttent  (None, 384, 768)    2362368     ['Encoder-9-FeedForward-Norm[0][0\n",
            " ion (MultiHeadAttention)                                        ]']                              \n",
            "                                                                                                  \n",
            " Encoder-10-MultiHeadSelfAttent  (None, 384, 768)    0           ['Encoder-10-MultiHeadSelfAttenti\n",
            " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
            "                                                                                                  \n",
            " Encoder-10-MultiHeadSelfAttent  (None, 384, 768)    0           ['Encoder-9-FeedForward-Norm[0][0\n",
            " ion-Add (Add)                                                   ]',                              \n",
            "                                                                  'Encoder-10-MultiHeadSelfAttenti\n",
            "                                                                 on-Dropout[0][0]']               \n",
            "                                                                                                  \n",
            " Encoder-10-MultiHeadSelfAttent  (None, 384, 768)    1536        ['Encoder-10-MultiHeadSelfAttenti\n",
            " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-10-FeedForward (FeedFo  (None, 384, 768)    4722432     ['Encoder-10-MultiHeadSelfAttenti\n",
            " rward)                                                          on-Norm[0][0]']                  \n",
            "                                                                                                  \n",
            " Encoder-10-FeedForward-Dropout  (None, 384, 768)    0           ['Encoder-10-FeedForward[0][0]'] \n",
            "  (Dropout)                                                                                       \n",
            "                                                                                                  \n",
            " Encoder-10-FeedForward-Add (Ad  (None, 384, 768)    0           ['Encoder-10-MultiHeadSelfAttenti\n",
            " d)                                                              on-Norm[0][0]',                  \n",
            "                                                                  'Encoder-10-FeedForward-Dropout[\n",
            "                                                                 0][0]']                          \n",
            "                                                                                                  \n",
            " Encoder-10-FeedForward-Norm (L  (None, 384, 768)    1536        ['Encoder-10-FeedForward-Add[0][0\n",
            " ayerNormalization)                                              ]']                              \n",
            "                                                                                                  \n",
            " Encoder-11-MultiHeadSelfAttent  (None, 384, 768)    2362368     ['Encoder-10-FeedForward-Norm[0][\n",
            " ion (MultiHeadAttention)                                        0]']                             \n",
            "                                                                                                  \n",
            " Encoder-11-MultiHeadSelfAttent  (None, 384, 768)    0           ['Encoder-11-MultiHeadSelfAttenti\n",
            " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
            "                                                                                                  \n",
            " Encoder-11-MultiHeadSelfAttent  (None, 384, 768)    0           ['Encoder-10-FeedForward-Norm[0][\n",
            " ion-Add (Add)                                                   0]',                             \n",
            "                                                                  'Encoder-11-MultiHeadSelfAttenti\n",
            "                                                                 on-Dropout[0][0]']               \n",
            "                                                                                                  \n",
            " Encoder-11-MultiHeadSelfAttent  (None, 384, 768)    1536        ['Encoder-11-MultiHeadSelfAttenti\n",
            " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-11-FeedForward (FeedFo  (None, 384, 768)    4722432     ['Encoder-11-MultiHeadSelfAttenti\n",
            " rward)                                                          on-Norm[0][0]']                  \n",
            "                                                                                                  \n",
            " Encoder-11-FeedForward-Dropout  (None, 384, 768)    0           ['Encoder-11-FeedForward[0][0]'] \n",
            "  (Dropout)                                                                                       \n",
            "                                                                                                  \n",
            " Encoder-11-FeedForward-Add (Ad  (None, 384, 768)    0           ['Encoder-11-MultiHeadSelfAttenti\n",
            " d)                                                              on-Norm[0][0]',                  \n",
            "                                                                  'Encoder-11-FeedForward-Dropout[\n",
            "                                                                 0][0]']                          \n",
            "                                                                                                  \n",
            " Encoder-11-FeedForward-Norm (L  (None, 384, 768)    1536        ['Encoder-11-FeedForward-Add[0][0\n",
            " ayerNormalization)                                              ]']                              \n",
            "                                                                                                  \n",
            " Encoder-12-MultiHeadSelfAttent  (None, 384, 768)    2362368     ['Encoder-11-FeedForward-Norm[0][\n",
            " ion (MultiHeadAttention)                                        0]']                             \n",
            "                                                                                                  \n",
            " Encoder-12-MultiHeadSelfAttent  (None, 384, 768)    0           ['Encoder-12-MultiHeadSelfAttenti\n",
            " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
            "                                                                                                  \n",
            " Encoder-12-MultiHeadSelfAttent  (None, 384, 768)    0           ['Encoder-11-FeedForward-Norm[0][\n",
            " ion-Add (Add)                                                   0]',                             \n",
            "                                                                  'Encoder-12-MultiHeadSelfAttenti\n",
            "                                                                 on-Dropout[0][0]']               \n",
            "                                                                                                  \n",
            " Encoder-12-MultiHeadSelfAttent  (None, 384, 768)    1536        ['Encoder-12-MultiHeadSelfAttenti\n",
            " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-12-FeedForward (FeedFo  (None, 384, 768)    4722432     ['Encoder-12-MultiHeadSelfAttenti\n",
            " rward)                                                          on-Norm[0][0]']                  \n",
            "                                                                                                  \n",
            " Encoder-12-FeedForward-Dropout  (None, 384, 768)    0           ['Encoder-12-FeedForward[0][0]'] \n",
            "  (Dropout)                                                                                       \n",
            "                                                                                                  \n",
            " Encoder-12-FeedForward-Add (Ad  (None, 384, 768)    0           ['Encoder-12-MultiHeadSelfAttenti\n",
            " d)                                                              on-Norm[0][0]',                  \n",
            "                                                                  'Encoder-12-FeedForward-Dropout[\n",
            "                                                                 0][0]']                          \n",
            "                                                                                                  \n",
            " Encoder-12-FeedForward-Norm (L  (None, 384, 768)    1536        ['Encoder-12-FeedForward-Add[0][0\n",
            " ayerNormalization)                                              ]']                              \n",
            "                                                                                                  \n",
            " non_masking_3 (NonMasking)     (None, 384, 768)     0           ['Encoder-12-FeedForward-Norm[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " start__end__prediction_3 (Star  [(None, 384),       1536        ['non_masking_3[0][0]']          \n",
            " t_End_Prediction)               (None, 384)]                                                     \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,794,880\n",
            "Trainable params: 108,794,880\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 87353 samples\n",
            "Epoch 1/2\n",
            "87353/87353 [==============================] - 5145s 59ms/sample - loss: 2.9515 - start__end__prediction_3_loss: 1.5206 - start__end__prediction_3_1_loss: 1.4310 - start__end__prediction_3_accuracy: 0.5839 - start__end__prediction_3_1_accuracy: 0.6224\n",
            "Epoch 2/2\n",
            "87353/87353 [==============================] - 5133s 59ms/sample - loss: 1.6944 - start__end__prediction_3_loss: 0.8945 - start__end__prediction_3_1_loss: 0.8001 - start__end__prediction_3_accuracy: 0.7197 - start__end__prediction_3_1_accuracy: 0.7586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# It shows accuracy about 76% in the above\n",
        "# Now need to get f1 score as alternative\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "preds = bert_model.predict(train_x)"
      ],
      "metadata": {
        "id": "NTcuBo7Xnu7P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "47cddcb1-f9d8-4688-9c19-9527a6674d6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-413c9c4c1926>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'bert_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_indexes = np.argmax(preds[0], axis=-1)\n",
        "end_indexes = np.argmax(preds[1], axis=-1)"
      ],
      "metadata": {
        "id": "4jtetDJLpyJY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "c48bfee5-3347-42a8-dde2-fd7fd6f2cb87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-e12362118d59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstart_indexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mend_indexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preds' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing F1 score of two tokens (start, end)\n",
        "\n",
        "# Start token ('start_indexes') : 82%\n",
        "print(classification_report(train_y[0], start_indexes))\n",
        "# End token ('end_indexes') : 85%\n",
        "print(classification_report(train_y[1], end_indexes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlvU8WpLvZeW",
        "outputId": "5946b9cd-8c1d-4227-828a-31723cca9087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           4       1.00      1.00      1.00         1\n",
            "           6       1.00      0.50      0.67         4\n",
            "           7       0.75      0.82      0.78        22\n",
            "           8       0.82      0.77      0.79        65\n",
            "           9       0.88      0.81      0.84       173\n",
            "          10       0.87      0.86      0.87       328\n",
            "          11       0.83      0.83      0.83       464\n",
            "          12       0.83      0.83      0.83       607\n",
            "          13       0.84      0.84      0.84       748\n",
            "          14       0.86      0.85      0.85       849\n",
            "          15       0.84      0.85      0.85       907\n",
            "          16       0.87      0.85      0.86       957\n",
            "          17       0.84      0.86      0.85       963\n",
            "          18       0.85      0.85      0.85       992\n",
            "          19       0.83      0.84      0.84      1001\n",
            "          20       0.83      0.84      0.83       972\n",
            "          21       0.81      0.83      0.82       988\n",
            "          22       0.82      0.85      0.83       986\n",
            "          23       0.83      0.84      0.84       938\n",
            "          24       0.84      0.85      0.84       931\n",
            "          25       0.83      0.86      0.85       959\n",
            "          26       0.83      0.82      0.82       911\n",
            "          27       0.80      0.83      0.81       909\n",
            "          28       0.82      0.84      0.83       894\n",
            "          29       0.84      0.83      0.83       901\n",
            "          30       0.82      0.82      0.82       869\n",
            "          31       0.81      0.80      0.81       822\n",
            "          32       0.85      0.83      0.84       871\n",
            "          33       0.81      0.81      0.81       820\n",
            "          34       0.83      0.84      0.83       798\n",
            "          35       0.80      0.82      0.81       819\n",
            "          36       0.83      0.83      0.83       826\n",
            "          37       0.83      0.82      0.83       810\n",
            "          38       0.81      0.83      0.82       776\n",
            "          39       0.83      0.83      0.83       787\n",
            "          40       0.84      0.80      0.82       794\n",
            "          41       0.79      0.84      0.81       729\n",
            "          42       0.81      0.82      0.82       749\n",
            "          43       0.83      0.81      0.82       776\n",
            "          44       0.82      0.82      0.82       736\n",
            "          45       0.80      0.82      0.81       760\n",
            "          46       0.80      0.81      0.80       772\n",
            "          47       0.81      0.84      0.82       735\n",
            "          48       0.82      0.82      0.82       752\n",
            "          49       0.80      0.82      0.81       720\n",
            "          50       0.83      0.84      0.83       719\n",
            "          51       0.81      0.81      0.81       674\n",
            "          52       0.82      0.84      0.83       720\n",
            "          53       0.83      0.83      0.83       672\n",
            "          54       0.83      0.81      0.82       701\n",
            "          55       0.82      0.81      0.81       667\n",
            "          56       0.82      0.80      0.81       690\n",
            "          57       0.82      0.80      0.81       630\n",
            "          58       0.82      0.81      0.81       657\n",
            "          59       0.80      0.83      0.82       701\n",
            "          60       0.82      0.80      0.81       673\n",
            "          61       0.83      0.84      0.83       645\n",
            "          62       0.81      0.80      0.81       658\n",
            "          63       0.83      0.83      0.83       615\n",
            "          64       0.83      0.83      0.83       596\n",
            "          65       0.84      0.83      0.83       658\n",
            "          66       0.84      0.80      0.82       617\n",
            "          67       0.81      0.82      0.81       586\n",
            "          68       0.82      0.83      0.82       576\n",
            "          69       0.84      0.84      0.84       604\n",
            "          70       0.85      0.80      0.82       584\n",
            "          71       0.83      0.81      0.82       602\n",
            "          72       0.83      0.82      0.83       604\n",
            "          73       0.83      0.83      0.83       605\n",
            "          74       0.83      0.85      0.84       569\n",
            "          75       0.81      0.82      0.81       589\n",
            "          76       0.82      0.78      0.80       544\n",
            "          77       0.79      0.83      0.81       521\n",
            "          78       0.83      0.83      0.83       557\n",
            "          79       0.81      0.82      0.81       548\n",
            "          80       0.81      0.82      0.82       542\n",
            "          81       0.83      0.80      0.81       509\n",
            "          82       0.82      0.80      0.81       527\n",
            "          83       0.86      0.83      0.84       571\n",
            "          84       0.82      0.82      0.82       507\n",
            "          85       0.81      0.83      0.82       490\n",
            "          86       0.81      0.81      0.81       514\n",
            "          87       0.81      0.82      0.82       515\n",
            "          88       0.80      0.80      0.80       486\n",
            "          89       0.83      0.83      0.83       474\n",
            "          90       0.80      0.82      0.81       503\n",
            "          91       0.85      0.81      0.83       461\n",
            "          92       0.82      0.83      0.82       498\n",
            "          93       0.81      0.81      0.81       471\n",
            "          94       0.80      0.82      0.81       484\n",
            "          95       0.85      0.82      0.84       485\n",
            "          96       0.80      0.83      0.81       429\n",
            "          97       0.78      0.83      0.80       465\n",
            "          98       0.77      0.78      0.77       455\n",
            "          99       0.80      0.78      0.79       456\n",
            "         100       0.81      0.80      0.80       490\n",
            "         101       0.79      0.78      0.79       451\n",
            "         102       0.77      0.79      0.78       485\n",
            "         103       0.78      0.81      0.79       453\n",
            "         104       0.80      0.79      0.79       456\n",
            "         105       0.81      0.80      0.81       447\n",
            "         106       0.78      0.80      0.79       433\n",
            "         107       0.79      0.82      0.80       432\n",
            "         108       0.76      0.80      0.78       407\n",
            "         109       0.79      0.79      0.79       433\n",
            "         110       0.79      0.83      0.81       422\n",
            "         111       0.82      0.83      0.83       401\n",
            "         112       0.81      0.82      0.81       399\n",
            "         113       0.81      0.82      0.82       417\n",
            "         114       0.81      0.80      0.81       387\n",
            "         115       0.80      0.79      0.79       370\n",
            "         116       0.80      0.82      0.81       388\n",
            "         117       0.84      0.81      0.82       372\n",
            "         118       0.82      0.79      0.81       413\n",
            "         119       0.83      0.81      0.82       370\n",
            "         120       0.83      0.81      0.82       382\n",
            "         121       0.82      0.79      0.80       356\n",
            "         122       0.79      0.78      0.78       341\n",
            "         123       0.81      0.83      0.82       350\n",
            "         124       0.81      0.79      0.80       364\n",
            "         125       0.78      0.83      0.81       298\n",
            "         126       0.83      0.78      0.81       344\n",
            "         127       0.81      0.83      0.82       316\n",
            "         128       0.82      0.79      0.81       320\n",
            "         129       0.81      0.81      0.81       300\n",
            "         130       0.81      0.82      0.81       284\n",
            "         131       0.83      0.80      0.82       311\n",
            "         132       0.80      0.79      0.80       296\n",
            "         133       0.81      0.82      0.82       298\n",
            "         134       0.79      0.80      0.79       299\n",
            "         135       0.80      0.78      0.79       281\n",
            "         136       0.80      0.83      0.81       303\n",
            "         137       0.83      0.80      0.82       279\n",
            "         138       0.79      0.78      0.79       244\n",
            "         139       0.83      0.82      0.83       256\n",
            "         140       0.84      0.84      0.84       246\n",
            "         141       0.85      0.82      0.84       236\n",
            "         142       0.83      0.80      0.82       250\n",
            "         143       0.81      0.81      0.81       221\n",
            "         144       0.81      0.80      0.81       205\n",
            "         145       0.80      0.77      0.79       210\n",
            "         146       0.82      0.76      0.79       180\n",
            "         147       0.78      0.78      0.78       209\n",
            "         148       0.83      0.82      0.82       195\n",
            "         149       0.77      0.77      0.77       185\n",
            "         150       0.84      0.77      0.80       217\n",
            "         151       0.82      0.80      0.81       200\n",
            "         152       0.81      0.77      0.79       176\n",
            "         153       0.81      0.79      0.80       196\n",
            "         154       0.79      0.79      0.79       182\n",
            "         155       0.77      0.84      0.80       166\n",
            "         156       0.82      0.80      0.81       165\n",
            "         157       0.83      0.83      0.83       173\n",
            "         158       0.86      0.75      0.80       142\n",
            "         159       0.83      0.84      0.84       153\n",
            "         160       0.81      0.82      0.82       157\n",
            "         161       0.82      0.78      0.80       143\n",
            "         162       0.76      0.81      0.79       148\n",
            "         163       0.86      0.83      0.84       161\n",
            "         164       0.86      0.85      0.85       141\n",
            "         165       0.69      0.73      0.71       135\n",
            "         166       0.72      0.76      0.74       114\n",
            "         167       0.85      0.83      0.84       156\n",
            "         168       0.79      0.81      0.80       146\n",
            "         169       0.77      0.78      0.78       122\n",
            "         170       0.87      0.78      0.82       133\n",
            "         171       0.79      0.84      0.81       105\n",
            "         172       0.83      0.76      0.79       116\n",
            "         173       0.87      0.85      0.86       110\n",
            "         174       0.81      0.83      0.82       101\n",
            "         175       0.84      0.77      0.80       112\n",
            "         176       0.75      0.78      0.76        98\n",
            "         177       0.78      0.83      0.81       110\n",
            "         178       0.77      0.81      0.79       102\n",
            "         179       0.75      0.78      0.77        92\n",
            "         180       0.86      0.74      0.80        92\n",
            "         181       0.83      0.84      0.84        95\n",
            "         182       0.88      0.86      0.87       101\n",
            "         183       0.83      0.85      0.84        91\n",
            "         184       0.89      0.85      0.87       100\n",
            "         185       0.90      0.80      0.85       105\n",
            "         186       0.79      0.82      0.81        78\n",
            "         187       0.85      0.79      0.82        91\n",
            "         188       0.86      0.87      0.87        93\n",
            "         189       0.76      0.75      0.75        76\n",
            "         190       0.82      0.79      0.81        78\n",
            "         191       0.81      0.82      0.82        89\n",
            "         192       0.88      0.90      0.89        86\n",
            "         193       0.90      0.88      0.89        75\n",
            "         194       0.84      0.84      0.84        79\n",
            "         195       0.86      0.80      0.83        81\n",
            "         196       0.70      0.78      0.74        51\n",
            "         197       0.83      0.76      0.79        83\n",
            "         198       0.81      0.80      0.81        65\n",
            "         199       0.86      0.88      0.87        68\n",
            "         200       0.77      0.79      0.78        61\n",
            "         201       0.85      0.73      0.79        48\n",
            "         202       0.84      0.74      0.79        62\n",
            "         203       0.81      0.73      0.77        66\n",
            "         204       0.70      0.79      0.74        57\n",
            "         205       0.77      0.81      0.79        62\n",
            "         206       0.76      0.74      0.75        50\n",
            "         207       0.76      0.74      0.75        43\n",
            "         208       0.83      0.82      0.83        55\n",
            "         209       0.75      0.77      0.76        61\n",
            "         210       0.80      0.77      0.79        57\n",
            "         211       0.79      0.75      0.77        56\n",
            "         212       0.68      0.70      0.69        43\n",
            "         213       0.78      0.83      0.80        46\n",
            "         214       0.85      0.91      0.88        44\n",
            "         215       0.88      0.81      0.84        36\n",
            "         216       0.81      0.94      0.87        31\n",
            "         217       0.87      0.85      0.86        39\n",
            "         218       0.82      0.82      0.82        40\n",
            "         219       0.84      0.75      0.79        51\n",
            "         220       0.78      0.92      0.84        38\n",
            "         221       0.71      0.73      0.72        33\n",
            "         222       0.82      0.74      0.78        42\n",
            "         223       0.75      0.73      0.74        45\n",
            "         224       0.71      0.69      0.70        35\n",
            "         225       0.87      0.79      0.82        42\n",
            "         226       0.82      0.82      0.82        34\n",
            "         227       0.71      0.71      0.71        28\n",
            "         228       0.74      0.77      0.76        22\n",
            "         229       0.85      0.91      0.88        32\n",
            "         230       0.81      0.73      0.77        41\n",
            "         231       0.81      0.86      0.83        35\n",
            "         232       0.84      0.82      0.83        33\n",
            "         233       0.92      0.88      0.90        26\n",
            "         234       0.84      0.74      0.78        42\n",
            "         235       0.83      0.73      0.77        33\n",
            "         236       0.79      0.84      0.82        37\n",
            "         237       0.88      0.75      0.81        20\n",
            "         238       0.86      0.80      0.83        30\n",
            "         239       0.78      0.83      0.81        30\n",
            "         240       0.74      0.92      0.82        25\n",
            "         241       0.73      0.79      0.76        24\n",
            "         242       0.90      0.75      0.82        36\n",
            "         243       0.82      0.78      0.79        40\n",
            "         244       0.75      0.69      0.72        26\n",
            "         245       0.89      0.76      0.82        33\n",
            "         246       0.87      1.00      0.93        26\n",
            "         247       0.85      1.00      0.92        28\n",
            "         248       0.91      0.95      0.93        21\n",
            "         249       0.93      0.81      0.87        16\n",
            "         250       0.71      0.83      0.77        12\n",
            "         251       0.69      0.85      0.76        13\n",
            "         252       1.00      0.67      0.80         9\n",
            "         253       0.83      0.88      0.86        17\n",
            "         254       0.89      0.89      0.89        18\n",
            "         255       0.94      0.77      0.85        22\n",
            "         256       0.71      0.92      0.80        13\n",
            "         257       1.00      0.77      0.87        26\n",
            "         258       0.90      0.90      0.90        10\n",
            "         259       0.86      0.86      0.86        14\n",
            "         260       0.87      0.76      0.81        17\n",
            "         261       0.92      0.80      0.86        15\n",
            "         262       1.00      0.92      0.96        13\n",
            "         263       0.57      0.80      0.67        10\n",
            "         264       0.71      0.92      0.80        13\n",
            "         265       0.93      0.76      0.84        17\n",
            "         266       0.80      0.86      0.83        14\n",
            "         267       1.00      0.70      0.82        10\n",
            "         268       0.67      0.80      0.73         5\n",
            "         269       0.83      0.83      0.83        12\n",
            "         270       0.69      0.85      0.76        13\n",
            "         271       0.69      0.60      0.64        15\n",
            "         272       0.79      0.79      0.79        19\n",
            "         273       0.85      0.73      0.79        15\n",
            "         274       0.75      0.86      0.80        14\n",
            "         275       0.86      0.75      0.80         8\n",
            "         276       0.79      1.00      0.88        11\n",
            "         277       1.00      0.82      0.90        11\n",
            "         278       1.00      0.89      0.94         9\n",
            "         279       0.71      0.71      0.71         7\n",
            "         280       0.67      0.57      0.62         7\n",
            "         281       0.90      0.82      0.86        11\n",
            "         282       0.75      0.86      0.80         7\n",
            "         283       1.00      0.92      0.96        12\n",
            "         284       0.87      0.93      0.90        14\n",
            "         285       0.80      0.80      0.80        10\n",
            "         286       0.80      1.00      0.89         8\n",
            "         287       0.80      1.00      0.89         8\n",
            "         288       0.82      0.75      0.78        12\n",
            "         289       0.80      0.80      0.80        10\n",
            "         290       0.75      0.90      0.82        10\n",
            "         291       0.50      1.00      0.67         1\n",
            "         292       1.00      0.83      0.91         6\n",
            "         293       0.86      1.00      0.92         6\n",
            "         294       1.00      0.60      0.75         5\n",
            "         295       0.67      1.00      0.80         6\n",
            "         296       1.00      0.86      0.92         7\n",
            "         297       1.00      1.00      1.00         6\n",
            "         298       0.86      0.86      0.86         7\n",
            "         299       1.00      0.83      0.91         6\n",
            "         300       0.50      0.50      0.50         2\n",
            "         301       0.89      0.73      0.80        11\n",
            "         302       1.00      0.67      0.80         9\n",
            "         303       0.80      0.89      0.84         9\n",
            "         304       0.33      1.00      0.50         2\n",
            "         305       0.80      0.57      0.67         7\n",
            "         306       0.86      1.00      0.92         6\n",
            "         307       1.00      0.67      0.80         3\n",
            "         308       1.00      1.00      1.00         2\n",
            "         309       0.67      1.00      0.80         2\n",
            "         310       0.50      0.25      0.33         4\n",
            "         311       0.88      1.00      0.93         7\n",
            "         312       0.60      1.00      0.75         3\n",
            "         313       1.00      1.00      1.00        10\n",
            "         314       1.00      0.17      0.29         6\n",
            "         315       0.86      1.00      0.92         6\n",
            "         316       1.00      0.83      0.91         6\n",
            "         317       0.50      0.67      0.57         3\n",
            "         318       1.00      0.75      0.86         4\n",
            "         319       0.90      0.82      0.86        11\n",
            "         320       1.00      0.50      0.67         2\n",
            "         321       1.00      1.00      1.00         3\n",
            "         322       0.80      0.67      0.73         6\n",
            "         323       1.00      0.67      0.80         3\n",
            "         324       1.00      0.67      0.80         6\n",
            "         325       0.67      1.00      0.80         2\n",
            "         326       1.00      0.86      0.92         7\n",
            "         327       0.50      1.00      0.67         2\n",
            "         328       1.00      0.80      0.89         5\n",
            "         329       1.00      1.00      1.00         3\n",
            "         330       1.00      1.00      1.00         5\n",
            "         331       0.33      0.50      0.40         2\n",
            "         332       1.00      1.00      1.00         5\n",
            "         333       1.00      1.00      1.00         4\n",
            "         334       1.00      0.80      0.89         5\n",
            "         335       0.67      0.67      0.67         6\n",
            "         336       1.00      1.00      1.00         2\n",
            "         337       1.00      1.00      1.00         4\n",
            "         338       0.67      0.67      0.67         3\n",
            "         339       1.00      0.50      0.67         4\n",
            "         340       0.67      0.67      0.67         3\n",
            "         341       0.71      0.71      0.71         7\n",
            "         342       0.00      0.00      0.00         1\n",
            "         343       0.50      1.00      0.67         1\n",
            "         344       1.00      1.00      1.00         1\n",
            "         345       0.00      0.00      0.00         0\n",
            "         346       1.00      1.00      1.00         3\n",
            "         347       0.50      0.50      0.50         2\n",
            "         348       0.75      0.75      0.75         4\n",
            "         349       0.25      0.50      0.33         2\n",
            "         350       1.00      0.50      0.67         2\n",
            "         351       1.00      1.00      1.00         1\n",
            "         352       1.00      1.00      1.00         4\n",
            "         353       1.00      0.50      0.67         4\n",
            "         354       1.00      0.50      0.67         4\n",
            "         355       0.80      0.80      0.80         5\n",
            "         356       1.00      1.00      1.00         2\n",
            "         357       1.00      1.00      1.00         2\n",
            "         358       1.00      0.50      0.67         2\n",
            "         359       0.50      1.00      0.67         3\n",
            "         360       0.00      0.00      0.00         2\n",
            "         361       0.50      0.50      0.50         2\n",
            "         362       1.00      0.50      0.67         2\n",
            "         363       0.67      0.67      0.67         3\n",
            "         364       0.80      1.00      0.89         4\n",
            "         365       1.00      0.67      0.80         3\n",
            "         366       1.00      0.67      0.80         3\n",
            "         367       0.67      0.67      0.67         3\n",
            "         368       1.00      0.50      0.67         2\n",
            "         369       0.00      0.00      0.00         0\n",
            "         370       1.00      1.00      1.00         1\n",
            "         371       0.00      0.00      0.00         1\n",
            "         372       1.00      1.00      1.00         3\n",
            "         373       1.00      1.00      1.00         1\n",
            "         375       1.00      1.00      1.00         1\n",
            "         376       0.75      1.00      0.86         3\n",
            "         377       1.00      1.00      1.00         1\n",
            "         378       1.00      1.00      1.00         1\n",
            "         380       0.00      0.00      0.00         0\n",
            "         381       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           0.82     87353\n",
            "   macro avg       0.81      0.80      0.80     87353\n",
            "weighted avg       0.82      0.82      0.82     87353\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           5       0.00      0.00      0.00         0\n",
            "           6       1.00      1.00      1.00         2\n",
            "           7       0.67      1.00      0.80         2\n",
            "           8       1.00      1.00      1.00        13\n",
            "           9       0.98      0.91      0.94        46\n",
            "          10       0.94      0.94      0.94       108\n",
            "          11       0.95      0.91      0.93       233\n",
            "          12       0.94      0.93      0.94       341\n",
            "          13       0.91      0.93      0.92       465\n",
            "          14       0.91      0.93      0.92       560\n",
            "          15       0.90      0.92      0.91       683\n",
            "          16       0.89      0.91      0.90       774\n",
            "          17       0.92      0.91      0.91       836\n",
            "          18       0.90      0.90      0.90       848\n",
            "          19       0.89      0.92      0.90       873\n",
            "          20       0.87      0.90      0.89       902\n",
            "          21       0.88      0.91      0.89       901\n",
            "          22       0.89      0.89      0.89       934\n",
            "          23       0.88      0.88      0.88       932\n",
            "          24       0.87      0.88      0.87       903\n",
            "          25       0.88      0.88      0.88       911\n",
            "          26       0.87      0.87      0.87       883\n",
            "          27       0.83      0.88      0.85       866\n",
            "          28       0.85      0.87      0.86       919\n",
            "          29       0.86      0.87      0.86       907\n",
            "          30       0.87      0.86      0.86       928\n",
            "          31       0.88      0.88      0.88       859\n",
            "          32       0.85      0.87      0.86       794\n",
            "          33       0.86      0.88      0.87       818\n",
            "          34       0.85      0.85      0.85       828\n",
            "          35       0.86      0.88      0.87       830\n",
            "          36       0.86      0.87      0.86       852\n",
            "          37       0.85      0.86      0.86       826\n",
            "          38       0.87      0.86      0.87       793\n",
            "          39       0.84      0.86      0.85       802\n",
            "          40       0.86      0.85      0.86       785\n",
            "          41       0.86      0.84      0.85       776\n",
            "          42       0.85      0.87      0.86       794\n",
            "          43       0.85      0.84      0.85       795\n",
            "          44       0.86      0.83      0.84       771\n",
            "          45       0.85      0.85      0.85       748\n",
            "          46       0.85      0.85      0.85       754\n",
            "          47       0.84      0.86      0.85       739\n",
            "          48       0.88      0.85      0.87       790\n",
            "          49       0.86      0.87      0.87       759\n",
            "          50       0.83      0.83      0.83       716\n",
            "          51       0.83      0.85      0.84       696\n",
            "          52       0.84      0.86      0.85       705\n",
            "          53       0.85      0.87      0.86       688\n",
            "          54       0.86      0.86      0.86       687\n",
            "          55       0.83      0.87      0.85       725\n",
            "          56       0.86      0.82      0.84       722\n",
            "          57       0.87      0.85      0.86       636\n",
            "          58       0.86      0.86      0.86       665\n",
            "          59       0.85      0.85      0.85       687\n",
            "          60       0.85      0.84      0.85       633\n",
            "          61       0.87      0.86      0.86       706\n",
            "          62       0.87      0.87      0.87       678\n",
            "          63       0.86      0.85      0.86       706\n",
            "          64       0.84      0.86      0.85       601\n",
            "          65       0.86      0.85      0.85       661\n",
            "          66       0.86      0.82      0.84       670\n",
            "          67       0.85      0.84      0.84       622\n",
            "          68       0.82      0.86      0.84       586\n",
            "          69       0.89      0.86      0.87       632\n",
            "          70       0.85      0.86      0.86       591\n",
            "          71       0.87      0.84      0.85       601\n",
            "          72       0.85      0.85      0.85       614\n",
            "          73       0.85      0.84      0.85       610\n",
            "          74       0.85      0.86      0.86       598\n",
            "          75       0.85      0.84      0.84       552\n",
            "          76       0.86      0.81      0.83       568\n",
            "          77       0.84      0.85      0.85       565\n",
            "          78       0.83      0.85      0.84       550\n",
            "          79       0.87      0.84      0.86       558\n",
            "          80       0.83      0.86      0.84       572\n",
            "          81       0.83      0.86      0.85       563\n",
            "          82       0.83      0.83      0.83       487\n",
            "          83       0.81      0.85      0.83       557\n",
            "          84       0.83      0.85      0.84       530\n",
            "          85       0.86      0.81      0.83       506\n",
            "          86       0.83      0.82      0.82       543\n",
            "          87       0.83      0.84      0.84       496\n",
            "          88       0.87      0.82      0.84       507\n",
            "          89       0.83      0.85      0.84       503\n",
            "          90       0.84      0.86      0.85       511\n",
            "          91       0.86      0.87      0.87       474\n",
            "          92       0.86      0.81      0.84       491\n",
            "          93       0.85      0.86      0.85       468\n",
            "          94       0.86      0.84      0.85       512\n",
            "          95       0.82      0.86      0.84       489\n",
            "          96       0.81      0.82      0.81       455\n",
            "          97       0.81      0.84      0.83       461\n",
            "          98       0.83      0.82      0.83       469\n",
            "          99       0.82      0.84      0.83       438\n",
            "         100       0.81      0.85      0.83       482\n",
            "         101       0.83      0.84      0.84       500\n",
            "         102       0.81      0.83      0.82       469\n",
            "         103       0.81      0.84      0.83       466\n",
            "         104       0.84      0.84      0.84       492\n",
            "         105       0.83      0.81      0.82       468\n",
            "         106       0.80      0.85      0.83       444\n",
            "         107       0.80      0.81      0.81       441\n",
            "         108       0.80      0.83      0.82       436\n",
            "         109       0.80      0.84      0.82       432\n",
            "         110       0.81      0.83      0.82       442\n",
            "         111       0.83      0.84      0.83       402\n",
            "         112       0.83      0.85      0.84       414\n",
            "         113       0.77      0.81      0.79       373\n",
            "         114       0.83      0.84      0.84       422\n",
            "         115       0.82      0.83      0.83       432\n",
            "         116       0.84      0.83      0.84       376\n",
            "         117       0.84      0.81      0.83       410\n",
            "         118       0.83      0.82      0.83       383\n",
            "         119       0.82      0.83      0.83       409\n",
            "         120       0.86      0.87      0.86       381\n",
            "         121       0.85      0.79      0.82       394\n",
            "         122       0.82      0.78      0.80       344\n",
            "         123       0.85      0.80      0.83       376\n",
            "         124       0.85      0.83      0.84       357\n",
            "         125       0.79      0.83      0.81       363\n",
            "         126       0.82      0.81      0.82       372\n",
            "         127       0.87      0.84      0.85       346\n",
            "         128       0.84      0.86      0.85       334\n",
            "         129       0.85      0.83      0.84       323\n",
            "         130       0.83      0.84      0.84       314\n",
            "         131       0.84      0.83      0.84       331\n",
            "         132       0.85      0.82      0.83       328\n",
            "         133       0.88      0.84      0.86       325\n",
            "         134       0.85      0.79      0.82       319\n",
            "         135       0.79      0.79      0.79       315\n",
            "         136       0.84      0.78      0.81       306\n",
            "         137       0.86      0.84      0.85       293\n",
            "         138       0.86      0.81      0.84       290\n",
            "         139       0.86      0.84      0.85       284\n",
            "         140       0.84      0.86      0.85       253\n",
            "         141       0.86      0.85      0.86       274\n",
            "         142       0.85      0.79      0.82       240\n",
            "         143       0.82      0.81      0.81       258\n",
            "         144       0.86      0.87      0.87       234\n",
            "         145       0.84      0.80      0.82       229\n",
            "         146       0.83      0.80      0.82       227\n",
            "         147       0.90      0.82      0.86       226\n",
            "         148       0.84      0.80      0.82       210\n",
            "         149       0.82      0.81      0.82       194\n",
            "         150       0.82      0.78      0.80       225\n",
            "         151       0.80      0.84      0.82       203\n",
            "         152       0.85      0.79      0.82       224\n",
            "         153       0.83      0.75      0.79       187\n",
            "         154       0.83      0.82      0.83       191\n",
            "         155       0.81      0.82      0.82       188\n",
            "         156       0.85      0.82      0.84       188\n",
            "         157       0.87      0.87      0.87       188\n",
            "         158       0.81      0.85      0.83       157\n",
            "         159       0.87      0.84      0.85       166\n",
            "         160       0.82      0.82      0.82       171\n",
            "         161       0.84      0.80      0.82       164\n",
            "         162       0.85      0.76      0.80       161\n",
            "         163       0.79      0.81      0.80       137\n",
            "         164       0.84      0.83      0.84       161\n",
            "         165       0.84      0.85      0.84       156\n",
            "         166       0.77      0.79      0.78       137\n",
            "         167       0.82      0.80      0.81       160\n",
            "         168       0.82      0.81      0.81       131\n",
            "         169       0.81      0.83      0.82       140\n",
            "         170       0.88      0.85      0.87       134\n",
            "         171       0.83      0.79      0.81       140\n",
            "         172       0.84      0.86      0.85       131\n",
            "         173       0.85      0.88      0.87       120\n",
            "         174       0.81      0.80      0.81       112\n",
            "         175       0.78      0.82      0.80       115\n",
            "         176       0.81      0.84      0.82       105\n",
            "         177       0.84      0.89      0.86       124\n",
            "         178       0.86      0.86      0.86       104\n",
            "         179       0.83      0.81      0.82       101\n",
            "         180       0.85      0.86      0.85       108\n",
            "         181       0.83      0.81      0.82        99\n",
            "         182       0.89      0.84      0.87       108\n",
            "         183       0.80      0.83      0.81        92\n",
            "         184       0.89      0.77      0.83       110\n",
            "         185       0.86      0.85      0.86       113\n",
            "         186       0.79      0.79      0.79        80\n",
            "         187       0.79      0.78      0.79        79\n",
            "         188       0.91      0.83      0.87       103\n",
            "         189       0.83      0.83      0.83        76\n",
            "         190       0.77      0.88      0.82        80\n",
            "         191       0.86      0.82      0.84        96\n",
            "         192       0.87      0.87      0.87        86\n",
            "         193       0.80      0.84      0.82        70\n",
            "         194       0.79      0.79      0.79        76\n",
            "         195       0.81      0.81      0.81        84\n",
            "         196       0.83      0.75      0.79        85\n",
            "         197       0.88      0.84      0.86        87\n",
            "         198       0.85      0.83      0.84        81\n",
            "         199       0.86      0.80      0.83        75\n",
            "         200       0.84      0.84      0.84        67\n",
            "         201       0.87      0.86      0.86        69\n",
            "         202       0.84      0.77      0.80        81\n",
            "         203       0.84      0.84      0.84        61\n",
            "         204       0.88      0.77      0.82        60\n",
            "         205       0.83      0.85      0.84        65\n",
            "         206       0.82      0.85      0.84        55\n",
            "         207       0.88      0.83      0.85        53\n",
            "         208       0.83      0.80      0.82        50\n",
            "         209       0.86      0.70      0.77        60\n",
            "         210       0.91      0.84      0.87        75\n",
            "         211       0.90      0.87      0.89        54\n",
            "         212       0.86      0.86      0.86        51\n",
            "         213       0.89      0.77      0.83        53\n",
            "         214       0.75      0.83      0.78        46\n",
            "         215       0.81      0.84      0.83        45\n",
            "         216       0.85      0.83      0.84        47\n",
            "         217       0.76      0.78      0.77        40\n",
            "         218       0.85      0.89      0.87        37\n",
            "         219       0.85      0.78      0.81        45\n",
            "         220       0.82      0.90      0.86        52\n",
            "         221       0.81      0.87      0.84        45\n",
            "         222       0.82      0.70      0.76        53\n",
            "         223       0.84      0.74      0.79        43\n",
            "         224       0.88      0.79      0.83        38\n",
            "         225       0.77      0.77      0.77        26\n",
            "         226       0.82      0.78      0.80        36\n",
            "         227       0.80      0.80      0.80        30\n",
            "         228       0.80      0.86      0.83        43\n",
            "         229       0.77      0.71      0.74        24\n",
            "         230       0.91      0.88      0.89        33\n",
            "         231       0.79      0.87      0.83        31\n",
            "         232       0.80      0.85      0.82        41\n",
            "         233       0.88      0.75      0.81        28\n",
            "         234       0.91      0.91      0.91        34\n",
            "         235       0.76      0.76      0.76        33\n",
            "         236       0.88      0.77      0.82        39\n",
            "         237       0.81      0.83      0.82        42\n",
            "         238       0.92      0.79      0.85        28\n",
            "         239       0.86      0.96      0.91        26\n",
            "         240       0.79      0.82      0.81        28\n",
            "         241       0.79      0.76      0.78        25\n",
            "         242       0.75      0.80      0.77        30\n",
            "         243       0.81      0.78      0.79        27\n",
            "         244       0.79      0.87      0.82        38\n",
            "         245       0.83      0.81      0.82        31\n",
            "         246       0.92      0.97      0.94        34\n",
            "         247       0.83      0.80      0.81        30\n",
            "         248       0.94      0.72      0.82        43\n",
            "         249       0.76      0.83      0.79        30\n",
            "         250       0.89      0.73      0.80        22\n",
            "         251       0.79      0.85      0.81        13\n",
            "         252       0.67      0.67      0.67        18\n",
            "         253       0.86      0.75      0.80        16\n",
            "         254       0.73      0.79      0.76        14\n",
            "         255       0.80      0.71      0.75        17\n",
            "         256       0.80      0.57      0.67        14\n",
            "         257       0.89      0.96      0.92        25\n",
            "         258       0.80      0.67      0.73        18\n",
            "         259       0.79      0.79      0.79        14\n",
            "         260       0.75      0.75      0.75        16\n",
            "         261       0.77      0.89      0.83        19\n",
            "         262       0.82      0.93      0.87        15\n",
            "         263       0.85      0.85      0.85        13\n",
            "         264       0.82      1.00      0.90        18\n",
            "         265       0.69      0.85      0.76        13\n",
            "         266       0.93      0.74      0.82        19\n",
            "         267       0.62      1.00      0.76         8\n",
            "         268       0.80      0.73      0.76        11\n",
            "         269       0.79      0.79      0.79        19\n",
            "         270       0.78      0.88      0.82         8\n",
            "         271       0.75      0.60      0.67         5\n",
            "         272       0.79      0.88      0.83        17\n",
            "         273       0.83      0.77      0.80        13\n",
            "         274       0.90      0.90      0.90        10\n",
            "         275       0.83      0.94      0.88        16\n",
            "         276       0.83      1.00      0.91        10\n",
            "         277       0.87      0.81      0.84        16\n",
            "         278       0.44      0.57      0.50         7\n",
            "         279       0.62      0.67      0.64        12\n",
            "         280       0.80      0.57      0.67         7\n",
            "         281       0.57      0.80      0.67         5\n",
            "         282       1.00      0.60      0.75        10\n",
            "         283       0.92      1.00      0.96        11\n",
            "         284       0.93      0.82      0.87        17\n",
            "         285       0.90      0.82      0.86        11\n",
            "         286       0.73      1.00      0.85        11\n",
            "         287       0.80      0.67      0.73        12\n",
            "         288       0.80      0.73      0.76        11\n",
            "         289       0.67      0.89      0.76         9\n",
            "         290       0.75      0.86      0.80         7\n",
            "         291       1.00      1.00      1.00        11\n",
            "         292       0.90      1.00      0.95         9\n",
            "         293       1.00      1.00      1.00         3\n",
            "         294       1.00      1.00      1.00         4\n",
            "         295       1.00      0.67      0.80         6\n",
            "         296       0.78      1.00      0.88         7\n",
            "         297       1.00      0.71      0.83         7\n",
            "         298       0.71      0.71      0.71         7\n",
            "         299       0.75      0.86      0.80         7\n",
            "         300       1.00      0.88      0.93         8\n",
            "         301       1.00      0.75      0.86         4\n",
            "         302       0.75      0.75      0.75         4\n",
            "         303       0.86      0.55      0.67        11\n",
            "         304       0.85      0.92      0.88        12\n",
            "         305       1.00      0.83      0.91         6\n",
            "         306       1.00      0.60      0.75         5\n",
            "         307       1.00      1.00      1.00         4\n",
            "         308       0.71      0.83      0.77         6\n",
            "         309       0.75      0.75      0.75         4\n",
            "         310       0.00      0.00      0.00         1\n",
            "         311       1.00      0.67      0.80         3\n",
            "         312       0.45      0.83      0.59         6\n",
            "         313       1.00      0.60      0.75         5\n",
            "         314       1.00      0.71      0.83         7\n",
            "         315       0.78      0.78      0.78         9\n",
            "         316       0.80      1.00      0.89         4\n",
            "         317       0.73      1.00      0.84         8\n",
            "         318       1.00      0.75      0.86         4\n",
            "         319       0.71      0.71      0.71         7\n",
            "         320       1.00      0.75      0.86         4\n",
            "         321       1.00      1.00      1.00         6\n",
            "         322       1.00      0.33      0.50         3\n",
            "         323       0.67      0.67      0.67         3\n",
            "         324       1.00      0.75      0.86         8\n",
            "         325       1.00      1.00      1.00         5\n",
            "         326       1.00      0.50      0.67         4\n",
            "         328       0.50      1.00      0.67         3\n",
            "         329       0.86      0.86      0.86         7\n",
            "         330       1.00      0.71      0.83         7\n",
            "         331       1.00      1.00      1.00         3\n",
            "         332       0.75      0.60      0.67         5\n",
            "         333       1.00      0.60      0.75         5\n",
            "         334       0.50      1.00      0.67         1\n",
            "         335       0.60      1.00      0.75         3\n",
            "         336       1.00      1.00      1.00         5\n",
            "         337       1.00      1.00      1.00         3\n",
            "         338       1.00      1.00      1.00         2\n",
            "         339       0.60      0.75      0.67         4\n",
            "         340       0.71      0.71      0.71         7\n",
            "         341       1.00      1.00      1.00         2\n",
            "         342       0.60      0.75      0.67         4\n",
            "         343       1.00      1.00      1.00         1\n",
            "         344       1.00      1.00      1.00         3\n",
            "         345       1.00      0.33      0.50         3\n",
            "         346       1.00      1.00      1.00         4\n",
            "         347       0.33      0.50      0.40         2\n",
            "         348       1.00      0.50      0.67         2\n",
            "         349       0.00      0.00      0.00         2\n",
            "         350       1.00      1.00      1.00         3\n",
            "         351       1.00      0.67      0.80         3\n",
            "         352       0.83      1.00      0.91         5\n",
            "         353       1.00      0.67      0.80         3\n",
            "         354       1.00      1.00      1.00         1\n",
            "         355       1.00      0.67      0.80         3\n",
            "         356       1.00      0.67      0.80         3\n",
            "         357       1.00      0.67      0.80         3\n",
            "         358       1.00      1.00      1.00         4\n",
            "         359       0.50      1.00      0.67         1\n",
            "         360       0.50      0.50      0.50         2\n",
            "         362       1.00      1.00      1.00         2\n",
            "         363       0.33      1.00      0.50         1\n",
            "         364       0.00      0.00      0.00         3\n",
            "         365       0.80      1.00      0.89         4\n",
            "         366       1.00      1.00      1.00         5\n",
            "         367       0.75      0.75      0.75         4\n",
            "         368       0.00      0.00      0.00         1\n",
            "         369       0.67      1.00      0.80         2\n",
            "         370       0.50      1.00      0.67         2\n",
            "         371       0.00      0.00      0.00         2\n",
            "         372       0.50      0.67      0.57         3\n",
            "         373       1.00      0.33      0.50         3\n",
            "         374       0.50      1.00      0.67         2\n",
            "         375       1.00      0.50      0.67         2\n",
            "         377       0.50      1.00      0.67         1\n",
            "         378       0.67      1.00      0.80         2\n",
            "         379       1.00      1.00      1.00         2\n",
            "         380       0.00      0.00      0.00         1\n",
            "         381       0.50      1.00      0.67         1\n",
            "         382       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           0.85     87353\n",
            "   macro avg       0.82      0.82      0.81     87353\n",
            "weighted avg       0.85      0.85      0.85     87353\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine_tuned trained model save\n",
        "# Since it takes too much time, trained model saved in the 'gdrive'\n",
        "path = \"gdrive/MyDrive/Colab Notebooks/squad\"\n",
        "bert_model.save_weights(path+\"/(Uncased)Squad.h5\")"
      ],
      "metadata": {
        "id": "Z5ktxZ6Hvuz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for loading pretrained BERT and fine_tuned trained model\n",
        "bert_model = get_bert_finetuning_model(model)\n",
        "path = \"gdrive/MyDrive/Colab Notebooks/squad\"\n",
        "bert_model.load_weights(path+\"/(Uncased)Squad.h5\")"
      ],
      "metadata": {
        "id": "LfNqTwMPw2i9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "8418174d-efd3-4a66-f6a0-c84ecc94e9e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Dataset"
      ],
      "metadata": {
        "id": "yvk0NddfsrNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SQuAD Test Dataset Setup\n",
        "# Since test dataset is different in form with train dataset, (question, context format is same, but answer is different)\n",
        "# A function make format unification required to be made\n",
        "\n",
        "def squad_json_to_dataframe_dev(input_file_path, record_path = ['data', 'paragraphs', 'qas', 'answers'], verbos = 1):\n",
        "  \"\"\"\n",
        "  input_file_path: path to the squad json file.\n",
        "  record_path: path to deepest level in json file default value is\n",
        "  ['data','paragraphs','qas','answers']\n",
        "  verbose: 0 to suppress it default is 1\n",
        "  \"\"\"\n",
        "  if verbose:\n",
        "    print(\"Reading the json file\")\n",
        "  file = json.loads(open(input_file_path).read())\n",
        "  if verbose:\n",
        "    print(\"processing...\")\n",
        "  \n",
        "  # Parsing different Level's in the json file\n",
        "  js = pd.io.json.json_normalize(file, record_path)\n",
        "  m = pd.io.json.json_normalize(file, record_path[:-1])\n",
        "  r = pd.io.json.json_normalize(file, record_path[:-2])\n",
        "\n",
        "  # Combining it into single dataframe\n",
        "  idx = np.repeat(r['context'].values, r.qas.str.len())\n",
        "  m['context'] = idx\n",
        "  main = m[['id', 'question', 'context', 'answers']].set_index('id').reset_index()\n",
        "  main['c_id'] = main['context'].factorize()[0]\n",
        "  if verbose:\n",
        "    print(\"shape of the dataframe is {}\".format(main.shape))\n",
        "    print(\"Done\")\n",
        "  return main"
      ],
      "metadata": {
        "id": "y1vRTO5Aq7v9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "b6260852-132f-4866-f5a3-454dffbc0cbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test dataset loading\n",
        "input_file_path = 'dev-v1.1.json'\n",
        "record_path = ['data', 'paragraphs', 'qas', 'answers']\n",
        "verbose = 0\n",
        "dev = squad_json_to_dataframe_dev(input_file_path=input_file_path, record_path=record_path)"
      ],
      "metadata": {
        "id": "bY4FqGKsvCXI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "75fca8b6-6f8c-4135-94dc-7aaf91f7780c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if test dataset is loaded well\n",
        "# ** Why answer needs to be in test dataset??? (Youngsun)\n",
        "dev"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "id": "8NmiyjisvWQ9",
        "outputId": "89271162-ec5b-4e70-eb79-47be8a0c3c08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a43ba6ba-b9b7-4375-b161-4d5ace62e428\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>answers</th>\n",
              "      <th>c_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>56be4db0acb8001400a502ec</td>\n",
              "      <td>Which NFL team represented the AFC at Super Bo...</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>[{'answer_start': 177, 'text': 'Denver Broncos...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>56be4db0acb8001400a502ed</td>\n",
              "      <td>Which NFL team represented the NFC at Super Bo...</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>[{'answer_start': 249, 'text': 'Carolina Panth...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>56be4db0acb8001400a502ee</td>\n",
              "      <td>Where did Super Bowl 50 take place?</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>[{'answer_start': 403, 'text': 'Santa Clara, C...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56be4db0acb8001400a502ef</td>\n",
              "      <td>Which NFL team won Super Bowl 50?</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>[{'answer_start': 177, 'text': 'Denver Broncos...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>56be4db0acb8001400a502f0</td>\n",
              "      <td>What color was used to emphasize the 50th anni...</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>[{'answer_start': 488, 'text': 'gold'}, {'answ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10565</th>\n",
              "      <td>5737aafd1c456719005744fb</td>\n",
              "      <td>What is the metric term less used than the New...</td>\n",
              "      <td>The pound-force has a metric counterpart, less...</td>\n",
              "      <td>[{'answer_start': 82, 'text': 'kilogram-force'...</td>\n",
              "      <td>2066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10566</th>\n",
              "      <td>5737aafd1c456719005744fc</td>\n",
              "      <td>What is the kilogram-force sometimes reffered ...</td>\n",
              "      <td>The pound-force has a metric counterpart, less...</td>\n",
              "      <td>[{'answer_start': 114, 'text': 'kilopond'}, {'...</td>\n",
              "      <td>2066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10567</th>\n",
              "      <td>5737aafd1c456719005744fd</td>\n",
              "      <td>What is a very seldom used unit of mass in the...</td>\n",
              "      <td>The pound-force has a metric counterpart, less...</td>\n",
              "      <td>[{'answer_start': 274, 'text': 'slug'}, {'answ...</td>\n",
              "      <td>2066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10568</th>\n",
              "      <td>5737aafd1c456719005744fe</td>\n",
              "      <td>What seldom used term of a unit of force equal...</td>\n",
              "      <td>The pound-force has a metric counterpart, less...</td>\n",
              "      <td>[{'answer_start': 712, 'text': 'kip'}, {'answe...</td>\n",
              "      <td>2066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10569</th>\n",
              "      <td>5737aafd1c456719005744ff</td>\n",
              "      <td>What is the seldom used force unit equal to on...</td>\n",
              "      <td>The pound-force has a metric counterpart, less...</td>\n",
              "      <td>[{'answer_start': 665, 'text': 'sthène'}, {'an...</td>\n",
              "      <td>2066</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10570 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a43ba6ba-b9b7-4375-b161-4d5ace62e428')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a43ba6ba-b9b7-4375-b161-4d5ace62e428 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a43ba6ba-b9b7-4375-b161-4d5ace62e428');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                             id  ...  c_id\n",
              "0      56be4db0acb8001400a502ec  ...     0\n",
              "1      56be4db0acb8001400a502ed  ...     0\n",
              "2      56be4db0acb8001400a502ee  ...     0\n",
              "3      56be4db0acb8001400a502ef  ...     0\n",
              "4      56be4db0acb8001400a502f0  ...     0\n",
              "...                         ...  ...   ...\n",
              "10565  5737aafd1c456719005744fb  ...  2066\n",
              "10566  5737aafd1c456719005744fc  ...  2066\n",
              "10567  5737aafd1c456719005744fd  ...  2066\n",
              "10568  5737aafd1c456719005744fe  ...  2066\n",
              "10569  5737aafd1c456719005744ff  ...  2066\n",
              "\n",
              "[10570 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Making a column for the number of answers\n",
        "dev['answer_len'] = dev['answers'].map(lambda x: len(x))"
      ],
      "metadata": {
        "id": "i7T1QWIdwqMX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "a53df5eb-55da-4db4-ea1d-3fc6be24c608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dev['answers'][0])\n",
        "print()\n",
        "print(dev['answers'][0][0]['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "id": "SPA1vnNaxtkI",
        "outputId": "ffb7686e-3dec-45dd-b1a3-27bbd19673e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'answer_start': 177, 'text': 'Denver Broncos'}, {'answer_start': 177, 'text': 'Denver Broncos'}, {'answer_start': 177, 'text': 'Denver Broncos'}]\n",
            "\n",
            "Denver Broncos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A function definition converting answers into list type for easy handling\n",
        "# text_len : Number of answers\n",
        "def get_text(text_len, answers):\n",
        "  texts = []\n",
        "  for i in range(text_len):\n",
        "    texts.append(answers[i]['text'])\n",
        "  return texts"
      ],
      "metadata": {
        "id": "n98MC0ezx0Q9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "283b8625-f9d3-4e0b-ba81-fc879c1aa1c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the above function\n",
        "# Taking the num of answers & answers as input, and converting them into List type\n",
        "get_text(3, dev['answers'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "qD_ohTnlzbZP",
        "outputId": "49c34ea4-589f-492e-951e-4d17eb0ed775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Denver Broncos', 'Denver Broncos', 'Denver Broncos']"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Doing the above job to all the test dataset\n",
        "dev['texts'] = dev.apply(lambda x: get_text(x['answer_len'], x['answers']), axis=1)"
      ],
      "metadata": {
        "id": "rCZ9-c3Rz1Be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "efb70e06-3939-4bba-c26d-42c08672c891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "Mj_adSrb5AzK",
        "outputId": "5ae6b7bb-bc31-4ef8-b221-e8116c65f64d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-aba3f861-4ae6-4fc3-8403-63767dfe880e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>answers</th>\n",
              "      <th>c_id</th>\n",
              "      <th>answer_len</th>\n",
              "      <th>texts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>56be4db0acb8001400a502ec</td>\n",
              "      <td>Which NFL team represented the AFC at Super Bo...</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>[{'answer_start': 177, 'text': 'Denver Broncos...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>[Denver Broncos, Denver Broncos, Denver Broncos]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>56be4db0acb8001400a502ed</td>\n",
              "      <td>Which NFL team represented the NFC at Super Bo...</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>[{'answer_start': 249, 'text': 'Carolina Panth...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>[Carolina Panthers, Carolina Panthers, Carolin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>56be4db0acb8001400a502ee</td>\n",
              "      <td>Where did Super Bowl 50 take place?</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>[{'answer_start': 403, 'text': 'Santa Clara, C...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>[Santa Clara, California, Levi's Stadium, Levi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56be4db0acb8001400a502ef</td>\n",
              "      <td>Which NFL team won Super Bowl 50?</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>[{'answer_start': 177, 'text': 'Denver Broncos...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>[Denver Broncos, Denver Broncos, Denver Broncos]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>56be4db0acb8001400a502f0</td>\n",
              "      <td>What color was used to emphasize the 50th anni...</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>[{'answer_start': 488, 'text': 'gold'}, {'answ...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>[gold, gold, gold]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aba3f861-4ae6-4fc3-8403-63767dfe880e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-aba3f861-4ae6-4fc3-8403-63767dfe880e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-aba3f861-4ae6-4fc3-8403-63767dfe880e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                         id  ...                                              texts\n",
              "0  56be4db0acb8001400a502ec  ...   [Denver Broncos, Denver Broncos, Denver Broncos]\n",
              "1  56be4db0acb8001400a502ed  ...  [Carolina Panthers, Carolina Panthers, Carolin...\n",
              "2  56be4db0acb8001400a502ee  ...  [Santa Clara, California, Levi's Stadium, Levi...\n",
              "3  56be4db0acb8001400a502ef  ...   [Denver Broncos, Denver Broncos, Denver Broncos]\n",
              "4  56be4db0acb8001400a502f0  ...                                 [gold, gold, gold]\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev['texts']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "_HEIqbnG5VZV",
        "outputId": "a72056b0-e6af-4e10-ab29-83a1680d6d83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         [Denver Broncos, Denver Broncos, Denver Broncos]\n",
              "1        [Carolina Panthers, Carolina Panthers, Carolin...\n",
              "2        [Santa Clara, California, Levi's Stadium, Levi...\n",
              "3         [Denver Broncos, Denver Broncos, Denver Broncos]\n",
              "4                                       [gold, gold, gold]\n",
              "                               ...                        \n",
              "10565    [kilogram-force, pound-force, kilogram-force (...\n",
              "10566    [kilopond, kilopond, kilopond, kilopond, kilop...\n",
              "10567    [slug, metric slug, metric slug, metric slug, ...\n",
              "10568                            [kip, kip, kip, kip, kip]\n",
              "10569             [sthène, sthène, sthène, sthène, sthène]\n",
              "Name: texts, Length: 10570, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Making Input Data (Test Dataset) for BERT Model -- Embedding\n",
        "# Utilizaing the above Input model in the past, but without answers!\n",
        "\n",
        "TEXT_COLUMN = 'texts'"
      ],
      "metadata": {
        "id": "aBFwTIPs5aAH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "cc044603-4ad1-4f01-8f1a-667be5890923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copied & Pasted, and a bit modified from the above function\n",
        "\n",
        "# Define a function calculating the length of answer in context dataset all at once\n",
        "def convert_data(data_df):\n",
        "  global tokenizer\n",
        "  indices, segments, target_start, target_end = [], [], [], []\n",
        "\n",
        "  for i in tqdm(range(len(data_df))):\n",
        "    # que : List of tokenized question\n",
        "    que, _ = tokenizer.encode(data_df[QUESTION_COLUMN][i])\n",
        "    # doc : List of tokenized context\n",
        "    doc, _ = tokenizer.encode(data_df[DATA_COLUMN][i])\n",
        "\n",
        "    # [CLS] token deleted in context\n",
        "    doc.pop(0)\n",
        "\n",
        "    # Length of question & context\n",
        "    que_len = len(que)\n",
        "    doc_len = len(doc)\n",
        "    # 1. Length of question\n",
        "    # The question is cut by the length of 64\n",
        "    if que_len > 64:\n",
        "      que = que[:63]\n",
        "      que.append(102) # [SEP] token added to make it clear the question block\n",
        "    # 2. Total length of question and context\n",
        "    # The total input is cut by the length of 384\n",
        "    if len(que+doc) > SEQ_LEN:\n",
        "      while len(que+doc) != SEQ_LEN:\n",
        "        doc.pop(-1)\n",
        "      doc.pop(-1)\n",
        "      doc.append(102)\n",
        "\n",
        "    # Segment embedding\n",
        "    # Question : 0 / Context 1 / Padding : 0 (remaining part for short sentences)\n",
        "        \n",
        "    ############################\n",
        "    ###### Segment 예시 ########\n",
        "    ############################\n",
        "    \n",
        "    # question, context, padding\n",
        "    # 00000000, 1111111, 0000000\n",
        "    \n",
        "    segment = [0]*len(que) + [1]*len(doc) + [0]*(SEQ_LEN-len(que)-len(doc))\n",
        "\n",
        "    # Padding\n",
        "    if len(que+doc) <= SEQ_LEN:\n",
        "      while len(que+doc) != SEQ_LEN:\n",
        "        doc.append(0)\n",
        "\n",
        "    # Final Input 'ids' (Question + Context)\n",
        "    ids = que + doc\n",
        "\n",
        "    # Sliding Part\n",
        "    texts = data_df[TEXT_COLUMN][i]\n",
        "    \n",
        "    for text_element in texts:\n",
        "      text = tokenizer.encode(text_element)[0]\n",
        "      text_slide_len = len(text[1:-1]) # text_slide_len = 8\n",
        "\n",
        "      for j in range(0, (len(doc))):\n",
        "        # exist_flag : showing whether it is answerable or not (similar with is_unanswerable in SimpleTransforemr)\n",
        "        # 0 : No answer / 1 : Having answer\n",
        "        exist_flag = 0 \n",
        "        if text[1:-1] == doc[j:j+text_slide_len]: # [0:8]->[1:9]->[2:10]->..->[159:160]\n",
        "          # Assign the location of answer (start, end)\n",
        "          ans_start = j + len(que)\n",
        "          ans_end = j + text_slide_len - 1 + len(que)\n",
        "          # If matched, exist_flag changed to 1\n",
        "          exist_flag = 1\n",
        "          break\n",
        "\n",
        "      # When no answer case (exist_flag = 0), starting & ending value become SEQ_LEN\n",
        "      # All the data of starting, ending = 384 (SEQ_LEN) will be deleted from the list\n",
        "      if exist_flag == 0:\n",
        "        ans_start = SEQ_LEN\n",
        "        ans_end = SEQ_LEN\n",
        "\n",
        "    # Input(ids), Segment saving into list type (indices, segments)\n",
        "    indices.append(ids)\n",
        "    segments.append(segment)\n",
        "    # Starting and ending info saving into list type (target_start, target_end)\n",
        "    target_start.append(ans_start)\n",
        "    target_end.append(ans_end)\n",
        "\n",
        "  # Converting the 4 lists into numpy array\n",
        "  indices_x = np.array(indices)\n",
        "  segments = np.array(segments)\n",
        "  target_start = np.array(target_start)\n",
        "  target_end = np.array(target_end)\n",
        "\n",
        "  # The cut part saved in del_list and deleted from data\n",
        "  del_list = np.where(target_start != SEQ_LEN)[0]\n",
        "  not_del_list = np.where(target_start == SEQ_LEN)[0]\n",
        "  indices_x = indices_x[del_list]\n",
        "  segments = segments[del_list]\n",
        "  \n",
        "  target_start = target_start[del_list]\n",
        "  target_end = target_end[del_list]\n",
        "\n",
        "  return [indices_x, segments], del_list"
      ],
      "metadata": {
        "id": "VB_PBnkw7BK3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "0dd4942c-d7c6-45cf-a62d-fbeb375edcad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load of the Calculator Function\n",
        "def load_data(pandas_dataframe):\n",
        "  data_df = pandas_dataframe\n",
        "  data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n",
        "  data_df[QUESTION_COLUMN] = data_df[QUESTION_COLUMN].astype(str)\n",
        "  data_x, data_y, del_list = convert_data(data_df)\n",
        "\n",
        "  return data_x, data_y, del_list"
      ],
      "metadata": {
        "id": "rx8nxmqB_giz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "c3bf0910-55fb-44f1-c106-84b6386c0462"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the test dataset into BERT input format\n",
        "dev_bert_input = convert_data(dev)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "X7ZZGcn8_t2A",
        "outputId": "0a784bfb-7f16-476d-d3bb-c9b5e49d11d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10570/10570 [00:26<00:00, 402.90it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Format : Token + Segment\n",
        "dev_bert_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "jdFavy3uAdNq",
        "outputId": "b2b7acff-d5c5-477f-c677-d2f5f6d6c66d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([array([[  101,  2029,  5088, ...,     0,     0,     0],\n",
              "         [  101,  2029,  5088, ...,     0,     0,     0],\n",
              "         [  101,  2073,  2106, ...,     0,     0,     0],\n",
              "         ...,\n",
              "         [  101,  2054,  2003, ...,     0,     0,     0],\n",
              "         [  101,  2054, 15839, ...,     0,     0,     0],\n",
              "         [  101,  2054,  2003, ...,     0,     0,     0]]),\n",
              "  array([[0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0]])],\n",
              " array([    0,     1,     2, ..., 10567, 10568, 10569]))"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cut the part over length of 384 ('del_list')\n",
        "dev_bert_input, del_list = dev_bert_input[0], dev_bert_input[1]\n",
        "dev = dev.iloc[del_list]\n",
        "dev = dev.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "C7o6t9sGAuup",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "9b0b21d5-9c35-462a-f45f-3a471f58a09c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hm1tLr2OkHET",
        "outputId": "720a11f8-b2ff-4abb-92ec-cfeb19f392eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-9f9605d1-1d71-4076-8809-4dfe47741199\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>answers</th>\n",
              "      <th>c_id</th>\n",
              "      <th>answer_len</th>\n",
              "      <th>texts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>56be4db0acb8001400a502ec</td>\n",
              "      <td>Which NFL team represented the AFC at Super Bo...</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>[{'answer_start': 177, 'text': 'Denver Broncos...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>[Denver Broncos, Denver Broncos, Denver Broncos]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>56be4db0acb8001400a502ed</td>\n",
              "      <td>Which NFL team represented the NFC at Super Bo...</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>[{'answer_start': 249, 'text': 'Carolina Panth...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>[Carolina Panthers, Carolina Panthers, Carolin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>56be4db0acb8001400a502ee</td>\n",
              "      <td>Where did Super Bowl 50 take place?</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>[{'answer_start': 403, 'text': 'Santa Clara, C...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>[Santa Clara, California, Levi's Stadium, Levi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56be4db0acb8001400a502ef</td>\n",
              "      <td>Which NFL team won Super Bowl 50?</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>[{'answer_start': 177, 'text': 'Denver Broncos...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>[Denver Broncos, Denver Broncos, Denver Broncos]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>56be4db0acb8001400a502f0</td>\n",
              "      <td>What color was used to emphasize the 50th anni...</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>[{'answer_start': 488, 'text': 'gold'}, {'answ...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>[gold, gold, gold]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10523</th>\n",
              "      <td>5737aafd1c456719005744fb</td>\n",
              "      <td>What is the metric term less used than the New...</td>\n",
              "      <td>The pound-force has a metric counterpart, less...</td>\n",
              "      <td>[{'answer_start': 82, 'text': 'kilogram-force'...</td>\n",
              "      <td>2066</td>\n",
              "      <td>5</td>\n",
              "      <td>[kilogram-force, pound-force, kilogram-force (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10524</th>\n",
              "      <td>5737aafd1c456719005744fc</td>\n",
              "      <td>What is the kilogram-force sometimes reffered ...</td>\n",
              "      <td>The pound-force has a metric counterpart, less...</td>\n",
              "      <td>[{'answer_start': 114, 'text': 'kilopond'}, {'...</td>\n",
              "      <td>2066</td>\n",
              "      <td>5</td>\n",
              "      <td>[kilopond, kilopond, kilopond, kilopond, kilop...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10525</th>\n",
              "      <td>5737aafd1c456719005744fd</td>\n",
              "      <td>What is a very seldom used unit of mass in the...</td>\n",
              "      <td>The pound-force has a metric counterpart, less...</td>\n",
              "      <td>[{'answer_start': 274, 'text': 'slug'}, {'answ...</td>\n",
              "      <td>2066</td>\n",
              "      <td>5</td>\n",
              "      <td>[slug, metric slug, metric slug, metric slug, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10526</th>\n",
              "      <td>5737aafd1c456719005744fe</td>\n",
              "      <td>What seldom used term of a unit of force equal...</td>\n",
              "      <td>The pound-force has a metric counterpart, less...</td>\n",
              "      <td>[{'answer_start': 712, 'text': 'kip'}, {'answe...</td>\n",
              "      <td>2066</td>\n",
              "      <td>5</td>\n",
              "      <td>[kip, kip, kip, kip, kip]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10527</th>\n",
              "      <td>5737aafd1c456719005744ff</td>\n",
              "      <td>What is the seldom used force unit equal to on...</td>\n",
              "      <td>The pound-force has a metric counterpart, less...</td>\n",
              "      <td>[{'answer_start': 665, 'text': 'sthène'}, {'an...</td>\n",
              "      <td>2066</td>\n",
              "      <td>5</td>\n",
              "      <td>[sthène, sthène, sthène, sthène, sthène]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10528 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9f9605d1-1d71-4076-8809-4dfe47741199')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9f9605d1-1d71-4076-8809-4dfe47741199 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9f9605d1-1d71-4076-8809-4dfe47741199');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                             id  ...                                              texts\n",
              "0      56be4db0acb8001400a502ec  ...   [Denver Broncos, Denver Broncos, Denver Broncos]\n",
              "1      56be4db0acb8001400a502ed  ...  [Carolina Panthers, Carolina Panthers, Carolin...\n",
              "2      56be4db0acb8001400a502ee  ...  [Santa Clara, California, Levi's Stadium, Levi...\n",
              "3      56be4db0acb8001400a502ef  ...   [Denver Broncos, Denver Broncos, Denver Broncos]\n",
              "4      56be4db0acb8001400a502f0  ...                                 [gold, gold, gold]\n",
              "...                         ...  ...                                                ...\n",
              "10523  5737aafd1c456719005744fb  ...  [kilogram-force, pound-force, kilogram-force (...\n",
              "10524  5737aafd1c456719005744fc  ...  [kilopond, kilopond, kilopond, kilopond, kilop...\n",
              "10525  5737aafd1c456719005744fd  ...  [slug, metric slug, metric slug, metric slug, ...\n",
              "10526  5737aafd1c456719005744fe  ...                          [kip, kip, kip, kip, kip]\n",
              "10527  5737aafd1c456719005744ff  ...           [sthène, sthène, sthène, sthène, sthène]\n",
              "\n",
              "[10528 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the only tokenized sentences input (1~10527)\n",
        "# Without the segment part\n",
        "indexes = dev_bert_input[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "uXyKyKakkHfS",
        "outputId": "f654cd38-406a-480f-8eca-378e0269473c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indexes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "hVmYAQvakWGb",
        "outputId": "f97c4ca5-b4e8-4223-de3d-8b96fd595d86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  101,  2029,  5088, ...,     0,     0,     0],\n",
              "       [  101,  2029,  5088, ...,     0,     0,     0],\n",
              "       [  101,  2073,  2106, ...,     0,     0,     0],\n",
              "       ...,\n",
              "       [  101,  2054,  2003, ...,     0,     0,     0],\n",
              "       [  101,  2054, 15839, ...,     0,     0,     0],\n",
              "       [  101,  2054,  2003, ...,     0,     0,     0]])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT prediction start to all the test dataset (10,527 * 384 tokens * 787 features), taking about 2 mins\n",
        "bert_predictions = bert_model.predict(dev_bert_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "N68yUPvkkY1a",
        "outputId": "ab2af40d-8188-4707-90c3-3c04868b02f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "NBxf1px-k74j",
        "outputId": "b74858fc-58fa-40ca-f0c3-fea6bb621ce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[5.3709050e-06, 7.7321565e-06, 5.7994794e-06, ..., 5.3632889e-06,\n",
              "         5.3633348e-06, 5.3637646e-06],\n",
              "        [1.9571360e-06, 2.4621302e-06, 2.1078642e-06, ..., 1.9545341e-06,\n",
              "         1.9547931e-06, 1.9553545e-06],\n",
              "        [5.4544432e-07, 1.3693410e-05, 4.6347361e-07, ..., 4.6127062e-07,\n",
              "         4.6152937e-07, 4.6169089e-07],\n",
              "        ...,\n",
              "        [5.5919617e-07, 3.6431882e-06, 4.7176647e-07, ..., 4.7234766e-07,\n",
              "         4.7220578e-07, 4.7221437e-07],\n",
              "        [1.0516256e-06, 5.4952608e-07, 4.1225911e-07, ..., 3.7572829e-07,\n",
              "         3.7566380e-07, 3.7612304e-07],\n",
              "        [5.4636922e-07, 8.9988237e-07, 4.7105181e-07, ..., 4.7742191e-07,\n",
              "         4.7719470e-07, 4.7725342e-07]], dtype=float32),\n",
              " array([[1.1915312e-05, 1.1619751e-05, 1.3103043e-05, ..., 1.2339353e-05,\n",
              "         1.2378755e-05, 1.2392942e-05],\n",
              "        [6.1851551e-06, 6.2660424e-06, 7.0434485e-06, ..., 6.7186561e-06,\n",
              "         6.7457108e-06, 6.7560318e-06],\n",
              "        [9.3594264e-07, 2.2043992e-06, 5.7649601e-07, ..., 5.1323985e-07,\n",
              "         5.1485165e-07, 5.1517981e-07],\n",
              "        ...,\n",
              "        [5.8868994e-07, 4.7238731e-07, 3.1479482e-07, ..., 2.9781106e-07,\n",
              "         2.9793890e-07, 2.9800341e-07],\n",
              "        [7.4833979e-06, 3.2672327e-07, 3.1991138e-07, ..., 3.2845466e-07,\n",
              "         3.2937436e-07, 3.2958647e-07],\n",
              "        [5.3623120e-07, 2.9906676e-07, 3.5138038e-07, ..., 3.3062275e-07,\n",
              "         3.3006390e-07, 3.2978232e-07]], dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Among 384 tokens, selecting the one having the largest probability\n",
        "start_indexes = np.argmax(bert_predictions[0], axis=-1)\n",
        "end_indexes = np.argmax(bert_predictions[1], axis=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "dJuptNQmmNJq",
        "outputId": "2ec55546-20c1-4615-db2d-7d91b123040b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete some non-sense case \n",
        "# e.g. position of starting token (start_indexes) is bigger than the ending token (end_indexes)\n",
        "not_del_list = np.where(start_indexes <= end_indexes)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "rGlaOya4nMm6",
        "outputId": "f516f4b3-8856-46c9-eb65-0874f5705d9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "not_del_list.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "qqosoatnntdC",
        "outputId": "ee5d4e82-d92e-468b-a029-e8eca978d229"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10227,)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_indexes = start_indexes[not_del_list]\n",
        "end_indexes = end_indexes[not_del_list]\n",
        "indexes = indexes[not_del_list]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "j_G3kgAAnvKa",
        "outputId": "eb9da545-6af9-4df2-e2e2-621c86f2c0e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Positions of starting tokens\n",
        "start_indexes[0:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "nJMLjaVIn_Xy",
        "outputId": "ba64f9b6-86d0-4065-b732-76fedc2a1dae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 46,  57,  78,  43, 118,  16,  72,  35,  16,  34,  73,  41,  80,\n",
              "        86, 156,  35,  40,  86,  80,  58,  77,  31,  42,  53,  41,  35,\n",
              "        42,  77,  11,  44,  27, 133,  66,  40,  87,  44,  85,  83, 127,\n",
              "        26,  28,  33,  87, 127,  95,  25,  43,  22,  42,  29,  44,  46,\n",
              "        24,  44,  65,  58,  72,  14,  59,  72,  25,  36,  57,  43,  68,\n",
              "        64,  60,  75,  75,  59,  70,  40,  47,  50,  61,  55,  38,  47,\n",
              "        61,  74,  16,  34,  55,  65,  76,  15,  31,  62,  62,  64,  12,\n",
              "        36,  67,  97,   9,  31,  56,  66,  10,  55])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Positions of end tokens\n",
        "end_indexes[0:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "tNWw3uUxoF3q",
        "outputId": "fcd8ce3a-9aa3-43e4-d357-8afc1966679e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 47,  58,  81,  44, 118,  30,  75,  37,  30,  36,  76,  42,  83,\n",
              "        87, 158,  35,  40,  87,  83,  60,  78,  31,  43,  54,  42,  35,\n",
              "        43,  80,  13,  45,  28, 133,  66,  41,  89,  45,  87,  85, 127,\n",
              "        27,  30,  34,  89, 127,  97,  26,  44, 132,  43,  30,  45,  47,\n",
              "        25,  45,  65,  59,  72,  14,  60,  72,  25,  36,  58,  43,  68,\n",
              "        65,  60,  75,  75,  60,  70,  40,  47,  50,  62,  56,  38,  47,\n",
              "        62,  74,  16,  36,  56,  68,  80,  15,  33,  65,  65,  67,  12,\n",
              "        38,  70,  97,   9,  33,  57,  69,  10,  56])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After deleting the non-sense case, rearranging dataset\n",
        "dev = dev.iloc[not_del_list].reset_index(drop=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "OyCuSwWjoJd5",
        "outputId": "66c581a5-3012-4ea1-d8b6-d75d5b205acd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Pk3_Ba_Dob18",
        "outputId": "d0d4db49-7bea-4b9d-b5c8-6d67562c03ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-681b6110-d6c6-41f5-a56a-d0dd83bc2e6e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>answers</th>\n",
              "      <th>c_id</th>\n",
              "      <th>answer_len</th>\n",
              "      <th>texts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>56be4db0acb8001400a502ec</td>\n",
              "      <td>Which NFL team represented the AFC at Super Bo...</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>[{'answer_start': 177, 'text': 'Denver Broncos...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>[Denver Broncos, Denver Broncos, Denver Broncos]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>56be4db0acb8001400a502ed</td>\n",
              "      <td>Which NFL team represented the NFC at Super Bo...</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>[{'answer_start': 249, 'text': 'Carolina Panth...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>[Carolina Panthers, Carolina Panthers, Carolin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>56be4db0acb8001400a502ee</td>\n",
              "      <td>Where did Super Bowl 50 take place?</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>[{'answer_start': 403, 'text': 'Santa Clara, C...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>[Santa Clara, California, Levi's Stadium, Levi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56be4db0acb8001400a502ef</td>\n",
              "      <td>Which NFL team won Super Bowl 50?</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>[{'answer_start': 177, 'text': 'Denver Broncos...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>[Denver Broncos, Denver Broncos, Denver Broncos]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>56be4db0acb8001400a502f0</td>\n",
              "      <td>What color was used to emphasize the 50th anni...</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>[{'answer_start': 488, 'text': 'gold'}, {'answ...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>[gold, gold, gold]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10222</th>\n",
              "      <td>5737aafd1c456719005744fb</td>\n",
              "      <td>What is the metric term less used than the New...</td>\n",
              "      <td>The pound-force has a metric counterpart, less...</td>\n",
              "      <td>[{'answer_start': 82, 'text': 'kilogram-force'...</td>\n",
              "      <td>2066</td>\n",
              "      <td>5</td>\n",
              "      <td>[kilogram-force, pound-force, kilogram-force (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10223</th>\n",
              "      <td>5737aafd1c456719005744fc</td>\n",
              "      <td>What is the kilogram-force sometimes reffered ...</td>\n",
              "      <td>The pound-force has a metric counterpart, less...</td>\n",
              "      <td>[{'answer_start': 114, 'text': 'kilopond'}, {'...</td>\n",
              "      <td>2066</td>\n",
              "      <td>5</td>\n",
              "      <td>[kilopond, kilopond, kilopond, kilopond, kilop...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10224</th>\n",
              "      <td>5737aafd1c456719005744fd</td>\n",
              "      <td>What is a very seldom used unit of mass in the...</td>\n",
              "      <td>The pound-force has a metric counterpart, less...</td>\n",
              "      <td>[{'answer_start': 274, 'text': 'slug'}, {'answ...</td>\n",
              "      <td>2066</td>\n",
              "      <td>5</td>\n",
              "      <td>[slug, metric slug, metric slug, metric slug, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10225</th>\n",
              "      <td>5737aafd1c456719005744fe</td>\n",
              "      <td>What seldom used term of a unit of force equal...</td>\n",
              "      <td>The pound-force has a metric counterpart, less...</td>\n",
              "      <td>[{'answer_start': 712, 'text': 'kip'}, {'answe...</td>\n",
              "      <td>2066</td>\n",
              "      <td>5</td>\n",
              "      <td>[kip, kip, kip, kip, kip]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10226</th>\n",
              "      <td>5737aafd1c456719005744ff</td>\n",
              "      <td>What is the seldom used force unit equal to on...</td>\n",
              "      <td>The pound-force has a metric counterpart, less...</td>\n",
              "      <td>[{'answer_start': 665, 'text': 'sthène'}, {'an...</td>\n",
              "      <td>2066</td>\n",
              "      <td>5</td>\n",
              "      <td>[sthène, sthène, sthène, sthène, sthène]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10227 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-681b6110-d6c6-41f5-a56a-d0dd83bc2e6e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-681b6110-d6c6-41f5-a56a-d0dd83bc2e6e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-681b6110-d6c6-41f5-a56a-d0dd83bc2e6e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                             id  ...                                              texts\n",
              "0      56be4db0acb8001400a502ec  ...   [Denver Broncos, Denver Broncos, Denver Broncos]\n",
              "1      56be4db0acb8001400a502ed  ...  [Carolina Panthers, Carolina Panthers, Carolin...\n",
              "2      56be4db0acb8001400a502ee  ...  [Santa Clara, California, Levi's Stadium, Levi...\n",
              "3      56be4db0acb8001400a502ef  ...   [Denver Broncos, Denver Broncos, Denver Broncos]\n",
              "4      56be4db0acb8001400a502f0  ...                                 [gold, gold, gold]\n",
              "...                         ...  ...                                                ...\n",
              "10222  5737aafd1c456719005744fb  ...  [kilogram-force, pound-force, kilogram-force (...\n",
              "10223  5737aafd1c456719005744fc  ...  [kilopond, kilopond, kilopond, kilopond, kilop...\n",
              "10224  5737aafd1c456719005744fd  ...  [slug, metric slug, metric slug, metric slug, ...\n",
              "10225  5737aafd1c456719005744fe  ...                          [kip, kip, kip, kip, kip]\n",
              "10226  5737aafd1c456719005744ff  ...           [sthène, sthène, sthène, sthène, sthène]\n",
              "\n",
              "[10227 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the length of test data\n",
        "length = len(dev)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "fMLRMyxyochr",
        "outputId": "ec26d440-62b4-43a3-d069-d39a2adba522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "7G1_Yl7coxJ6",
        "outputId": "08d18813-ad1e-4aee-97e0-d340d98471ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10227"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "untokenized = []\n",
        "\n",
        "for j in range(len(start_indexes)):\n",
        "  sentence = []\n",
        "  # Saving each tokenized word into the list of sentence (sentence = [])\n",
        "  for i in range(start_indexes[j], end_indexes[j]+1):\n",
        "    token_based_word = reverse_token_dict[indexes[j][i]]\n",
        "    sentence.append(token_based_word)\n",
        "  sentence_string = \"\"\n",
        "\n",
        "  for w in sentence:\n",
        "    # Special token ## delete, if a token starts with ##\n",
        "    if w.startswith(\"##\"):\n",
        "      w = w.replace(\"##\", \"\")\n",
        "    # If a token has no ##, a space added to the word\n",
        "    else:\n",
        "      w = \" \" + w\n",
        "    # Putting together all the tokens in list format\n",
        "    sentence_string += w\n",
        "  \n",
        "  # If senetence_string starts with a space (\" \"), delete the space\n",
        "  if sentence_string.startswith(\" \"):\n",
        "    sentence_string = \"\" + sentence_string[1:]\n",
        "  \n",
        "  # After putting together all the list of tokens, and assigning it in the list of the 'Untokenized'\n",
        "  untokenized.append(sentence_string)\n",
        "  sentences.append(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "XIUrx6nRoyrq",
        "outputId": "7351e3e6-b0c9-4ff0-a004-bdb0e5a5bae4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences[:30])\n",
        "print('\\n')\n",
        "print(untokenized[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "b3qHcL01uUgT",
        "outputId": "3d39adca-a8af-4f56-ea07-2ad8c0786ecc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['denver', 'broncos'], ['carolina', 'panthers'], ['levi', \"'\", 's', 'stadium'], ['denver', 'broncos'], ['gold'], ['american', 'football', 'game', 'to', 'determine', 'the', 'champion', 'of', 'the', 'national', 'football', 'league', '(', 'nfl', ')'], ['february', '7', ',', '2016'], ['american', 'football', 'conference'], ['american', 'football', 'game', 'to', 'determine', 'the', 'champion', 'of', 'the', 'national', 'football', 'league', '(', 'nfl', ')'], ['american', 'football', 'conference'], ['february', '7', ',', '2016'], ['denver', 'broncos'], ['levi', \"'\", 's', 'stadium'], ['san', 'francisco'], ['super', 'bowl', 'l'], ['2015'], ['2015'], ['san', 'francisco'], ['levi', \"'\", 's', 'stadium'], ['24', '–', '10'], ['february', '7'], ['2015'], ['denver', 'broncos'], ['carolina', 'panthers'], ['denver', 'broncos'], ['2015'], ['denver', 'broncos'], ['levi', \"'\", 's', 'stadium'], ['super', 'bowl', '50'], ['denver', 'broncos']]\n",
            "\n",
            "\n",
            "['denver broncos', 'carolina panthers', \"levi ' s stadium\", 'denver broncos', 'gold', 'american football game to determine the champion of the national football league ( nfl )', 'february 7 , 2016', 'american football conference', 'american football game to determine the champion of the national football league ( nfl )', 'american football conference', 'february 7 , 2016', 'denver broncos', \"levi ' s stadium\", 'san francisco', 'super bowl l', '2015', '2015', 'san francisco', \"levi ' s stadium\", '24 – 10', 'february 7', '2015', 'denver broncos', 'carolina panthers', 'denver broncos', '2015', 'denver broncos', \"levi ' s stadium\", 'super bowl 50', 'denver broncos']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the answers into list format\n",
        "dev_answers = []\n",
        "for i in range(length):\n",
        "  dev_answer = []\n",
        "  texts_dict = dev['answers'][i]\n",
        "\n",
        "  for j in range(len(texts_dict)):\n",
        "    dev_answer.append(texts_dict[j]['text'])\n",
        "  dev_answers.append(dev_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "dzMeuZfhugH6",
        "outputId": "36765018-1fc3-4ac4-bffa-932f8e0b0e40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev_answers[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "1XJqdtH6v0Mj",
        "outputId": "205274ac-1eab-44a5-caae-09b031654ca2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],\n",
              " ['Carolina Panthers', 'Carolina Panthers', 'Carolina Panthers'],\n",
              " ['Santa Clara, California',\n",
              "  \"Levi's Stadium\",\n",
              "  \"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"],\n",
              " ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],\n",
              " ['gold', 'gold', 'gold'],\n",
              " ['\"golden anniversary\"', 'gold-themed', '\"golden anniversary'],\n",
              " ['February 7, 2016', 'February 7', 'February 7, 2016'],\n",
              " ['American Football Conference',\n",
              "  'American Football Conference',\n",
              "  'American Football Conference'],\n",
              " ['\"golden anniversary\"', 'gold-themed', 'gold'],\n",
              " ['American Football Conference',\n",
              "  'American Football Conference',\n",
              "  'American Football Conference']]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the answers\n",
        "dev_tokens = []\n",
        "for i in dev_answers:\n",
        "  dev_tokened = []\n",
        "  for j in i:\n",
        "    temp_token = tokenizer.tokenize(j)\n",
        "    # Tokenize an answer\n",
        "    temp_token.pop(0)\n",
        "    # [CLS] elimination\n",
        "    temp_token.pop(-1)\n",
        "    # [SEP] elimination\n",
        "    dev_tokened.append(temp_token)\n",
        "  dev_tokens.append(dev_tokened)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "13fZ9vwLv1cb",
        "outputId": "99098c00-fc70-4564-8cf0-4aa99299592b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dev_tokens[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "IJZx3WvLwl6a",
        "outputId": "3437025c-ee21-4ddc-a41e-ffc2a050418a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[['denver', 'broncos'], ['denver', 'broncos'], ['denver', 'broncos']], [['carolina', 'panthers'], ['carolina', 'panthers'], ['carolina', 'panthers']], [['santa', 'clara', ',', 'california'], ['levi', \"'\", 's', 'stadium'], ['levi', \"'\", 's', 'stadium', 'in', 'the', 'san', 'francisco', 'bay', 'area', 'at', 'santa', 'clara', ',', 'california', '.']], [['denver', 'broncos'], ['denver', 'broncos'], ['denver', 'broncos']], [['gold'], ['gold'], ['gold']]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting tokenized answers into sentences\n",
        "# And putting all together\n",
        "dev_answer_lists = []\n",
        "for dev_answers in dev_tokens:\n",
        "  dev_answer_list = []\n",
        "  for dev_answer in dev_answers:\n",
        "    dev_answer_string = \" \".join(dev_answer)\n",
        "    dev_answer_list.append(dev_answer_string)\n",
        "  dev_answer_lists.append(dev_answer_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Qx4ukZO6wpHy",
        "outputId": "e73337ef-217e-4c68-a535-e85d615882bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dev_answer_lists[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "we5TK3LjxE87",
        "outputId": "65bd13b1-b8da-45ab-85c6-e229b443b894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['denver broncos', 'denver broncos', 'denver broncos'], ['carolina panthers', 'carolina panthers', 'carolina panthers'], ['santa clara , california', \"levi ' s stadium\", \"levi ' s stadium in the san francisco bay area at santa clara , california .\"], ['denver broncos', 'denver broncos', 'denver broncos'], ['gold', 'gold', 'gold']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Untokenizing (including deleting \" ##\" etc)\n",
        "dev_strings_end = []\n",
        "for dev_strings in dev_answer_lists:\n",
        "  dev_strings_processed = []\n",
        "  for dev_string in dev_strings:\n",
        "    dev_string = dev_string.replace(\" ##\", \"\")\n",
        "    dev_strings_processed.append(dev_string)\n",
        "  dev_strings_end.append(dev_strings_processed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "YzeFT3AxxHyb",
        "outputId": "0b4e9646-b231-4806-db38-3971f95c1109"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev_answers = dev_strings_end"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "uMtFl0B1xs50",
        "outputId": "e35ee4e5-a28b-421c-faee-11810e1e99c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dev_answers[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "nli6rzxaxvDL",
        "outputId": "f9e8187a-f2c3-4de4-b604-94f077e4ab3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['denver broncos', 'denver broncos', 'denver broncos'], ['carolina panthers', 'carolina panthers', 'carolina panthers'], ['santa clara , california', \"levi ' s stadium\", \"levi ' s stadium in the san francisco bay area at santa clara , california .\"], ['denver broncos', 'denver broncos', 'denver broncos'], ['gold', 'gold', 'gold']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation (F1 & EM)\n",
        "Reference : https://github.com/allenai/bi-att-flow/blob/master/squad/evaluate-v1.1.py"
      ],
      "metadata": {
        "id": "dnAhc8NNaEkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# F1 & EM function defined by myself\n",
        "from collections import Counter\n",
        "import string\n",
        "\n",
        "# In normalize step, doing some works such as converting words into lower case,\n",
        "# and deleting punctuations, clearing unnecessary spaces etc\n",
        "def normalize_answer(s):\n",
        "  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "  def remove_articles(text):\n",
        "    return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "  \n",
        "  def white_space_fix(text):\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "  def remove_punc(text):\n",
        "    exclude = set(string.punctuation)\n",
        "    return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "  def lower(text):\n",
        "    return text.lower()\n",
        "  \n",
        "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "  prediction_tokens = normalize_answer(prediction).split()\n",
        "  ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "  common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "  num_same = sum(common.values())\n",
        "  if num_same == 0:\n",
        "    return 0\n",
        "  precision = 1.0 * num_same / len(prediction_tokens)\n",
        "  recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "  f1 = (2 * precision * recall) / (precision + recall)\n",
        "  return f1\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "  return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "  scores_for_ground_truths = []\n",
        "  for ground_truth in ground_truths:\n",
        "    score = metric_fn(prediction, ground_truth)\n",
        "    scores_for_ground_truths.append(score)\n",
        "  return max(scores_for_ground_truths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "pjRGndV4xxAT",
        "outputId": "2386a0f7-8163-4e59-9932-f086cfab8ea8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating F1 Score\n",
        "# Theoretical Max : 88%\n",
        "f1_sum = 0\n",
        "\n",
        "for i in range(len(untokenized)):\n",
        "  f1 = metric_max_over_ground_truths(f1_score, untokenized[i], dev_answers[i])\n",
        "  f1_sum += f1\n",
        "print(\"f1 score: \", f1_sum/length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Sr3nPoMubM1t",
        "outputId": "0705c1fb-fc24-4368-f5a9-db233a4d8549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1 score:  0.8707850134616738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating EM score\n",
        "# Theoretical Max : 80%\n",
        "EM_sum = 0\n",
        "\n",
        "for i in range(len(untokenized)):\n",
        "  EM = metric_max_over_ground_truths(exact_match_score, untokenized[i], dev_answers[i])\n",
        "  EM_sum += EM\n",
        "print(\"EM score: \", EM_sum/length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "K9RKBP05eOZu",
        "outputId": "ec5652e1-cb74-45b4-fa62-75830e76bee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EM score:  0.787523222841498\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test by Random Data"
      ],
      "metadata": {
        "id": "84LS4gvdfcaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copied & Pasted, and a bit modified from the above function\n",
        "# Define a function calculating the length of answer in context dataset all at once\n",
        "def convert_data(data_df):\n",
        "  global tokenizer\n",
        "  indices, segments = [], []\n",
        "\n",
        "  que, _ = tokenizer.encode(data_df[QUESTION_COLUMN])\n",
        "  doc, _ = tokenizer.encode(data_df[DATA_COLUMN])\n",
        "  doc.pop(0)\n",
        "\n",
        "  que_len = len(que)\n",
        "  doc_len = len(doc)\n",
        "\n",
        "  # 1. Length of question\n",
        "  # The question is cut by the length of 64\n",
        "  if que_len > 64:\n",
        "    que = que[:63]\n",
        "    que.append(102) # [SEP] token added to make it clear the question block\n",
        "  \n",
        "  # 2. Total length of question and context\n",
        "  # The total input is cut by the length of 384\n",
        "  if len(que+doc) > SEQ_LEN:\n",
        "    while len(que+doc) != SEQ_LEN:\n",
        "      doc.pop(-1)\n",
        "    doc.pop(-1)\n",
        "    doc.append(102)\n",
        "\n",
        "    # Segment embedding\n",
        "    # Question : 0 / Context 1 / Padding : 0 (remaining part for short sentences)\n",
        "        \n",
        "    ############################\n",
        "    ###### Segment 예시 ########\n",
        "    ############################\n",
        "    \n",
        "    # question, context, padding\n",
        "    # 00000000, 1111111, 0000000\n",
        "    \n",
        "  segment = [0]*len(que) + [1]*len(doc) + [0]*(SEQ_LEN-len(que)-len(doc))\n",
        "\n",
        "  # Padding\n",
        "  if len(que+doc) <= SEQ_LEN:\n",
        "    while len(que+doc) != SEQ_LEN:\n",
        "      doc.append(0)\n",
        "\n",
        "  # Final Input 'ids' (Question + Context)\n",
        "  ids = que + doc\n",
        "\n",
        "  # Input(ids), Segment saving into list type (indices, segments)\n",
        "  indices.append(ids)\n",
        "  segments.append(segment)\n",
        "\n",
        "  # Converting the 4 lists into numpy array\n",
        "  indices = np.array(indices)\n",
        "  segments = np.array(segments)\n",
        "  \n",
        "  return [indices, segments]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "fzpsK6-2hK0a",
        "outputId": "5f4f4aa3-95c2-43ea-c792-fcb9ec051f2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_letter(df):\n",
        "  test_input = convert_data(df)\n",
        "  test_start, test_end = bert_model.predict(test_input)\n",
        "\n",
        "  indexes = test_input[0].tolist()[0]\n",
        "  start = np.argmax(test_start, axis=1).item()\n",
        "  end = np.argmax(test_end, axis=1).item()\n",
        "  start_tok = indexes[start]\n",
        "  end_tok = indexes[end]\n",
        "\n",
        "  print(\"Question: \", df['question'])\n",
        "  print(\"-\"*50)\n",
        "  print(\"Context: \", end = \" \")\n",
        "\n",
        "  def split_text(text, n):\n",
        "    for line in text.splitlines():\n",
        "      while len(line) > n:\n",
        "        x, line = line[:n], line[n:]\n",
        "        yield x\n",
        "      yield line\n",
        "  \n",
        "  for line in split_text(df['context'], 100):\n",
        "    print(line)\n",
        "\n",
        "  print(\"-\"*50)\n",
        "  print(\"ANSWER: \", end = \" \")\n",
        "  # print(\"\\n\")\n",
        "  sentences = []\n",
        "\n",
        "  for i in range(start, end+1):\n",
        "    token_based_word = reverse_token_dict[indexes[i]]\n",
        "    sentences.append(token_based_word)\n",
        "    # print(token_based_word, end= \" \")\n",
        "  # print(\"\\n\")\n",
        "  # print(\"Untokenized Answer: \", end= \"\")\n",
        "  \n",
        "  for w in sentences:\n",
        "    if w.startswith(\"##\"):\n",
        "      w = w.replace(\"##\", \"\")\n",
        "    else:\n",
        "      w = \" \" + w\n",
        "    print(w, end=\"\")\n",
        "  print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "kHUkxy_Pf4xG",
        "outputId": "ac5f72c6-2057-49a4-a349-d849892d77ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "for i in random.sample(range(1), 1):\n",
        "  answers = dev['answers'][i]\n",
        "  predict_letter(dev.iloc[i])\n",
        "  print(\"\")\n",
        "  print(\"real answer : \", answers)\n",
        "  print(\"\")"
      ],
      "metadata": {
        "id": "Pn9BJh4aewbm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "8e0a3802-e7f3-4f10-d7ff-f6089b508880"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:  Which NFL team represented the AFC at Super Bowl 50?\n",
            "--------------------------------------------------\n",
            "Context:  Super Bowl 50 was an American football game to determine the champion of the National Football Leagu\n",
            "e (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated\n",
            " the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super B\n",
            "owl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area \n",
            "at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniv\n",
            "ersary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of nam\n",
            "ing each Super Bowl game with Roman numerals (under which the game would have been known as \"Super B\n",
            "owl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
            "--------------------------------------------------\n",
            "ANSWER:   denver broncos\n",
            "\n",
            "real answer :  [{'answer_start': 177, 'text': 'Denver Broncos'}, {'answer_start': 177, 'text': 'Denver Broncos'}, {'answer_start': 177, 'text': 'Denver Broncos'}]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev.iloc[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "juKh3PTd1dMH",
        "outputId": "b856ccd3-4f47-46b6-f698-7e9e0cb1d951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id                                     56be4db0acb8001400a502ee\n",
              "question                    Where did Super Bowl 50 take place?\n",
              "context       Super Bowl 50 was an American football game to...\n",
              "answers       [{'answer_start': 403, 'text': 'Santa Clara, C...\n",
              "c_id                                                          0\n",
              "answer_len                                                    3\n",
              "texts         [Santa Clara, California, Levi's Stadium, Levi...\n",
              "Name: 2, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Customized Training"
      ],
      "metadata": {
        "id": "iSFE_SznKUgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Our dataset loading in Pandas format\n",
        "import pandas as pd\n",
        "fine_train = pd.read_excel('Train_QA_mod.xlsx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "BsEKigwILBpT",
        "outputId": "82020c7a-bc18-4b2f-c977-a1f3f99b7d9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fine_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 855
        },
        "id": "SFLuQo3hLDXL",
        "outputId": "740633f8-e192-4232-a95a-1688f936dc5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d3db9078-638b-48d7-8b75-ff15b1041546\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Index</th>\n",
              "      <th>Paper</th>\n",
              "      <th>Category</th>\n",
              "      <th>Question</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Experimental Studies of Brain Tumor Developmen...</td>\n",
              "      <td>Research Subject</td>\n",
              "      <td>What animal has been used?</td>\n",
              "      <td>It has been suggested that electromagnetic fie...</td>\n",
              "      <td>Fischer 344 rats</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Experimental Studies of Brain Tumor Developmen...</td>\n",
              "      <td>Number of Research Subject</td>\n",
              "      <td>How many animals were used?</td>\n",
              "      <td>It has been suggested that electromagnetic fie...</td>\n",
              "      <td>37 experimental rats and 37 matched controls</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Experimental Studies of Brain Tumor Developmen...</td>\n",
              "      <td>Radio Frequency</td>\n",
              "      <td>What is the signal frequency?</td>\n",
              "      <td>It has been suggested that electromagnetic fie...</td>\n",
              "      <td>915 MHz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>Experimental Studies of Brain Tumor Developmen...</td>\n",
              "      <td>Other Units of Exposure Level</td>\n",
              "      <td>How much W/kg was used?</td>\n",
              "      <td>It has been suggested that electromagnetic fie...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>Chronic Exposure of Cancer-Prone Mice to Low-L...</td>\n",
              "      <td>Research Subject</td>\n",
              "      <td>What animal has been used?</td>\n",
              "      <td>The purpose of this study was to determine whe...</td>\n",
              "      <td>C3H/ HeJ mice</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>663</th>\n",
              "      <td>368</td>\n",
              "      <td>Melatonin modulates 900 Mhz microwave-induced ...</td>\n",
              "      <td>Other Units of Exposure Level</td>\n",
              "      <td>How much W/kg was used?</td>\n",
              "      <td>Microwaves (MW) from cellular phones may affec...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>664</th>\n",
              "      <td>370</td>\n",
              "      <td>Does 900 MHz GSM Mobile Phone Exposure Affect ...</td>\n",
              "      <td>Research Subject</td>\n",
              "      <td>What animal has been used?</td>\n",
              "      <td>This study investigated the effects of cell ph...</td>\n",
              "      <td>Sprague-Dawley rats</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>665</th>\n",
              "      <td>370</td>\n",
              "      <td>Does 900 MHz GSM Mobile Phone Exposure Affect ...</td>\n",
              "      <td>Number of Research Subject</td>\n",
              "      <td>How many animals were used?</td>\n",
              "      <td>This study investigated the effects of cell ph...</td>\n",
              "      <td>Sixteen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>666</th>\n",
              "      <td>370</td>\n",
              "      <td>Does 900 MHz GSM Mobile Phone Exposure Affect ...</td>\n",
              "      <td>Radio Frequency</td>\n",
              "      <td>What is the signal frequency?</td>\n",
              "      <td>This study investigated the effects of cell ph...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>667</th>\n",
              "      <td>370</td>\n",
              "      <td>Does 900 MHz GSM Mobile Phone Exposure Affect ...</td>\n",
              "      <td>Other Units of Exposure Level</td>\n",
              "      <td>How much W/kg was used?</td>\n",
              "      <td>This study investigated the effects of cell ph...</td>\n",
              "      <td>0.52 W/kg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>668 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d3db9078-638b-48d7-8b75-ff15b1041546')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d3db9078-638b-48d7-8b75-ff15b1041546 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d3db9078-638b-48d7-8b75-ff15b1041546');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     Index  ...                                        Answer\n",
              "0        1  ...                              Fischer 344 rats\n",
              "1        1  ...  37 experimental rats and 37 matched controls\n",
              "2        1  ...                                       915 MHz\n",
              "3        1  ...                                           NaN\n",
              "4        3  ...                                 C3H/ HeJ mice\n",
              "..     ...  ...                                           ...\n",
              "663    368  ...                                           NaN\n",
              "664    370  ...                           Sprague-Dawley rats\n",
              "665    370  ...                                       Sixteen\n",
              "666    370  ...                                           NaN\n",
              "667    370  ...                                     0.52 W/kg\n",
              "\n",
              "[668 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters set up\n",
        "\n",
        "# Input sequence maximum length setup (Question + Context)\n",
        "SEQ_LEN = 384\n",
        "\n",
        "# Batch size (training size at one time)\n",
        "# Refer to this site https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "EPOCH = 4\n",
        "\n",
        "# Learning rate is set in very small number like 1.5e-5.\n",
        "# In fine tuning session, Learning rate must be very small \n",
        "# (Optimizer RAdam starts from LR 0 to 1.5e-5)\n",
        "LR = 1.5e-5\n",
        "\n",
        "# Model location designation\n",
        "pretrained_path = \"bert\"\n",
        "# Pretrained hyperparameters like weights\n",
        "checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
        "# Word corpus file location designation\n",
        "vocab_path =  os.path.join(pretrained_path, 'vocab.txt')\n",
        "\n",
        "# Configuration file designation\n",
        "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
        "\n",
        "# Column name of context\n",
        "FINE_DATA_COLUMN = \"Abstract\"\n",
        "# Column name of question\n",
        "FINE_QUESTION_COLUMN = \"Question\"\n",
        "# Column name of answer ('text')\n",
        "FINE_TEXT = \"Answer\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "v9WwWG6vLDUu",
        "outputId": "7e8e961a-0d90-4897-b2ad-a858f832a930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary Construction for indexing each word\n",
        "token_dict = {}\n",
        "with codecs.open(vocab_path, 'r', 'utf8') as reader:\n",
        "  for line in reader:\n",
        "    token = line.strip()\n",
        "    token_dict[token] = len(token_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "0rZzyDqpLDSO",
        "outputId": "5428b2c6-f4d3-4f10-e344-9f000e3b8a18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0y9FIdB1LDPl",
        "outputId": "0962db86-febe-4b31-c2a7-ab81eb8bdcf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'[PAD]': 0,\n",
              " '[unused0]': 1,\n",
              " '[unused1]': 2,\n",
              " '[unused2]': 3,\n",
              " '[unused3]': 4,\n",
              " '[unused4]': 5,\n",
              " '[unused5]': 6,\n",
              " '[unused6]': 7,\n",
              " '[unused7]': 8,\n",
              " '[unused8]': 9,\n",
              " '[unused9]': 10,\n",
              " '[unused10]': 11,\n",
              " '[unused11]': 12,\n",
              " '[unused12]': 13,\n",
              " '[unused13]': 14,\n",
              " '[unused14]': 15,\n",
              " '[unused15]': 16,\n",
              " '[unused16]': 17,\n",
              " '[unused17]': 18,\n",
              " '[unused18]': 19,\n",
              " '[unused19]': 20,\n",
              " '[unused20]': 21,\n",
              " '[unused21]': 22,\n",
              " '[unused22]': 23,\n",
              " '[unused23]': 24,\n",
              " '[unused24]': 25,\n",
              " '[unused25]': 26,\n",
              " '[unused26]': 27,\n",
              " '[unused27]': 28,\n",
              " '[unused28]': 29,\n",
              " '[unused29]': 30,\n",
              " '[unused30]': 31,\n",
              " '[unused31]': 32,\n",
              " '[unused32]': 33,\n",
              " '[unused33]': 34,\n",
              " '[unused34]': 35,\n",
              " '[unused35]': 36,\n",
              " '[unused36]': 37,\n",
              " '[unused37]': 38,\n",
              " '[unused38]': 39,\n",
              " '[unused39]': 40,\n",
              " '[unused40]': 41,\n",
              " '[unused41]': 42,\n",
              " '[unused42]': 43,\n",
              " '[unused43]': 44,\n",
              " '[unused44]': 45,\n",
              " '[unused45]': 46,\n",
              " '[unused46]': 47,\n",
              " '[unused47]': 48,\n",
              " '[unused48]': 49,\n",
              " '[unused49]': 50,\n",
              " '[unused50]': 51,\n",
              " '[unused51]': 52,\n",
              " '[unused52]': 53,\n",
              " '[unused53]': 54,\n",
              " '[unused54]': 55,\n",
              " '[unused55]': 56,\n",
              " '[unused56]': 57,\n",
              " '[unused57]': 58,\n",
              " '[unused58]': 59,\n",
              " '[unused59]': 60,\n",
              " '[unused60]': 61,\n",
              " '[unused61]': 62,\n",
              " '[unused62]': 63,\n",
              " '[unused63]': 64,\n",
              " '[unused64]': 65,\n",
              " '[unused65]': 66,\n",
              " '[unused66]': 67,\n",
              " '[unused67]': 68,\n",
              " '[unused68]': 69,\n",
              " '[unused69]': 70,\n",
              " '[unused70]': 71,\n",
              " '[unused71]': 72,\n",
              " '[unused72]': 73,\n",
              " '[unused73]': 74,\n",
              " '[unused74]': 75,\n",
              " '[unused75]': 76,\n",
              " '[unused76]': 77,\n",
              " '[unused77]': 78,\n",
              " '[unused78]': 79,\n",
              " '[unused79]': 80,\n",
              " '[unused80]': 81,\n",
              " '[unused81]': 82,\n",
              " '[unused82]': 83,\n",
              " '[unused83]': 84,\n",
              " '[unused84]': 85,\n",
              " '[unused85]': 86,\n",
              " '[unused86]': 87,\n",
              " '[unused87]': 88,\n",
              " '[unused88]': 89,\n",
              " '[unused89]': 90,\n",
              " '[unused90]': 91,\n",
              " '[unused91]': 92,\n",
              " '[unused92]': 93,\n",
              " '[unused93]': 94,\n",
              " '[unused94]': 95,\n",
              " '[unused95]': 96,\n",
              " '[unused96]': 97,\n",
              " '[unused97]': 98,\n",
              " '[unused98]': 99,\n",
              " '[UNK]': 100,\n",
              " '[CLS]': 101,\n",
              " '[SEP]': 102,\n",
              " '[MASK]': 103,\n",
              " '[unused99]': 104,\n",
              " '[unused100]': 105,\n",
              " '[unused101]': 106,\n",
              " '[unused102]': 107,\n",
              " '[unused103]': 108,\n",
              " '[unused104]': 109,\n",
              " '[unused105]': 110,\n",
              " '[unused106]': 111,\n",
              " '[unused107]': 112,\n",
              " '[unused108]': 113,\n",
              " '[unused109]': 114,\n",
              " '[unused110]': 115,\n",
              " '[unused111]': 116,\n",
              " '[unused112]': 117,\n",
              " '[unused113]': 118,\n",
              " '[unused114]': 119,\n",
              " '[unused115]': 120,\n",
              " '[unused116]': 121,\n",
              " '[unused117]': 122,\n",
              " '[unused118]': 123,\n",
              " '[unused119]': 124,\n",
              " '[unused120]': 125,\n",
              " '[unused121]': 126,\n",
              " '[unused122]': 127,\n",
              " '[unused123]': 128,\n",
              " '[unused124]': 129,\n",
              " '[unused125]': 130,\n",
              " '[unused126]': 131,\n",
              " '[unused127]': 132,\n",
              " '[unused128]': 133,\n",
              " '[unused129]': 134,\n",
              " '[unused130]': 135,\n",
              " '[unused131]': 136,\n",
              " '[unused132]': 137,\n",
              " '[unused133]': 138,\n",
              " '[unused134]': 139,\n",
              " '[unused135]': 140,\n",
              " '[unused136]': 141,\n",
              " '[unused137]': 142,\n",
              " '[unused138]': 143,\n",
              " '[unused139]': 144,\n",
              " '[unused140]': 145,\n",
              " '[unused141]': 146,\n",
              " '[unused142]': 147,\n",
              " '[unused143]': 148,\n",
              " '[unused144]': 149,\n",
              " '[unused145]': 150,\n",
              " '[unused146]': 151,\n",
              " '[unused147]': 152,\n",
              " '[unused148]': 153,\n",
              " '[unused149]': 154,\n",
              " '[unused150]': 155,\n",
              " '[unused151]': 156,\n",
              " '[unused152]': 157,\n",
              " '[unused153]': 158,\n",
              " '[unused154]': 159,\n",
              " '[unused155]': 160,\n",
              " '[unused156]': 161,\n",
              " '[unused157]': 162,\n",
              " '[unused158]': 163,\n",
              " '[unused159]': 164,\n",
              " '[unused160]': 165,\n",
              " '[unused161]': 166,\n",
              " '[unused162]': 167,\n",
              " '[unused163]': 168,\n",
              " '[unused164]': 169,\n",
              " '[unused165]': 170,\n",
              " '[unused166]': 171,\n",
              " '[unused167]': 172,\n",
              " '[unused168]': 173,\n",
              " '[unused169]': 174,\n",
              " '[unused170]': 175,\n",
              " '[unused171]': 176,\n",
              " '[unused172]': 177,\n",
              " '[unused173]': 178,\n",
              " '[unused174]': 179,\n",
              " '[unused175]': 180,\n",
              " '[unused176]': 181,\n",
              " '[unused177]': 182,\n",
              " '[unused178]': 183,\n",
              " '[unused179]': 184,\n",
              " '[unused180]': 185,\n",
              " '[unused181]': 186,\n",
              " '[unused182]': 187,\n",
              " '[unused183]': 188,\n",
              " '[unused184]': 189,\n",
              " '[unused185]': 190,\n",
              " '[unused186]': 191,\n",
              " '[unused187]': 192,\n",
              " '[unused188]': 193,\n",
              " '[unused189]': 194,\n",
              " '[unused190]': 195,\n",
              " '[unused191]': 196,\n",
              " '[unused192]': 197,\n",
              " '[unused193]': 198,\n",
              " '[unused194]': 199,\n",
              " '[unused195]': 200,\n",
              " '[unused196]': 201,\n",
              " '[unused197]': 202,\n",
              " '[unused198]': 203,\n",
              " '[unused199]': 204,\n",
              " '[unused200]': 205,\n",
              " '[unused201]': 206,\n",
              " '[unused202]': 207,\n",
              " '[unused203]': 208,\n",
              " '[unused204]': 209,\n",
              " '[unused205]': 210,\n",
              " '[unused206]': 211,\n",
              " '[unused207]': 212,\n",
              " '[unused208]': 213,\n",
              " '[unused209]': 214,\n",
              " '[unused210]': 215,\n",
              " '[unused211]': 216,\n",
              " '[unused212]': 217,\n",
              " '[unused213]': 218,\n",
              " '[unused214]': 219,\n",
              " '[unused215]': 220,\n",
              " '[unused216]': 221,\n",
              " '[unused217]': 222,\n",
              " '[unused218]': 223,\n",
              " '[unused219]': 224,\n",
              " '[unused220]': 225,\n",
              " '[unused221]': 226,\n",
              " '[unused222]': 227,\n",
              " '[unused223]': 228,\n",
              " '[unused224]': 229,\n",
              " '[unused225]': 230,\n",
              " '[unused226]': 231,\n",
              " '[unused227]': 232,\n",
              " '[unused228]': 233,\n",
              " '[unused229]': 234,\n",
              " '[unused230]': 235,\n",
              " '[unused231]': 236,\n",
              " '[unused232]': 237,\n",
              " '[unused233]': 238,\n",
              " '[unused234]': 239,\n",
              " '[unused235]': 240,\n",
              " '[unused236]': 241,\n",
              " '[unused237]': 242,\n",
              " '[unused238]': 243,\n",
              " '[unused239]': 244,\n",
              " '[unused240]': 245,\n",
              " '[unused241]': 246,\n",
              " '[unused242]': 247,\n",
              " '[unused243]': 248,\n",
              " '[unused244]': 249,\n",
              " '[unused245]': 250,\n",
              " '[unused246]': 251,\n",
              " '[unused247]': 252,\n",
              " '[unused248]': 253,\n",
              " '[unused249]': 254,\n",
              " '[unused250]': 255,\n",
              " '[unused251]': 256,\n",
              " '[unused252]': 257,\n",
              " '[unused253]': 258,\n",
              " '[unused254]': 259,\n",
              " '[unused255]': 260,\n",
              " '[unused256]': 261,\n",
              " '[unused257]': 262,\n",
              " '[unused258]': 263,\n",
              " '[unused259]': 264,\n",
              " '[unused260]': 265,\n",
              " '[unused261]': 266,\n",
              " '[unused262]': 267,\n",
              " '[unused263]': 268,\n",
              " '[unused264]': 269,\n",
              " '[unused265]': 270,\n",
              " '[unused266]': 271,\n",
              " '[unused267]': 272,\n",
              " '[unused268]': 273,\n",
              " '[unused269]': 274,\n",
              " '[unused270]': 275,\n",
              " '[unused271]': 276,\n",
              " '[unused272]': 277,\n",
              " '[unused273]': 278,\n",
              " '[unused274]': 279,\n",
              " '[unused275]': 280,\n",
              " '[unused276]': 281,\n",
              " '[unused277]': 282,\n",
              " '[unused278]': 283,\n",
              " '[unused279]': 284,\n",
              " '[unused280]': 285,\n",
              " '[unused281]': 286,\n",
              " '[unused282]': 287,\n",
              " '[unused283]': 288,\n",
              " '[unused284]': 289,\n",
              " '[unused285]': 290,\n",
              " '[unused286]': 291,\n",
              " '[unused287]': 292,\n",
              " '[unused288]': 293,\n",
              " '[unused289]': 294,\n",
              " '[unused290]': 295,\n",
              " '[unused291]': 296,\n",
              " '[unused292]': 297,\n",
              " '[unused293]': 298,\n",
              " '[unused294]': 299,\n",
              " '[unused295]': 300,\n",
              " '[unused296]': 301,\n",
              " '[unused297]': 302,\n",
              " '[unused298]': 303,\n",
              " '[unused299]': 304,\n",
              " '[unused300]': 305,\n",
              " '[unused301]': 306,\n",
              " '[unused302]': 307,\n",
              " '[unused303]': 308,\n",
              " '[unused304]': 309,\n",
              " '[unused305]': 310,\n",
              " '[unused306]': 311,\n",
              " '[unused307]': 312,\n",
              " '[unused308]': 313,\n",
              " '[unused309]': 314,\n",
              " '[unused310]': 315,\n",
              " '[unused311]': 316,\n",
              " '[unused312]': 317,\n",
              " '[unused313]': 318,\n",
              " '[unused314]': 319,\n",
              " '[unused315]': 320,\n",
              " '[unused316]': 321,\n",
              " '[unused317]': 322,\n",
              " '[unused318]': 323,\n",
              " '[unused319]': 324,\n",
              " '[unused320]': 325,\n",
              " '[unused321]': 326,\n",
              " '[unused322]': 327,\n",
              " '[unused323]': 328,\n",
              " '[unused324]': 329,\n",
              " '[unused325]': 330,\n",
              " '[unused326]': 331,\n",
              " '[unused327]': 332,\n",
              " '[unused328]': 333,\n",
              " '[unused329]': 334,\n",
              " '[unused330]': 335,\n",
              " '[unused331]': 336,\n",
              " '[unused332]': 337,\n",
              " '[unused333]': 338,\n",
              " '[unused334]': 339,\n",
              " '[unused335]': 340,\n",
              " '[unused336]': 341,\n",
              " '[unused337]': 342,\n",
              " '[unused338]': 343,\n",
              " '[unused339]': 344,\n",
              " '[unused340]': 345,\n",
              " '[unused341]': 346,\n",
              " '[unused342]': 347,\n",
              " '[unused343]': 348,\n",
              " '[unused344]': 349,\n",
              " '[unused345]': 350,\n",
              " '[unused346]': 351,\n",
              " '[unused347]': 352,\n",
              " '[unused348]': 353,\n",
              " '[unused349]': 354,\n",
              " '[unused350]': 355,\n",
              " '[unused351]': 356,\n",
              " '[unused352]': 357,\n",
              " '[unused353]': 358,\n",
              " '[unused354]': 359,\n",
              " '[unused355]': 360,\n",
              " '[unused356]': 361,\n",
              " '[unused357]': 362,\n",
              " '[unused358]': 363,\n",
              " '[unused359]': 364,\n",
              " '[unused360]': 365,\n",
              " '[unused361]': 366,\n",
              " '[unused362]': 367,\n",
              " '[unused363]': 368,\n",
              " '[unused364]': 369,\n",
              " '[unused365]': 370,\n",
              " '[unused366]': 371,\n",
              " '[unused367]': 372,\n",
              " '[unused368]': 373,\n",
              " '[unused369]': 374,\n",
              " '[unused370]': 375,\n",
              " '[unused371]': 376,\n",
              " '[unused372]': 377,\n",
              " '[unused373]': 378,\n",
              " '[unused374]': 379,\n",
              " '[unused375]': 380,\n",
              " '[unused376]': 381,\n",
              " '[unused377]': 382,\n",
              " '[unused378]': 383,\n",
              " '[unused379]': 384,\n",
              " '[unused380]': 385,\n",
              " '[unused381]': 386,\n",
              " '[unused382]': 387,\n",
              " '[unused383]': 388,\n",
              " '[unused384]': 389,\n",
              " '[unused385]': 390,\n",
              " '[unused386]': 391,\n",
              " '[unused387]': 392,\n",
              " '[unused388]': 393,\n",
              " '[unused389]': 394,\n",
              " '[unused390]': 395,\n",
              " '[unused391]': 396,\n",
              " '[unused392]': 397,\n",
              " '[unused393]': 398,\n",
              " '[unused394]': 399,\n",
              " '[unused395]': 400,\n",
              " '[unused396]': 401,\n",
              " '[unused397]': 402,\n",
              " '[unused398]': 403,\n",
              " '[unused399]': 404,\n",
              " '[unused400]': 405,\n",
              " '[unused401]': 406,\n",
              " '[unused402]': 407,\n",
              " '[unused403]': 408,\n",
              " '[unused404]': 409,\n",
              " '[unused405]': 410,\n",
              " '[unused406]': 411,\n",
              " '[unused407]': 412,\n",
              " '[unused408]': 413,\n",
              " '[unused409]': 414,\n",
              " '[unused410]': 415,\n",
              " '[unused411]': 416,\n",
              " '[unused412]': 417,\n",
              " '[unused413]': 418,\n",
              " '[unused414]': 419,\n",
              " '[unused415]': 420,\n",
              " '[unused416]': 421,\n",
              " '[unused417]': 422,\n",
              " '[unused418]': 423,\n",
              " '[unused419]': 424,\n",
              " '[unused420]': 425,\n",
              " '[unused421]': 426,\n",
              " '[unused422]': 427,\n",
              " '[unused423]': 428,\n",
              " '[unused424]': 429,\n",
              " '[unused425]': 430,\n",
              " '[unused426]': 431,\n",
              " '[unused427]': 432,\n",
              " '[unused428]': 433,\n",
              " '[unused429]': 434,\n",
              " '[unused430]': 435,\n",
              " '[unused431]': 436,\n",
              " '[unused432]': 437,\n",
              " '[unused433]': 438,\n",
              " '[unused434]': 439,\n",
              " '[unused435]': 440,\n",
              " '[unused436]': 441,\n",
              " '[unused437]': 442,\n",
              " '[unused438]': 443,\n",
              " '[unused439]': 444,\n",
              " '[unused440]': 445,\n",
              " '[unused441]': 446,\n",
              " '[unused442]': 447,\n",
              " '[unused443]': 448,\n",
              " '[unused444]': 449,\n",
              " '[unused445]': 450,\n",
              " '[unused446]': 451,\n",
              " '[unused447]': 452,\n",
              " '[unused448]': 453,\n",
              " '[unused449]': 454,\n",
              " '[unused450]': 455,\n",
              " '[unused451]': 456,\n",
              " '[unused452]': 457,\n",
              " '[unused453]': 458,\n",
              " '[unused454]': 459,\n",
              " '[unused455]': 460,\n",
              " '[unused456]': 461,\n",
              " '[unused457]': 462,\n",
              " '[unused458]': 463,\n",
              " '[unused459]': 464,\n",
              " '[unused460]': 465,\n",
              " '[unused461]': 466,\n",
              " '[unused462]': 467,\n",
              " '[unused463]': 468,\n",
              " '[unused464]': 469,\n",
              " '[unused465]': 470,\n",
              " '[unused466]': 471,\n",
              " '[unused467]': 472,\n",
              " '[unused468]': 473,\n",
              " '[unused469]': 474,\n",
              " '[unused470]': 475,\n",
              " '[unused471]': 476,\n",
              " '[unused472]': 477,\n",
              " '[unused473]': 478,\n",
              " '[unused474]': 479,\n",
              " '[unused475]': 480,\n",
              " '[unused476]': 481,\n",
              " '[unused477]': 482,\n",
              " '[unused478]': 483,\n",
              " '[unused479]': 484,\n",
              " '[unused480]': 485,\n",
              " '[unused481]': 486,\n",
              " '[unused482]': 487,\n",
              " '[unused483]': 488,\n",
              " '[unused484]': 489,\n",
              " '[unused485]': 490,\n",
              " '[unused486]': 491,\n",
              " '[unused487]': 492,\n",
              " '[unused488]': 493,\n",
              " '[unused489]': 494,\n",
              " '[unused490]': 495,\n",
              " '[unused491]': 496,\n",
              " '[unused492]': 497,\n",
              " '[unused493]': 498,\n",
              " '[unused494]': 499,\n",
              " '[unused495]': 500,\n",
              " '[unused496]': 501,\n",
              " '[unused497]': 502,\n",
              " '[unused498]': 503,\n",
              " '[unused499]': 504,\n",
              " '[unused500]': 505,\n",
              " '[unused501]': 506,\n",
              " '[unused502]': 507,\n",
              " '[unused503]': 508,\n",
              " '[unused504]': 509,\n",
              " '[unused505]': 510,\n",
              " '[unused506]': 511,\n",
              " '[unused507]': 512,\n",
              " '[unused508]': 513,\n",
              " '[unused509]': 514,\n",
              " '[unused510]': 515,\n",
              " '[unused511]': 516,\n",
              " '[unused512]': 517,\n",
              " '[unused513]': 518,\n",
              " '[unused514]': 519,\n",
              " '[unused515]': 520,\n",
              " '[unused516]': 521,\n",
              " '[unused517]': 522,\n",
              " '[unused518]': 523,\n",
              " '[unused519]': 524,\n",
              " '[unused520]': 525,\n",
              " '[unused521]': 526,\n",
              " '[unused522]': 527,\n",
              " '[unused523]': 528,\n",
              " '[unused524]': 529,\n",
              " '[unused525]': 530,\n",
              " '[unused526]': 531,\n",
              " '[unused527]': 532,\n",
              " '[unused528]': 533,\n",
              " '[unused529]': 534,\n",
              " '[unused530]': 535,\n",
              " '[unused531]': 536,\n",
              " '[unused532]': 537,\n",
              " '[unused533]': 538,\n",
              " '[unused534]': 539,\n",
              " '[unused535]': 540,\n",
              " '[unused536]': 541,\n",
              " '[unused537]': 542,\n",
              " '[unused538]': 543,\n",
              " '[unused539]': 544,\n",
              " '[unused540]': 545,\n",
              " '[unused541]': 546,\n",
              " '[unused542]': 547,\n",
              " '[unused543]': 548,\n",
              " '[unused544]': 549,\n",
              " '[unused545]': 550,\n",
              " '[unused546]': 551,\n",
              " '[unused547]': 552,\n",
              " '[unused548]': 553,\n",
              " '[unused549]': 554,\n",
              " '[unused550]': 555,\n",
              " '[unused551]': 556,\n",
              " '[unused552]': 557,\n",
              " '[unused553]': 558,\n",
              " '[unused554]': 559,\n",
              " '[unused555]': 560,\n",
              " '[unused556]': 561,\n",
              " '[unused557]': 562,\n",
              " '[unused558]': 563,\n",
              " '[unused559]': 564,\n",
              " '[unused560]': 565,\n",
              " '[unused561]': 566,\n",
              " '[unused562]': 567,\n",
              " '[unused563]': 568,\n",
              " '[unused564]': 569,\n",
              " '[unused565]': 570,\n",
              " '[unused566]': 571,\n",
              " '[unused567]': 572,\n",
              " '[unused568]': 573,\n",
              " '[unused569]': 574,\n",
              " '[unused570]': 575,\n",
              " '[unused571]': 576,\n",
              " '[unused572]': 577,\n",
              " '[unused573]': 578,\n",
              " '[unused574]': 579,\n",
              " '[unused575]': 580,\n",
              " '[unused576]': 581,\n",
              " '[unused577]': 582,\n",
              " '[unused578]': 583,\n",
              " '[unused579]': 584,\n",
              " '[unused580]': 585,\n",
              " '[unused581]': 586,\n",
              " '[unused582]': 587,\n",
              " '[unused583]': 588,\n",
              " '[unused584]': 589,\n",
              " '[unused585]': 590,\n",
              " '[unused586]': 591,\n",
              " '[unused587]': 592,\n",
              " '[unused588]': 593,\n",
              " '[unused589]': 594,\n",
              " '[unused590]': 595,\n",
              " '[unused591]': 596,\n",
              " '[unused592]': 597,\n",
              " '[unused593]': 598,\n",
              " '[unused594]': 599,\n",
              " '[unused595]': 600,\n",
              " '[unused596]': 601,\n",
              " '[unused597]': 602,\n",
              " '[unused598]': 603,\n",
              " '[unused599]': 604,\n",
              " '[unused600]': 605,\n",
              " '[unused601]': 606,\n",
              " '[unused602]': 607,\n",
              " '[unused603]': 608,\n",
              " '[unused604]': 609,\n",
              " '[unused605]': 610,\n",
              " '[unused606]': 611,\n",
              " '[unused607]': 612,\n",
              " '[unused608]': 613,\n",
              " '[unused609]': 614,\n",
              " '[unused610]': 615,\n",
              " '[unused611]': 616,\n",
              " '[unused612]': 617,\n",
              " '[unused613]': 618,\n",
              " '[unused614]': 619,\n",
              " '[unused615]': 620,\n",
              " '[unused616]': 621,\n",
              " '[unused617]': 622,\n",
              " '[unused618]': 623,\n",
              " '[unused619]': 624,\n",
              " '[unused620]': 625,\n",
              " '[unused621]': 626,\n",
              " '[unused622]': 627,\n",
              " '[unused623]': 628,\n",
              " '[unused624]': 629,\n",
              " '[unused625]': 630,\n",
              " '[unused626]': 631,\n",
              " '[unused627]': 632,\n",
              " '[unused628]': 633,\n",
              " '[unused629]': 634,\n",
              " '[unused630]': 635,\n",
              " '[unused631]': 636,\n",
              " '[unused632]': 637,\n",
              " '[unused633]': 638,\n",
              " '[unused634]': 639,\n",
              " '[unused635]': 640,\n",
              " '[unused636]': 641,\n",
              " '[unused637]': 642,\n",
              " '[unused638]': 643,\n",
              " '[unused639]': 644,\n",
              " '[unused640]': 645,\n",
              " '[unused641]': 646,\n",
              " '[unused642]': 647,\n",
              " '[unused643]': 648,\n",
              " '[unused644]': 649,\n",
              " '[unused645]': 650,\n",
              " '[unused646]': 651,\n",
              " '[unused647]': 652,\n",
              " '[unused648]': 653,\n",
              " '[unused649]': 654,\n",
              " '[unused650]': 655,\n",
              " '[unused651]': 656,\n",
              " '[unused652]': 657,\n",
              " '[unused653]': 658,\n",
              " '[unused654]': 659,\n",
              " '[unused655]': 660,\n",
              " '[unused656]': 661,\n",
              " '[unused657]': 662,\n",
              " '[unused658]': 663,\n",
              " '[unused659]': 664,\n",
              " '[unused660]': 665,\n",
              " '[unused661]': 666,\n",
              " '[unused662]': 667,\n",
              " '[unused663]': 668,\n",
              " '[unused664]': 669,\n",
              " '[unused665]': 670,\n",
              " '[unused666]': 671,\n",
              " '[unused667]': 672,\n",
              " '[unused668]': 673,\n",
              " '[unused669]': 674,\n",
              " '[unused670]': 675,\n",
              " '[unused671]': 676,\n",
              " '[unused672]': 677,\n",
              " '[unused673]': 678,\n",
              " '[unused674]': 679,\n",
              " '[unused675]': 680,\n",
              " '[unused676]': 681,\n",
              " '[unused677]': 682,\n",
              " '[unused678]': 683,\n",
              " '[unused679]': 684,\n",
              " '[unused680]': 685,\n",
              " '[unused681]': 686,\n",
              " '[unused682]': 687,\n",
              " '[unused683]': 688,\n",
              " '[unused684]': 689,\n",
              " '[unused685]': 690,\n",
              " '[unused686]': 691,\n",
              " '[unused687]': 692,\n",
              " '[unused688]': 693,\n",
              " '[unused689]': 694,\n",
              " '[unused690]': 695,\n",
              " '[unused691]': 696,\n",
              " '[unused692]': 697,\n",
              " '[unused693]': 698,\n",
              " '[unused694]': 699,\n",
              " '[unused695]': 700,\n",
              " '[unused696]': 701,\n",
              " '[unused697]': 702,\n",
              " '[unused698]': 703,\n",
              " '[unused699]': 704,\n",
              " '[unused700]': 705,\n",
              " '[unused701]': 706,\n",
              " '[unused702]': 707,\n",
              " '[unused703]': 708,\n",
              " '[unused704]': 709,\n",
              " '[unused705]': 710,\n",
              " '[unused706]': 711,\n",
              " '[unused707]': 712,\n",
              " '[unused708]': 713,\n",
              " '[unused709]': 714,\n",
              " '[unused710]': 715,\n",
              " '[unused711]': 716,\n",
              " '[unused712]': 717,\n",
              " '[unused713]': 718,\n",
              " '[unused714]': 719,\n",
              " '[unused715]': 720,\n",
              " '[unused716]': 721,\n",
              " '[unused717]': 722,\n",
              " '[unused718]': 723,\n",
              " '[unused719]': 724,\n",
              " '[unused720]': 725,\n",
              " '[unused721]': 726,\n",
              " '[unused722]': 727,\n",
              " '[unused723]': 728,\n",
              " '[unused724]': 729,\n",
              " '[unused725]': 730,\n",
              " '[unused726]': 731,\n",
              " '[unused727]': 732,\n",
              " '[unused728]': 733,\n",
              " '[unused729]': 734,\n",
              " '[unused730]': 735,\n",
              " '[unused731]': 736,\n",
              " '[unused732]': 737,\n",
              " '[unused733]': 738,\n",
              " '[unused734]': 739,\n",
              " '[unused735]': 740,\n",
              " '[unused736]': 741,\n",
              " '[unused737]': 742,\n",
              " '[unused738]': 743,\n",
              " '[unused739]': 744,\n",
              " '[unused740]': 745,\n",
              " '[unused741]': 746,\n",
              " '[unused742]': 747,\n",
              " '[unused743]': 748,\n",
              " '[unused744]': 749,\n",
              " '[unused745]': 750,\n",
              " '[unused746]': 751,\n",
              " '[unused747]': 752,\n",
              " '[unused748]': 753,\n",
              " '[unused749]': 754,\n",
              " '[unused750]': 755,\n",
              " '[unused751]': 756,\n",
              " '[unused752]': 757,\n",
              " '[unused753]': 758,\n",
              " '[unused754]': 759,\n",
              " '[unused755]': 760,\n",
              " '[unused756]': 761,\n",
              " '[unused757]': 762,\n",
              " '[unused758]': 763,\n",
              " '[unused759]': 764,\n",
              " '[unused760]': 765,\n",
              " '[unused761]': 766,\n",
              " '[unused762]': 767,\n",
              " '[unused763]': 768,\n",
              " '[unused764]': 769,\n",
              " '[unused765]': 770,\n",
              " '[unused766]': 771,\n",
              " '[unused767]': 772,\n",
              " '[unused768]': 773,\n",
              " '[unused769]': 774,\n",
              " '[unused770]': 775,\n",
              " '[unused771]': 776,\n",
              " '[unused772]': 777,\n",
              " '[unused773]': 778,\n",
              " '[unused774]': 779,\n",
              " '[unused775]': 780,\n",
              " '[unused776]': 781,\n",
              " '[unused777]': 782,\n",
              " '[unused778]': 783,\n",
              " '[unused779]': 784,\n",
              " '[unused780]': 785,\n",
              " '[unused781]': 786,\n",
              " '[unused782]': 787,\n",
              " '[unused783]': 788,\n",
              " '[unused784]': 789,\n",
              " '[unused785]': 790,\n",
              " '[unused786]': 791,\n",
              " '[unused787]': 792,\n",
              " '[unused788]': 793,\n",
              " '[unused789]': 794,\n",
              " '[unused790]': 795,\n",
              " '[unused791]': 796,\n",
              " '[unused792]': 797,\n",
              " '[unused793]': 798,\n",
              " '[unused794]': 799,\n",
              " '[unused795]': 800,\n",
              " '[unused796]': 801,\n",
              " '[unused797]': 802,\n",
              " '[unused798]': 803,\n",
              " '[unused799]': 804,\n",
              " '[unused800]': 805,\n",
              " '[unused801]': 806,\n",
              " '[unused802]': 807,\n",
              " '[unused803]': 808,\n",
              " '[unused804]': 809,\n",
              " '[unused805]': 810,\n",
              " '[unused806]': 811,\n",
              " '[unused807]': 812,\n",
              " '[unused808]': 813,\n",
              " '[unused809]': 814,\n",
              " '[unused810]': 815,\n",
              " '[unused811]': 816,\n",
              " '[unused812]': 817,\n",
              " '[unused813]': 818,\n",
              " '[unused814]': 819,\n",
              " '[unused815]': 820,\n",
              " '[unused816]': 821,\n",
              " '[unused817]': 822,\n",
              " '[unused818]': 823,\n",
              " '[unused819]': 824,\n",
              " '[unused820]': 825,\n",
              " '[unused821]': 826,\n",
              " '[unused822]': 827,\n",
              " '[unused823]': 828,\n",
              " '[unused824]': 829,\n",
              " '[unused825]': 830,\n",
              " '[unused826]': 831,\n",
              " '[unused827]': 832,\n",
              " '[unused828]': 833,\n",
              " '[unused829]': 834,\n",
              " '[unused830]': 835,\n",
              " '[unused831]': 836,\n",
              " '[unused832]': 837,\n",
              " '[unused833]': 838,\n",
              " '[unused834]': 839,\n",
              " '[unused835]': 840,\n",
              " '[unused836]': 841,\n",
              " '[unused837]': 842,\n",
              " '[unused838]': 843,\n",
              " '[unused839]': 844,\n",
              " '[unused840]': 845,\n",
              " '[unused841]': 846,\n",
              " '[unused842]': 847,\n",
              " '[unused843]': 848,\n",
              " '[unused844]': 849,\n",
              " '[unused845]': 850,\n",
              " '[unused846]': 851,\n",
              " '[unused847]': 852,\n",
              " '[unused848]': 853,\n",
              " '[unused849]': 854,\n",
              " '[unused850]': 855,\n",
              " '[unused851]': 856,\n",
              " '[unused852]': 857,\n",
              " '[unused853]': 858,\n",
              " '[unused854]': 859,\n",
              " '[unused855]': 860,\n",
              " '[unused856]': 861,\n",
              " '[unused857]': 862,\n",
              " '[unused858]': 863,\n",
              " '[unused859]': 864,\n",
              " '[unused860]': 865,\n",
              " '[unused861]': 866,\n",
              " '[unused862]': 867,\n",
              " '[unused863]': 868,\n",
              " '[unused864]': 869,\n",
              " '[unused865]': 870,\n",
              " '[unused866]': 871,\n",
              " '[unused867]': 872,\n",
              " '[unused868]': 873,\n",
              " '[unused869]': 874,\n",
              " '[unused870]': 875,\n",
              " '[unused871]': 876,\n",
              " '[unused872]': 877,\n",
              " '[unused873]': 878,\n",
              " '[unused874]': 879,\n",
              " '[unused875]': 880,\n",
              " '[unused876]': 881,\n",
              " '[unused877]': 882,\n",
              " '[unused878]': 883,\n",
              " '[unused879]': 884,\n",
              " '[unused880]': 885,\n",
              " '[unused881]': 886,\n",
              " '[unused882]': 887,\n",
              " '[unused883]': 888,\n",
              " '[unused884]': 889,\n",
              " '[unused885]': 890,\n",
              " '[unused886]': 891,\n",
              " '[unused887]': 892,\n",
              " '[unused888]': 893,\n",
              " '[unused889]': 894,\n",
              " '[unused890]': 895,\n",
              " '[unused891]': 896,\n",
              " '[unused892]': 897,\n",
              " '[unused893]': 898,\n",
              " '[unused894]': 899,\n",
              " '[unused895]': 900,\n",
              " '[unused896]': 901,\n",
              " '[unused897]': 902,\n",
              " '[unused898]': 903,\n",
              " '[unused899]': 904,\n",
              " '[unused900]': 905,\n",
              " '[unused901]': 906,\n",
              " '[unused902]': 907,\n",
              " '[unused903]': 908,\n",
              " '[unused904]': 909,\n",
              " '[unused905]': 910,\n",
              " '[unused906]': 911,\n",
              " '[unused907]': 912,\n",
              " '[unused908]': 913,\n",
              " '[unused909]': 914,\n",
              " '[unused910]': 915,\n",
              " '[unused911]': 916,\n",
              " '[unused912]': 917,\n",
              " '[unused913]': 918,\n",
              " '[unused914]': 919,\n",
              " '[unused915]': 920,\n",
              " '[unused916]': 921,\n",
              " '[unused917]': 922,\n",
              " '[unused918]': 923,\n",
              " '[unused919]': 924,\n",
              " '[unused920]': 925,\n",
              " '[unused921]': 926,\n",
              " '[unused922]': 927,\n",
              " '[unused923]': 928,\n",
              " '[unused924]': 929,\n",
              " '[unused925]': 930,\n",
              " '[unused926]': 931,\n",
              " '[unused927]': 932,\n",
              " '[unused928]': 933,\n",
              " '[unused929]': 934,\n",
              " '[unused930]': 935,\n",
              " '[unused931]': 936,\n",
              " '[unused932]': 937,\n",
              " '[unused933]': 938,\n",
              " '[unused934]': 939,\n",
              " '[unused935]': 940,\n",
              " '[unused936]': 941,\n",
              " '[unused937]': 942,\n",
              " '[unused938]': 943,\n",
              " '[unused939]': 944,\n",
              " '[unused940]': 945,\n",
              " '[unused941]': 946,\n",
              " '[unused942]': 947,\n",
              " '[unused943]': 948,\n",
              " '[unused944]': 949,\n",
              " '[unused945]': 950,\n",
              " '[unused946]': 951,\n",
              " '[unused947]': 952,\n",
              " '[unused948]': 953,\n",
              " '[unused949]': 954,\n",
              " '[unused950]': 955,\n",
              " '[unused951]': 956,\n",
              " '[unused952]': 957,\n",
              " '[unused953]': 958,\n",
              " '[unused954]': 959,\n",
              " '[unused955]': 960,\n",
              " '[unused956]': 961,\n",
              " '[unused957]': 962,\n",
              " '[unused958]': 963,\n",
              " '[unused959]': 964,\n",
              " '[unused960]': 965,\n",
              " '[unused961]': 966,\n",
              " '[unused962]': 967,\n",
              " '[unused963]': 968,\n",
              " '[unused964]': 969,\n",
              " '[unused965]': 970,\n",
              " '[unused966]': 971,\n",
              " '[unused967]': 972,\n",
              " '[unused968]': 973,\n",
              " '[unused969]': 974,\n",
              " '[unused970]': 975,\n",
              " '[unused971]': 976,\n",
              " '[unused972]': 977,\n",
              " '[unused973]': 978,\n",
              " '[unused974]': 979,\n",
              " '[unused975]': 980,\n",
              " '[unused976]': 981,\n",
              " '[unused977]': 982,\n",
              " '[unused978]': 983,\n",
              " '[unused979]': 984,\n",
              " '[unused980]': 985,\n",
              " '[unused981]': 986,\n",
              " '[unused982]': 987,\n",
              " '[unused983]': 988,\n",
              " '[unused984]': 989,\n",
              " '[unused985]': 990,\n",
              " '[unused986]': 991,\n",
              " '[unused987]': 992,\n",
              " '[unused988]': 993,\n",
              " '[unused989]': 994,\n",
              " '[unused990]': 995,\n",
              " '[unused991]': 996,\n",
              " '[unused992]': 997,\n",
              " '[unused993]': 998,\n",
              " '!': 999,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer Construction\n",
        "tokenizer = Tokenizer(token_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "7-Jy3ZukLDNf",
        "outputId": "263d3a94-509f-45ce-bd16-49eb9aa6fc1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if the tokenizer works well\n",
        "print(tokenizer.tokenize(\"Youngsun is really awesome.\"))\n",
        "print(tokenizer.tokenize(\"We can manipulate AI.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "id": "ogXL1ScjLDK_",
        "outputId": "0b58d3ce-c7da-4839-9491-4590f9ef6fd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'young', '##sun', 'is', 'really', 'awesome', '.', '[SEP]']\n",
            "['[CLS]', 'we', 'can', 'manipulate', 'ai', '.', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For later use, updated dictionary constructed (number - word order)\n",
        "reverse_token_dict = {v : k for k, v in token_dict.items()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "_niyf0VxLDIT",
        "outputId": "22a92dde-48fd-4a96-83e2-ea6a3f30da3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_token_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qo7YX6mELDF2",
        "outputId": "e92a75e8-15fb-4a7f-ac10-1ecf2b96a160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '[PAD]',\n",
              " 1: '[unused0]',\n",
              " 2: '[unused1]',\n",
              " 3: '[unused2]',\n",
              " 4: '[unused3]',\n",
              " 5: '[unused4]',\n",
              " 6: '[unused5]',\n",
              " 7: '[unused6]',\n",
              " 8: '[unused7]',\n",
              " 9: '[unused8]',\n",
              " 10: '[unused9]',\n",
              " 11: '[unused10]',\n",
              " 12: '[unused11]',\n",
              " 13: '[unused12]',\n",
              " 14: '[unused13]',\n",
              " 15: '[unused14]',\n",
              " 16: '[unused15]',\n",
              " 17: '[unused16]',\n",
              " 18: '[unused17]',\n",
              " 19: '[unused18]',\n",
              " 20: '[unused19]',\n",
              " 21: '[unused20]',\n",
              " 22: '[unused21]',\n",
              " 23: '[unused22]',\n",
              " 24: '[unused23]',\n",
              " 25: '[unused24]',\n",
              " 26: '[unused25]',\n",
              " 27: '[unused26]',\n",
              " 28: '[unused27]',\n",
              " 29: '[unused28]',\n",
              " 30: '[unused29]',\n",
              " 31: '[unused30]',\n",
              " 32: '[unused31]',\n",
              " 33: '[unused32]',\n",
              " 34: '[unused33]',\n",
              " 35: '[unused34]',\n",
              " 36: '[unused35]',\n",
              " 37: '[unused36]',\n",
              " 38: '[unused37]',\n",
              " 39: '[unused38]',\n",
              " 40: '[unused39]',\n",
              " 41: '[unused40]',\n",
              " 42: '[unused41]',\n",
              " 43: '[unused42]',\n",
              " 44: '[unused43]',\n",
              " 45: '[unused44]',\n",
              " 46: '[unused45]',\n",
              " 47: '[unused46]',\n",
              " 48: '[unused47]',\n",
              " 49: '[unused48]',\n",
              " 50: '[unused49]',\n",
              " 51: '[unused50]',\n",
              " 52: '[unused51]',\n",
              " 53: '[unused52]',\n",
              " 54: '[unused53]',\n",
              " 55: '[unused54]',\n",
              " 56: '[unused55]',\n",
              " 57: '[unused56]',\n",
              " 58: '[unused57]',\n",
              " 59: '[unused58]',\n",
              " 60: '[unused59]',\n",
              " 61: '[unused60]',\n",
              " 62: '[unused61]',\n",
              " 63: '[unused62]',\n",
              " 64: '[unused63]',\n",
              " 65: '[unused64]',\n",
              " 66: '[unused65]',\n",
              " 67: '[unused66]',\n",
              " 68: '[unused67]',\n",
              " 69: '[unused68]',\n",
              " 70: '[unused69]',\n",
              " 71: '[unused70]',\n",
              " 72: '[unused71]',\n",
              " 73: '[unused72]',\n",
              " 74: '[unused73]',\n",
              " 75: '[unused74]',\n",
              " 76: '[unused75]',\n",
              " 77: '[unused76]',\n",
              " 78: '[unused77]',\n",
              " 79: '[unused78]',\n",
              " 80: '[unused79]',\n",
              " 81: '[unused80]',\n",
              " 82: '[unused81]',\n",
              " 83: '[unused82]',\n",
              " 84: '[unused83]',\n",
              " 85: '[unused84]',\n",
              " 86: '[unused85]',\n",
              " 87: '[unused86]',\n",
              " 88: '[unused87]',\n",
              " 89: '[unused88]',\n",
              " 90: '[unused89]',\n",
              " 91: '[unused90]',\n",
              " 92: '[unused91]',\n",
              " 93: '[unused92]',\n",
              " 94: '[unused93]',\n",
              " 95: '[unused94]',\n",
              " 96: '[unused95]',\n",
              " 97: '[unused96]',\n",
              " 98: '[unused97]',\n",
              " 99: '[unused98]',\n",
              " 100: '[UNK]',\n",
              " 101: '[CLS]',\n",
              " 102: '[SEP]',\n",
              " 103: '[MASK]',\n",
              " 104: '[unused99]',\n",
              " 105: '[unused100]',\n",
              " 106: '[unused101]',\n",
              " 107: '[unused102]',\n",
              " 108: '[unused103]',\n",
              " 109: '[unused104]',\n",
              " 110: '[unused105]',\n",
              " 111: '[unused106]',\n",
              " 112: '[unused107]',\n",
              " 113: '[unused108]',\n",
              " 114: '[unused109]',\n",
              " 115: '[unused110]',\n",
              " 116: '[unused111]',\n",
              " 117: '[unused112]',\n",
              " 118: '[unused113]',\n",
              " 119: '[unused114]',\n",
              " 120: '[unused115]',\n",
              " 121: '[unused116]',\n",
              " 122: '[unused117]',\n",
              " 123: '[unused118]',\n",
              " 124: '[unused119]',\n",
              " 125: '[unused120]',\n",
              " 126: '[unused121]',\n",
              " 127: '[unused122]',\n",
              " 128: '[unused123]',\n",
              " 129: '[unused124]',\n",
              " 130: '[unused125]',\n",
              " 131: '[unused126]',\n",
              " 132: '[unused127]',\n",
              " 133: '[unused128]',\n",
              " 134: '[unused129]',\n",
              " 135: '[unused130]',\n",
              " 136: '[unused131]',\n",
              " 137: '[unused132]',\n",
              " 138: '[unused133]',\n",
              " 139: '[unused134]',\n",
              " 140: '[unused135]',\n",
              " 141: '[unused136]',\n",
              " 142: '[unused137]',\n",
              " 143: '[unused138]',\n",
              " 144: '[unused139]',\n",
              " 145: '[unused140]',\n",
              " 146: '[unused141]',\n",
              " 147: '[unused142]',\n",
              " 148: '[unused143]',\n",
              " 149: '[unused144]',\n",
              " 150: '[unused145]',\n",
              " 151: '[unused146]',\n",
              " 152: '[unused147]',\n",
              " 153: '[unused148]',\n",
              " 154: '[unused149]',\n",
              " 155: '[unused150]',\n",
              " 156: '[unused151]',\n",
              " 157: '[unused152]',\n",
              " 158: '[unused153]',\n",
              " 159: '[unused154]',\n",
              " 160: '[unused155]',\n",
              " 161: '[unused156]',\n",
              " 162: '[unused157]',\n",
              " 163: '[unused158]',\n",
              " 164: '[unused159]',\n",
              " 165: '[unused160]',\n",
              " 166: '[unused161]',\n",
              " 167: '[unused162]',\n",
              " 168: '[unused163]',\n",
              " 169: '[unused164]',\n",
              " 170: '[unused165]',\n",
              " 171: '[unused166]',\n",
              " 172: '[unused167]',\n",
              " 173: '[unused168]',\n",
              " 174: '[unused169]',\n",
              " 175: '[unused170]',\n",
              " 176: '[unused171]',\n",
              " 177: '[unused172]',\n",
              " 178: '[unused173]',\n",
              " 179: '[unused174]',\n",
              " 180: '[unused175]',\n",
              " 181: '[unused176]',\n",
              " 182: '[unused177]',\n",
              " 183: '[unused178]',\n",
              " 184: '[unused179]',\n",
              " 185: '[unused180]',\n",
              " 186: '[unused181]',\n",
              " 187: '[unused182]',\n",
              " 188: '[unused183]',\n",
              " 189: '[unused184]',\n",
              " 190: '[unused185]',\n",
              " 191: '[unused186]',\n",
              " 192: '[unused187]',\n",
              " 193: '[unused188]',\n",
              " 194: '[unused189]',\n",
              " 195: '[unused190]',\n",
              " 196: '[unused191]',\n",
              " 197: '[unused192]',\n",
              " 198: '[unused193]',\n",
              " 199: '[unused194]',\n",
              " 200: '[unused195]',\n",
              " 201: '[unused196]',\n",
              " 202: '[unused197]',\n",
              " 203: '[unused198]',\n",
              " 204: '[unused199]',\n",
              " 205: '[unused200]',\n",
              " 206: '[unused201]',\n",
              " 207: '[unused202]',\n",
              " 208: '[unused203]',\n",
              " 209: '[unused204]',\n",
              " 210: '[unused205]',\n",
              " 211: '[unused206]',\n",
              " 212: '[unused207]',\n",
              " 213: '[unused208]',\n",
              " 214: '[unused209]',\n",
              " 215: '[unused210]',\n",
              " 216: '[unused211]',\n",
              " 217: '[unused212]',\n",
              " 218: '[unused213]',\n",
              " 219: '[unused214]',\n",
              " 220: '[unused215]',\n",
              " 221: '[unused216]',\n",
              " 222: '[unused217]',\n",
              " 223: '[unused218]',\n",
              " 224: '[unused219]',\n",
              " 225: '[unused220]',\n",
              " 226: '[unused221]',\n",
              " 227: '[unused222]',\n",
              " 228: '[unused223]',\n",
              " 229: '[unused224]',\n",
              " 230: '[unused225]',\n",
              " 231: '[unused226]',\n",
              " 232: '[unused227]',\n",
              " 233: '[unused228]',\n",
              " 234: '[unused229]',\n",
              " 235: '[unused230]',\n",
              " 236: '[unused231]',\n",
              " 237: '[unused232]',\n",
              " 238: '[unused233]',\n",
              " 239: '[unused234]',\n",
              " 240: '[unused235]',\n",
              " 241: '[unused236]',\n",
              " 242: '[unused237]',\n",
              " 243: '[unused238]',\n",
              " 244: '[unused239]',\n",
              " 245: '[unused240]',\n",
              " 246: '[unused241]',\n",
              " 247: '[unused242]',\n",
              " 248: '[unused243]',\n",
              " 249: '[unused244]',\n",
              " 250: '[unused245]',\n",
              " 251: '[unused246]',\n",
              " 252: '[unused247]',\n",
              " 253: '[unused248]',\n",
              " 254: '[unused249]',\n",
              " 255: '[unused250]',\n",
              " 256: '[unused251]',\n",
              " 257: '[unused252]',\n",
              " 258: '[unused253]',\n",
              " 259: '[unused254]',\n",
              " 260: '[unused255]',\n",
              " 261: '[unused256]',\n",
              " 262: '[unused257]',\n",
              " 263: '[unused258]',\n",
              " 264: '[unused259]',\n",
              " 265: '[unused260]',\n",
              " 266: '[unused261]',\n",
              " 267: '[unused262]',\n",
              " 268: '[unused263]',\n",
              " 269: '[unused264]',\n",
              " 270: '[unused265]',\n",
              " 271: '[unused266]',\n",
              " 272: '[unused267]',\n",
              " 273: '[unused268]',\n",
              " 274: '[unused269]',\n",
              " 275: '[unused270]',\n",
              " 276: '[unused271]',\n",
              " 277: '[unused272]',\n",
              " 278: '[unused273]',\n",
              " 279: '[unused274]',\n",
              " 280: '[unused275]',\n",
              " 281: '[unused276]',\n",
              " 282: '[unused277]',\n",
              " 283: '[unused278]',\n",
              " 284: '[unused279]',\n",
              " 285: '[unused280]',\n",
              " 286: '[unused281]',\n",
              " 287: '[unused282]',\n",
              " 288: '[unused283]',\n",
              " 289: '[unused284]',\n",
              " 290: '[unused285]',\n",
              " 291: '[unused286]',\n",
              " 292: '[unused287]',\n",
              " 293: '[unused288]',\n",
              " 294: '[unused289]',\n",
              " 295: '[unused290]',\n",
              " 296: '[unused291]',\n",
              " 297: '[unused292]',\n",
              " 298: '[unused293]',\n",
              " 299: '[unused294]',\n",
              " 300: '[unused295]',\n",
              " 301: '[unused296]',\n",
              " 302: '[unused297]',\n",
              " 303: '[unused298]',\n",
              " 304: '[unused299]',\n",
              " 305: '[unused300]',\n",
              " 306: '[unused301]',\n",
              " 307: '[unused302]',\n",
              " 308: '[unused303]',\n",
              " 309: '[unused304]',\n",
              " 310: '[unused305]',\n",
              " 311: '[unused306]',\n",
              " 312: '[unused307]',\n",
              " 313: '[unused308]',\n",
              " 314: '[unused309]',\n",
              " 315: '[unused310]',\n",
              " 316: '[unused311]',\n",
              " 317: '[unused312]',\n",
              " 318: '[unused313]',\n",
              " 319: '[unused314]',\n",
              " 320: '[unused315]',\n",
              " 321: '[unused316]',\n",
              " 322: '[unused317]',\n",
              " 323: '[unused318]',\n",
              " 324: '[unused319]',\n",
              " 325: '[unused320]',\n",
              " 326: '[unused321]',\n",
              " 327: '[unused322]',\n",
              " 328: '[unused323]',\n",
              " 329: '[unused324]',\n",
              " 330: '[unused325]',\n",
              " 331: '[unused326]',\n",
              " 332: '[unused327]',\n",
              " 333: '[unused328]',\n",
              " 334: '[unused329]',\n",
              " 335: '[unused330]',\n",
              " 336: '[unused331]',\n",
              " 337: '[unused332]',\n",
              " 338: '[unused333]',\n",
              " 339: '[unused334]',\n",
              " 340: '[unused335]',\n",
              " 341: '[unused336]',\n",
              " 342: '[unused337]',\n",
              " 343: '[unused338]',\n",
              " 344: '[unused339]',\n",
              " 345: '[unused340]',\n",
              " 346: '[unused341]',\n",
              " 347: '[unused342]',\n",
              " 348: '[unused343]',\n",
              " 349: '[unused344]',\n",
              " 350: '[unused345]',\n",
              " 351: '[unused346]',\n",
              " 352: '[unused347]',\n",
              " 353: '[unused348]',\n",
              " 354: '[unused349]',\n",
              " 355: '[unused350]',\n",
              " 356: '[unused351]',\n",
              " 357: '[unused352]',\n",
              " 358: '[unused353]',\n",
              " 359: '[unused354]',\n",
              " 360: '[unused355]',\n",
              " 361: '[unused356]',\n",
              " 362: '[unused357]',\n",
              " 363: '[unused358]',\n",
              " 364: '[unused359]',\n",
              " 365: '[unused360]',\n",
              " 366: '[unused361]',\n",
              " 367: '[unused362]',\n",
              " 368: '[unused363]',\n",
              " 369: '[unused364]',\n",
              " 370: '[unused365]',\n",
              " 371: '[unused366]',\n",
              " 372: '[unused367]',\n",
              " 373: '[unused368]',\n",
              " 374: '[unused369]',\n",
              " 375: '[unused370]',\n",
              " 376: '[unused371]',\n",
              " 377: '[unused372]',\n",
              " 378: '[unused373]',\n",
              " 379: '[unused374]',\n",
              " 380: '[unused375]',\n",
              " 381: '[unused376]',\n",
              " 382: '[unused377]',\n",
              " 383: '[unused378]',\n",
              " 384: '[unused379]',\n",
              " 385: '[unused380]',\n",
              " 386: '[unused381]',\n",
              " 387: '[unused382]',\n",
              " 388: '[unused383]',\n",
              " 389: '[unused384]',\n",
              " 390: '[unused385]',\n",
              " 391: '[unused386]',\n",
              " 392: '[unused387]',\n",
              " 393: '[unused388]',\n",
              " 394: '[unused389]',\n",
              " 395: '[unused390]',\n",
              " 396: '[unused391]',\n",
              " 397: '[unused392]',\n",
              " 398: '[unused393]',\n",
              " 399: '[unused394]',\n",
              " 400: '[unused395]',\n",
              " 401: '[unused396]',\n",
              " 402: '[unused397]',\n",
              " 403: '[unused398]',\n",
              " 404: '[unused399]',\n",
              " 405: '[unused400]',\n",
              " 406: '[unused401]',\n",
              " 407: '[unused402]',\n",
              " 408: '[unused403]',\n",
              " 409: '[unused404]',\n",
              " 410: '[unused405]',\n",
              " 411: '[unused406]',\n",
              " 412: '[unused407]',\n",
              " 413: '[unused408]',\n",
              " 414: '[unused409]',\n",
              " 415: '[unused410]',\n",
              " 416: '[unused411]',\n",
              " 417: '[unused412]',\n",
              " 418: '[unused413]',\n",
              " 419: '[unused414]',\n",
              " 420: '[unused415]',\n",
              " 421: '[unused416]',\n",
              " 422: '[unused417]',\n",
              " 423: '[unused418]',\n",
              " 424: '[unused419]',\n",
              " 425: '[unused420]',\n",
              " 426: '[unused421]',\n",
              " 427: '[unused422]',\n",
              " 428: '[unused423]',\n",
              " 429: '[unused424]',\n",
              " 430: '[unused425]',\n",
              " 431: '[unused426]',\n",
              " 432: '[unused427]',\n",
              " 433: '[unused428]',\n",
              " 434: '[unused429]',\n",
              " 435: '[unused430]',\n",
              " 436: '[unused431]',\n",
              " 437: '[unused432]',\n",
              " 438: '[unused433]',\n",
              " 439: '[unused434]',\n",
              " 440: '[unused435]',\n",
              " 441: '[unused436]',\n",
              " 442: '[unused437]',\n",
              " 443: '[unused438]',\n",
              " 444: '[unused439]',\n",
              " 445: '[unused440]',\n",
              " 446: '[unused441]',\n",
              " 447: '[unused442]',\n",
              " 448: '[unused443]',\n",
              " 449: '[unused444]',\n",
              " 450: '[unused445]',\n",
              " 451: '[unused446]',\n",
              " 452: '[unused447]',\n",
              " 453: '[unused448]',\n",
              " 454: '[unused449]',\n",
              " 455: '[unused450]',\n",
              " 456: '[unused451]',\n",
              " 457: '[unused452]',\n",
              " 458: '[unused453]',\n",
              " 459: '[unused454]',\n",
              " 460: '[unused455]',\n",
              " 461: '[unused456]',\n",
              " 462: '[unused457]',\n",
              " 463: '[unused458]',\n",
              " 464: '[unused459]',\n",
              " 465: '[unused460]',\n",
              " 466: '[unused461]',\n",
              " 467: '[unused462]',\n",
              " 468: '[unused463]',\n",
              " 469: '[unused464]',\n",
              " 470: '[unused465]',\n",
              " 471: '[unused466]',\n",
              " 472: '[unused467]',\n",
              " 473: '[unused468]',\n",
              " 474: '[unused469]',\n",
              " 475: '[unused470]',\n",
              " 476: '[unused471]',\n",
              " 477: '[unused472]',\n",
              " 478: '[unused473]',\n",
              " 479: '[unused474]',\n",
              " 480: '[unused475]',\n",
              " 481: '[unused476]',\n",
              " 482: '[unused477]',\n",
              " 483: '[unused478]',\n",
              " 484: '[unused479]',\n",
              " 485: '[unused480]',\n",
              " 486: '[unused481]',\n",
              " 487: '[unused482]',\n",
              " 488: '[unused483]',\n",
              " 489: '[unused484]',\n",
              " 490: '[unused485]',\n",
              " 491: '[unused486]',\n",
              " 492: '[unused487]',\n",
              " 493: '[unused488]',\n",
              " 494: '[unused489]',\n",
              " 495: '[unused490]',\n",
              " 496: '[unused491]',\n",
              " 497: '[unused492]',\n",
              " 498: '[unused493]',\n",
              " 499: '[unused494]',\n",
              " 500: '[unused495]',\n",
              " 501: '[unused496]',\n",
              " 502: '[unused497]',\n",
              " 503: '[unused498]',\n",
              " 504: '[unused499]',\n",
              " 505: '[unused500]',\n",
              " 506: '[unused501]',\n",
              " 507: '[unused502]',\n",
              " 508: '[unused503]',\n",
              " 509: '[unused504]',\n",
              " 510: '[unused505]',\n",
              " 511: '[unused506]',\n",
              " 512: '[unused507]',\n",
              " 513: '[unused508]',\n",
              " 514: '[unused509]',\n",
              " 515: '[unused510]',\n",
              " 516: '[unused511]',\n",
              " 517: '[unused512]',\n",
              " 518: '[unused513]',\n",
              " 519: '[unused514]',\n",
              " 520: '[unused515]',\n",
              " 521: '[unused516]',\n",
              " 522: '[unused517]',\n",
              " 523: '[unused518]',\n",
              " 524: '[unused519]',\n",
              " 525: '[unused520]',\n",
              " 526: '[unused521]',\n",
              " 527: '[unused522]',\n",
              " 528: '[unused523]',\n",
              " 529: '[unused524]',\n",
              " 530: '[unused525]',\n",
              " 531: '[unused526]',\n",
              " 532: '[unused527]',\n",
              " 533: '[unused528]',\n",
              " 534: '[unused529]',\n",
              " 535: '[unused530]',\n",
              " 536: '[unused531]',\n",
              " 537: '[unused532]',\n",
              " 538: '[unused533]',\n",
              " 539: '[unused534]',\n",
              " 540: '[unused535]',\n",
              " 541: '[unused536]',\n",
              " 542: '[unused537]',\n",
              " 543: '[unused538]',\n",
              " 544: '[unused539]',\n",
              " 545: '[unused540]',\n",
              " 546: '[unused541]',\n",
              " 547: '[unused542]',\n",
              " 548: '[unused543]',\n",
              " 549: '[unused544]',\n",
              " 550: '[unused545]',\n",
              " 551: '[unused546]',\n",
              " 552: '[unused547]',\n",
              " 553: '[unused548]',\n",
              " 554: '[unused549]',\n",
              " 555: '[unused550]',\n",
              " 556: '[unused551]',\n",
              " 557: '[unused552]',\n",
              " 558: '[unused553]',\n",
              " 559: '[unused554]',\n",
              " 560: '[unused555]',\n",
              " 561: '[unused556]',\n",
              " 562: '[unused557]',\n",
              " 563: '[unused558]',\n",
              " 564: '[unused559]',\n",
              " 565: '[unused560]',\n",
              " 566: '[unused561]',\n",
              " 567: '[unused562]',\n",
              " 568: '[unused563]',\n",
              " 569: '[unused564]',\n",
              " 570: '[unused565]',\n",
              " 571: '[unused566]',\n",
              " 572: '[unused567]',\n",
              " 573: '[unused568]',\n",
              " 574: '[unused569]',\n",
              " 575: '[unused570]',\n",
              " 576: '[unused571]',\n",
              " 577: '[unused572]',\n",
              " 578: '[unused573]',\n",
              " 579: '[unused574]',\n",
              " 580: '[unused575]',\n",
              " 581: '[unused576]',\n",
              " 582: '[unused577]',\n",
              " 583: '[unused578]',\n",
              " 584: '[unused579]',\n",
              " 585: '[unused580]',\n",
              " 586: '[unused581]',\n",
              " 587: '[unused582]',\n",
              " 588: '[unused583]',\n",
              " 589: '[unused584]',\n",
              " 590: '[unused585]',\n",
              " 591: '[unused586]',\n",
              " 592: '[unused587]',\n",
              " 593: '[unused588]',\n",
              " 594: '[unused589]',\n",
              " 595: '[unused590]',\n",
              " 596: '[unused591]',\n",
              " 597: '[unused592]',\n",
              " 598: '[unused593]',\n",
              " 599: '[unused594]',\n",
              " 600: '[unused595]',\n",
              " 601: '[unused596]',\n",
              " 602: '[unused597]',\n",
              " 603: '[unused598]',\n",
              " 604: '[unused599]',\n",
              " 605: '[unused600]',\n",
              " 606: '[unused601]',\n",
              " 607: '[unused602]',\n",
              " 608: '[unused603]',\n",
              " 609: '[unused604]',\n",
              " 610: '[unused605]',\n",
              " 611: '[unused606]',\n",
              " 612: '[unused607]',\n",
              " 613: '[unused608]',\n",
              " 614: '[unused609]',\n",
              " 615: '[unused610]',\n",
              " 616: '[unused611]',\n",
              " 617: '[unused612]',\n",
              " 618: '[unused613]',\n",
              " 619: '[unused614]',\n",
              " 620: '[unused615]',\n",
              " 621: '[unused616]',\n",
              " 622: '[unused617]',\n",
              " 623: '[unused618]',\n",
              " 624: '[unused619]',\n",
              " 625: '[unused620]',\n",
              " 626: '[unused621]',\n",
              " 627: '[unused622]',\n",
              " 628: '[unused623]',\n",
              " 629: '[unused624]',\n",
              " 630: '[unused625]',\n",
              " 631: '[unused626]',\n",
              " 632: '[unused627]',\n",
              " 633: '[unused628]',\n",
              " 634: '[unused629]',\n",
              " 635: '[unused630]',\n",
              " 636: '[unused631]',\n",
              " 637: '[unused632]',\n",
              " 638: '[unused633]',\n",
              " 639: '[unused634]',\n",
              " 640: '[unused635]',\n",
              " 641: '[unused636]',\n",
              " 642: '[unused637]',\n",
              " 643: '[unused638]',\n",
              " 644: '[unused639]',\n",
              " 645: '[unused640]',\n",
              " 646: '[unused641]',\n",
              " 647: '[unused642]',\n",
              " 648: '[unused643]',\n",
              " 649: '[unused644]',\n",
              " 650: '[unused645]',\n",
              " 651: '[unused646]',\n",
              " 652: '[unused647]',\n",
              " 653: '[unused648]',\n",
              " 654: '[unused649]',\n",
              " 655: '[unused650]',\n",
              " 656: '[unused651]',\n",
              " 657: '[unused652]',\n",
              " 658: '[unused653]',\n",
              " 659: '[unused654]',\n",
              " 660: '[unused655]',\n",
              " 661: '[unused656]',\n",
              " 662: '[unused657]',\n",
              " 663: '[unused658]',\n",
              " 664: '[unused659]',\n",
              " 665: '[unused660]',\n",
              " 666: '[unused661]',\n",
              " 667: '[unused662]',\n",
              " 668: '[unused663]',\n",
              " 669: '[unused664]',\n",
              " 670: '[unused665]',\n",
              " 671: '[unused666]',\n",
              " 672: '[unused667]',\n",
              " 673: '[unused668]',\n",
              " 674: '[unused669]',\n",
              " 675: '[unused670]',\n",
              " 676: '[unused671]',\n",
              " 677: '[unused672]',\n",
              " 678: '[unused673]',\n",
              " 679: '[unused674]',\n",
              " 680: '[unused675]',\n",
              " 681: '[unused676]',\n",
              " 682: '[unused677]',\n",
              " 683: '[unused678]',\n",
              " 684: '[unused679]',\n",
              " 685: '[unused680]',\n",
              " 686: '[unused681]',\n",
              " 687: '[unused682]',\n",
              " 688: '[unused683]',\n",
              " 689: '[unused684]',\n",
              " 690: '[unused685]',\n",
              " 691: '[unused686]',\n",
              " 692: '[unused687]',\n",
              " 693: '[unused688]',\n",
              " 694: '[unused689]',\n",
              " 695: '[unused690]',\n",
              " 696: '[unused691]',\n",
              " 697: '[unused692]',\n",
              " 698: '[unused693]',\n",
              " 699: '[unused694]',\n",
              " 700: '[unused695]',\n",
              " 701: '[unused696]',\n",
              " 702: '[unused697]',\n",
              " 703: '[unused698]',\n",
              " 704: '[unused699]',\n",
              " 705: '[unused700]',\n",
              " 706: '[unused701]',\n",
              " 707: '[unused702]',\n",
              " 708: '[unused703]',\n",
              " 709: '[unused704]',\n",
              " 710: '[unused705]',\n",
              " 711: '[unused706]',\n",
              " 712: '[unused707]',\n",
              " 713: '[unused708]',\n",
              " 714: '[unused709]',\n",
              " 715: '[unused710]',\n",
              " 716: '[unused711]',\n",
              " 717: '[unused712]',\n",
              " 718: '[unused713]',\n",
              " 719: '[unused714]',\n",
              " 720: '[unused715]',\n",
              " 721: '[unused716]',\n",
              " 722: '[unused717]',\n",
              " 723: '[unused718]',\n",
              " 724: '[unused719]',\n",
              " 725: '[unused720]',\n",
              " 726: '[unused721]',\n",
              " 727: '[unused722]',\n",
              " 728: '[unused723]',\n",
              " 729: '[unused724]',\n",
              " 730: '[unused725]',\n",
              " 731: '[unused726]',\n",
              " 732: '[unused727]',\n",
              " 733: '[unused728]',\n",
              " 734: '[unused729]',\n",
              " 735: '[unused730]',\n",
              " 736: '[unused731]',\n",
              " 737: '[unused732]',\n",
              " 738: '[unused733]',\n",
              " 739: '[unused734]',\n",
              " 740: '[unused735]',\n",
              " 741: '[unused736]',\n",
              " 742: '[unused737]',\n",
              " 743: '[unused738]',\n",
              " 744: '[unused739]',\n",
              " 745: '[unused740]',\n",
              " 746: '[unused741]',\n",
              " 747: '[unused742]',\n",
              " 748: '[unused743]',\n",
              " 749: '[unused744]',\n",
              " 750: '[unused745]',\n",
              " 751: '[unused746]',\n",
              " 752: '[unused747]',\n",
              " 753: '[unused748]',\n",
              " 754: '[unused749]',\n",
              " 755: '[unused750]',\n",
              " 756: '[unused751]',\n",
              " 757: '[unused752]',\n",
              " 758: '[unused753]',\n",
              " 759: '[unused754]',\n",
              " 760: '[unused755]',\n",
              " 761: '[unused756]',\n",
              " 762: '[unused757]',\n",
              " 763: '[unused758]',\n",
              " 764: '[unused759]',\n",
              " 765: '[unused760]',\n",
              " 766: '[unused761]',\n",
              " 767: '[unused762]',\n",
              " 768: '[unused763]',\n",
              " 769: '[unused764]',\n",
              " 770: '[unused765]',\n",
              " 771: '[unused766]',\n",
              " 772: '[unused767]',\n",
              " 773: '[unused768]',\n",
              " 774: '[unused769]',\n",
              " 775: '[unused770]',\n",
              " 776: '[unused771]',\n",
              " 777: '[unused772]',\n",
              " 778: '[unused773]',\n",
              " 779: '[unused774]',\n",
              " 780: '[unused775]',\n",
              " 781: '[unused776]',\n",
              " 782: '[unused777]',\n",
              " 783: '[unused778]',\n",
              " 784: '[unused779]',\n",
              " 785: '[unused780]',\n",
              " 786: '[unused781]',\n",
              " 787: '[unused782]',\n",
              " 788: '[unused783]',\n",
              " 789: '[unused784]',\n",
              " 790: '[unused785]',\n",
              " 791: '[unused786]',\n",
              " 792: '[unused787]',\n",
              " 793: '[unused788]',\n",
              " 794: '[unused789]',\n",
              " 795: '[unused790]',\n",
              " 796: '[unused791]',\n",
              " 797: '[unused792]',\n",
              " 798: '[unused793]',\n",
              " 799: '[unused794]',\n",
              " 800: '[unused795]',\n",
              " 801: '[unused796]',\n",
              " 802: '[unused797]',\n",
              " 803: '[unused798]',\n",
              " 804: '[unused799]',\n",
              " 805: '[unused800]',\n",
              " 806: '[unused801]',\n",
              " 807: '[unused802]',\n",
              " 808: '[unused803]',\n",
              " 809: '[unused804]',\n",
              " 810: '[unused805]',\n",
              " 811: '[unused806]',\n",
              " 812: '[unused807]',\n",
              " 813: '[unused808]',\n",
              " 814: '[unused809]',\n",
              " 815: '[unused810]',\n",
              " 816: '[unused811]',\n",
              " 817: '[unused812]',\n",
              " 818: '[unused813]',\n",
              " 819: '[unused814]',\n",
              " 820: '[unused815]',\n",
              " 821: '[unused816]',\n",
              " 822: '[unused817]',\n",
              " 823: '[unused818]',\n",
              " 824: '[unused819]',\n",
              " 825: '[unused820]',\n",
              " 826: '[unused821]',\n",
              " 827: '[unused822]',\n",
              " 828: '[unused823]',\n",
              " 829: '[unused824]',\n",
              " 830: '[unused825]',\n",
              " 831: '[unused826]',\n",
              " 832: '[unused827]',\n",
              " 833: '[unused828]',\n",
              " 834: '[unused829]',\n",
              " 835: '[unused830]',\n",
              " 836: '[unused831]',\n",
              " 837: '[unused832]',\n",
              " 838: '[unused833]',\n",
              " 839: '[unused834]',\n",
              " 840: '[unused835]',\n",
              " 841: '[unused836]',\n",
              " 842: '[unused837]',\n",
              " 843: '[unused838]',\n",
              " 844: '[unused839]',\n",
              " 845: '[unused840]',\n",
              " 846: '[unused841]',\n",
              " 847: '[unused842]',\n",
              " 848: '[unused843]',\n",
              " 849: '[unused844]',\n",
              " 850: '[unused845]',\n",
              " 851: '[unused846]',\n",
              " 852: '[unused847]',\n",
              " 853: '[unused848]',\n",
              " 854: '[unused849]',\n",
              " 855: '[unused850]',\n",
              " 856: '[unused851]',\n",
              " 857: '[unused852]',\n",
              " 858: '[unused853]',\n",
              " 859: '[unused854]',\n",
              " 860: '[unused855]',\n",
              " 861: '[unused856]',\n",
              " 862: '[unused857]',\n",
              " 863: '[unused858]',\n",
              " 864: '[unused859]',\n",
              " 865: '[unused860]',\n",
              " 866: '[unused861]',\n",
              " 867: '[unused862]',\n",
              " 868: '[unused863]',\n",
              " 869: '[unused864]',\n",
              " 870: '[unused865]',\n",
              " 871: '[unused866]',\n",
              " 872: '[unused867]',\n",
              " 873: '[unused868]',\n",
              " 874: '[unused869]',\n",
              " 875: '[unused870]',\n",
              " 876: '[unused871]',\n",
              " 877: '[unused872]',\n",
              " 878: '[unused873]',\n",
              " 879: '[unused874]',\n",
              " 880: '[unused875]',\n",
              " 881: '[unused876]',\n",
              " 882: '[unused877]',\n",
              " 883: '[unused878]',\n",
              " 884: '[unused879]',\n",
              " 885: '[unused880]',\n",
              " 886: '[unused881]',\n",
              " 887: '[unused882]',\n",
              " 888: '[unused883]',\n",
              " 889: '[unused884]',\n",
              " 890: '[unused885]',\n",
              " 891: '[unused886]',\n",
              " 892: '[unused887]',\n",
              " 893: '[unused888]',\n",
              " 894: '[unused889]',\n",
              " 895: '[unused890]',\n",
              " 896: '[unused891]',\n",
              " 897: '[unused892]',\n",
              " 898: '[unused893]',\n",
              " 899: '[unused894]',\n",
              " 900: '[unused895]',\n",
              " 901: '[unused896]',\n",
              " 902: '[unused897]',\n",
              " 903: '[unused898]',\n",
              " 904: '[unused899]',\n",
              " 905: '[unused900]',\n",
              " 906: '[unused901]',\n",
              " 907: '[unused902]',\n",
              " 908: '[unused903]',\n",
              " 909: '[unused904]',\n",
              " 910: '[unused905]',\n",
              " 911: '[unused906]',\n",
              " 912: '[unused907]',\n",
              " 913: '[unused908]',\n",
              " 914: '[unused909]',\n",
              " 915: '[unused910]',\n",
              " 916: '[unused911]',\n",
              " 917: '[unused912]',\n",
              " 918: '[unused913]',\n",
              " 919: '[unused914]',\n",
              " 920: '[unused915]',\n",
              " 921: '[unused916]',\n",
              " 922: '[unused917]',\n",
              " 923: '[unused918]',\n",
              " 924: '[unused919]',\n",
              " 925: '[unused920]',\n",
              " 926: '[unused921]',\n",
              " 927: '[unused922]',\n",
              " 928: '[unused923]',\n",
              " 929: '[unused924]',\n",
              " 930: '[unused925]',\n",
              " 931: '[unused926]',\n",
              " 932: '[unused927]',\n",
              " 933: '[unused928]',\n",
              " 934: '[unused929]',\n",
              " 935: '[unused930]',\n",
              " 936: '[unused931]',\n",
              " 937: '[unused932]',\n",
              " 938: '[unused933]',\n",
              " 939: '[unused934]',\n",
              " 940: '[unused935]',\n",
              " 941: '[unused936]',\n",
              " 942: '[unused937]',\n",
              " 943: '[unused938]',\n",
              " 944: '[unused939]',\n",
              " 945: '[unused940]',\n",
              " 946: '[unused941]',\n",
              " 947: '[unused942]',\n",
              " 948: '[unused943]',\n",
              " 949: '[unused944]',\n",
              " 950: '[unused945]',\n",
              " 951: '[unused946]',\n",
              " 952: '[unused947]',\n",
              " 953: '[unused948]',\n",
              " 954: '[unused949]',\n",
              " 955: '[unused950]',\n",
              " 956: '[unused951]',\n",
              " 957: '[unused952]',\n",
              " 958: '[unused953]',\n",
              " 959: '[unused954]',\n",
              " 960: '[unused955]',\n",
              " 961: '[unused956]',\n",
              " 962: '[unused957]',\n",
              " 963: '[unused958]',\n",
              " 964: '[unused959]',\n",
              " 965: '[unused960]',\n",
              " 966: '[unused961]',\n",
              " 967: '[unused962]',\n",
              " 968: '[unused963]',\n",
              " 969: '[unused964]',\n",
              " 970: '[unused965]',\n",
              " 971: '[unused966]',\n",
              " 972: '[unused967]',\n",
              " 973: '[unused968]',\n",
              " 974: '[unused969]',\n",
              " 975: '[unused970]',\n",
              " 976: '[unused971]',\n",
              " 977: '[unused972]',\n",
              " 978: '[unused973]',\n",
              " 979: '[unused974]',\n",
              " 980: '[unused975]',\n",
              " 981: '[unused976]',\n",
              " 982: '[unused977]',\n",
              " 983: '[unused978]',\n",
              " 984: '[unused979]',\n",
              " 985: '[unused980]',\n",
              " 986: '[unused981]',\n",
              " 987: '[unused982]',\n",
              " 988: '[unused983]',\n",
              " 989: '[unused984]',\n",
              " 990: '[unused985]',\n",
              " 991: '[unused986]',\n",
              " 992: '[unused987]',\n",
              " 993: '[unused988]',\n",
              " 994: '[unused989]',\n",
              " 995: '[unused990]',\n",
              " 996: '[unused991]',\n",
              " 997: '[unused992]',\n",
              " 998: '[unused993]',\n",
              " 999: '!',\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if the tokenizer works well again in QnA format ([CLS} Question [SEP] Context [SEP] order)\n",
        "print(tokenizer.tokenize(\"Where does Youngsun live in now?\", \"My name is Youngsun Jang, and I live in US now.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "id": "Gp2yFOGgLDDU",
        "outputId": "e6b9efd9-c0da-4fee-b687-4b64184ff3a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'where', 'does', 'young', '##sun', 'live', 'in', 'now', '?', '[SEP]', 'my', 'name', 'is', 'young', '##sun', 'jang', ',', 'and', 'i', 'live', 'in', 'us', 'now', '.', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Embedding (Numberical representation)\n",
        "print(tokenizer.encode(\"Where does Youngsun live in now?\", \"My name is Youngsun Jang, and I live in US now.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "CjOItcBWLDAt",
        "outputId": "6c906454-2d50-48e8-edf4-5fcfc4aee932"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([101, 2073, 2515, 2402, 19729, 2444, 1999, 2085, 1029, 102, 2026, 2171, 2003, 2402, 19729, 23769, 1010, 1998, 1045, 2444, 1999, 2149, 2085, 1012, 102], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fine_train.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "1UqLrciCLKN9",
        "outputId": "31cddd6d-053f-4b6e-fb32-4c651d50efcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-73301f13-41c2-4c3b-9e96-a801be2bd371\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Index</th>\n",
              "      <th>Paper</th>\n",
              "      <th>Category</th>\n",
              "      <th>Question</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Experimental Studies of Brain Tumor Developmen...</td>\n",
              "      <td>Research Subject</td>\n",
              "      <td>What animal has been used?</td>\n",
              "      <td>It has been suggested that electromagnetic fie...</td>\n",
              "      <td>Fischer 344 rats</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Experimental Studies of Brain Tumor Developmen...</td>\n",
              "      <td>Number of Research Subject</td>\n",
              "      <td>How many animals were used?</td>\n",
              "      <td>It has been suggested that electromagnetic fie...</td>\n",
              "      <td>37 experimental rats and 37 matched controls</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Experimental Studies of Brain Tumor Developmen...</td>\n",
              "      <td>Radio Frequency</td>\n",
              "      <td>What is the signal frequency?</td>\n",
              "      <td>It has been suggested that electromagnetic fie...</td>\n",
              "      <td>915 MHz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>Experimental Studies of Brain Tumor Developmen...</td>\n",
              "      <td>Other Units of Exposure Level</td>\n",
              "      <td>How much W/kg was used?</td>\n",
              "      <td>It has been suggested that electromagnetic fie...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>Chronic Exposure of Cancer-Prone Mice to Low-L...</td>\n",
              "      <td>Research Subject</td>\n",
              "      <td>What animal has been used?</td>\n",
              "      <td>The purpose of this study was to determine whe...</td>\n",
              "      <td>C3H/ HeJ mice</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-73301f13-41c2-4c3b-9e96-a801be2bd371')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-73301f13-41c2-4c3b-9e96-a801be2bd371 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-73301f13-41c2-4c3b-9e96-a801be2bd371');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Index  ...                                        Answer\n",
              "0      1  ...                              Fischer 344 rats\n",
              "1      1  ...  37 experimental rats and 37 matched controls\n",
              "2      1  ...                                       915 MHz\n",
              "3      1  ...                                           NaN\n",
              "4      3  ...                                 C3H/ HeJ mice\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Question: \", fine_train.loc[0, 'Question'])\n",
        "print(\"Context: \", fine_train.loc[0, 'Abstract'])\n",
        "print(\"Answer: \", fine_train.loc[0, 'Answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "xIcG0IMXLKLm",
        "outputId": "c6834024-db1f-4fe6-e40c-30e74c23f53d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:  What animal has been used?\n",
            "Context:  It has been suggested that electromagnetic fields (EMFs) act as a promoter late in the carcinogenesis process. To date, however, noconvincing laboratory evidence has been obtained indicating that EMFs cause tumour promotion at non-thermal exposure levels. The effects of EMF exposure in a rat brain glioma model were investigated. The exposure consisted of 915 MHz microwaves, both as continuous waves (1 W), and modulated with 4, 8, 16 and 200 Hz in 0.5 ms pulses and 50 Hz in 6 ms pulses (2 W per pulse). Fischer 344 rats of both sexes, weighing 150–250 g, were used in the experiments. 5000 RG2 cells in 5 μ1 nutrient solution were injected by the stereotaxic technique into the head of the right caudate nucleus in 37 experimental rats and 37 matched controls. The exposed animals were kept unanaesthetized in well ventilated transverse electromagnetic (TEM) cells producing 915 MHz continuous or modulated microwaves. Exposure was started on day five after inoculation. The animals were exposed for 7 hd d−1 for 5 d per week during two to three weeks. The controls were kept in an identical TEM cell without EMF exposure. All brains were examined histopathologically and the tumour size was determined. Our study does not show a significant difference in tumour size between animals exposed to 915 MHz microwaves, and those not exposed. Our preliminary results do not support that even an extensive daily exposure to EMF promotes tumour growth when given from the fifth day after the start of tumour growth in the rat brain until the death of the animal which by then has a large brain tumour. Further studies with higher specific absorption rate levels are in progress.\n",
            "Answer:  Fischer 344 rats\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QA is to confirm the position of the Answer token in Context \n",
        "# position of Saint Bernadette Soubirous !\n",
        "# That is, from 3002 - 2271\n",
        "\n",
        "print(\"Question: \", tokenizer.tokenize(fine_train.loc[0, 'Question']))\n",
        "print(\"Context: \", tokenizer.tokenize(fine_train.loc[0, 'Abstract']))\n",
        "print(\"Answer: \", tokenizer.tokenize(fine_train.loc[0, 'Answer']))\n",
        "\n",
        "print(\"Question: \", tokenizer.encode(fine_train.loc[0, 'Question']))\n",
        "print(\"Context: \", tokenizer.encode(fine_train.loc[0, 'Abstract']))\n",
        "print(\"Answer: \", tokenizer.encode(fine_train.loc[0, 'Answer']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "o2SFrtVKLKJM",
        "outputId": "c52b1bff-9978-42f7-8ab9-23f3378be41c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:  ['[CLS]', 'what', 'animal', 'has', 'been', 'used', '?', '[SEP]']\n",
            "Context:  ['[CLS]', 'it', 'has', 'been', 'suggested', 'that', 'electromagnetic', 'fields', '(', 'em', '##fs', ')', 'act', 'as', 'a', 'promoter', 'late', 'in', 'the', 'car', '##cino', '##genesis', 'process', '.', 'to', 'date', ',', 'however', ',', 'no', '##con', '##vin', '##cing', 'laboratory', 'evidence', 'has', 'been', 'obtained', 'indicating', 'that', 'em', '##fs', 'cause', 'tu', '##mour', 'promotion', 'at', 'non', '-', 'thermal', 'exposure', 'levels', '.', 'the', 'effects', 'of', 'em', '##f', 'exposure', 'in', 'a', 'rat', 'brain', 'g', '##lio', '##ma', 'model', 'were', 'investigated', '.', 'the', 'exposure', 'consisted', 'of', '91', '##5', 'mhz', 'microwave', '##s', ',', 'both', 'as', 'continuous', 'waves', '(', '1', 'w', ')', ',', 'and', 'mod', '##ulated', 'with', '4', ',', '8', ',', '16', 'and', '200', 'hz', 'in', '0', '.', '5', 'ms', 'pulses', 'and', '50', 'hz', 'in', '6', 'ms', 'pulses', '(', '2', 'w', 'per', 'pulse', ')', '.', 'fischer', '344', 'rats', 'of', 'both', 'sexes', ',', 'weighing', '150', '–', '250', 'g', ',', 'were', 'used', 'in', 'the', 'experiments', '.', '5000', 'r', '##g', '##2', 'cells', 'in', '5', 'μ', '##1', 'nutrient', 'solution', 'were', 'injected', 'by', 'the', 'stereo', '##ta', '##xi', '##c', 'technique', 'into', 'the', 'head', 'of', 'the', 'right', 'ca', '##uda', '##te', 'nucleus', 'in', '37', 'experimental', 'rats', 'and', '37', 'matched', 'controls', '.', 'the', 'exposed', 'animals', 'were', 'kept', 'una', '##nae', '##st', '##het', '##ized', 'in', 'well', 'vent', '##ila', '##ted', 'transverse', 'electromagnetic', '(', 'te', '##m', ')', 'cells', 'producing', '91', '##5', 'mhz', 'continuous', 'or', 'mod', '##ulated', 'microwave', '##s', '.', 'exposure', 'was', 'started', 'on', 'day', 'five', 'after', 'in', '##oc', '##ulation', '.', 'the', 'animals', 'were', 'exposed', 'for', '7', 'hd', 'd', '##−1', 'for', '5', 'd', 'per', 'week', 'during', 'two', 'to', 'three', 'weeks', '.', 'the', 'controls', 'were', 'kept', 'in', 'an', 'identical', 'te', '##m', 'cell', 'without', 'em', '##f', 'exposure', '.', 'all', 'brains', 'were', 'examined', 'his', '##top', '##ath', '##ological', '##ly', 'and', 'the', 'tu', '##mour', 'size', 'was', 'determined', '.', 'our', 'study', 'does', 'not', 'show', 'a', 'significant', 'difference', 'in', 'tu', '##mour', 'size', 'between', 'animals', 'exposed', 'to', '91', '##5', 'mhz', 'microwave', '##s', ',', 'and', 'those', 'not', 'exposed', '.', 'our', 'preliminary', 'results', 'do', 'not', 'support', 'that', 'even', 'an', 'extensive', 'daily', 'exposure', 'to', 'em', '##f', 'promotes', 'tu', '##mour', 'growth', 'when', 'given', 'from', 'the', 'fifth', 'day', 'after', 'the', 'start', 'of', 'tu', '##mour', 'growth', 'in', 'the', 'rat', 'brain', 'until', 'the', 'death', 'of', 'the', 'animal', 'which', 'by', 'then', 'has', 'a', 'large', 'brain', 'tu', '##mour', '.', 'further', 'studies', 'with', 'higher', 'specific', 'absorption', 'rate', 'levels', 'are', 'in', 'progress', '.', '[SEP]']\n",
            "Answer:  ['[CLS]', 'fischer', '344', 'rats', '[SEP]']\n",
            "Question:  ([101, 2054, 4111, 2038, 2042, 2109, 1029, 102], [0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Context:  ([101, 2009, 2038, 2042, 4081, 2008, 17225, 4249, 1006, 7861, 10343, 1007, 2552, 2004, 1037, 15543, 2397, 1999, 1996, 2482, 21081, 23737, 2832, 1012, 2000, 3058, 1010, 2174, 1010, 2053, 8663, 6371, 6129, 5911, 3350, 2038, 2042, 4663, 8131, 2008, 7861, 10343, 3426, 10722, 20360, 4712, 2012, 2512, 1011, 9829, 7524, 3798, 1012, 1996, 3896, 1997, 7861, 2546, 7524, 1999, 1037, 9350, 4167, 1043, 12798, 2863, 2944, 2020, 10847, 1012, 1996, 7524, 5031, 1997, 6205, 2629, 11413, 18302, 2015, 1010, 2119, 2004, 7142, 5975, 1006, 1015, 1059, 1007, 1010, 1998, 16913, 8898, 2007, 1018, 1010, 1022, 1010, 2385, 1998, 3263, 22100, 1999, 1014, 1012, 1019, 5796, 23894, 1998, 2753, 22100, 1999, 1020, 5796, 23894, 1006, 1016, 1059, 2566, 8187, 1007, 1012, 13042, 29386, 11432, 1997, 2119, 21024, 1010, 15243, 5018, 1516, 5539, 1043, 1010, 2020, 2109, 1999, 1996, 7885, 1012, 13509, 1054, 2290, 2475, 4442, 1999, 1019, 1166, 2487, 26780, 5576, 2020, 19737, 2011, 1996, 12991, 2696, 9048, 2278, 6028, 2046, 1996, 2132, 1997, 1996, 2157, 6187, 14066, 2618, 13502, 1999, 4261, 6388, 11432, 1998, 4261, 10349, 7711, 1012, 1996, 6086, 4176, 2020, 2921, 14477, 17452, 3367, 27065, 3550, 1999, 2092, 18834, 11733, 3064, 18323, 17225, 1006, 8915, 2213, 1007, 4442, 5155, 6205, 2629, 11413, 7142, 2030, 16913, 8898, 18302, 2015, 1012, 7524, 2001, 2318, 2006, 2154, 2274, 2044, 1999, 10085, 9513, 1012, 1996, 4176, 2020, 6086, 2005, 1021, 10751, 1040, 27944, 2005, 1019, 1040, 2566, 2733, 2076, 2048, 2000, 2093, 3134, 1012, 1996, 7711, 2020, 2921, 1999, 2019, 7235, 8915, 2213, 3526, 2302, 7861, 2546, 7524, 1012, 2035, 14332, 2020, 8920, 2010, 14399, 8988, 10091, 2135, 1998, 1996, 10722, 20360, 2946, 2001, 4340, 1012, 2256, 2817, 2515, 2025, 2265, 1037, 3278, 4489, 1999, 10722, 20360, 2946, 2090, 4176, 6086, 2000, 6205, 2629, 11413, 18302, 2015, 1010, 1998, 2216, 2025, 6086, 1012, 2256, 8824, 3463, 2079, 2025, 2490, 2008, 2130, 2019, 4866, 3679, 7524, 2000, 7861, 2546, 14067, 10722, 20360, 3930, 2043, 2445, 2013, 1996, 3587, 2154, 2044, 1996, 2707, 1997, 10722, 20360, 3930, 1999, 1996, 9350, 4167, 2127, 1996, 2331, 1997, 1996, 4111, 2029, 2011, 2059, 2038, 1037, 2312, 4167, 10722, 20360, 1012, 2582, 2913, 2007, 3020, 3563, 16326, 3446, 3798, 2024, 1999, 5082, 1012, 102], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Answer:  ([101, 13042, 29386, 11432, 102], [0, 0, 0, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting only token part excepting the segment part\n",
        "fine_context = tokenizer.encode(fine_train.loc[0, 'Abstract'])[0]\n",
        "fine_text = tokenizer.encode(fine_train.loc[0, 'Answer'])[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "g1N1d0hULKGv",
        "outputId": "2d70bf93-2b69-44cb-cad5-a8c3f77a56cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CLS, SEP token delete\n",
        "fine_text.pop(0)\n",
        "fine_text.pop(-1)\n",
        "print(fine_text)\n",
        "print(len(fine_text))\n",
        "len(fine_context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "xxGn3nYwLKEV",
        "outputId": "c5271966-bccd-4083-d709-b22cb4816ff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13042, 29386, 11432]\n",
            "3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "367"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To calculate the length of text (answer)\n",
        "# Method : Doing 'Sliding' the context with the length of the answer\n",
        "fine_text_slide_len = len(fine_text) # text_slide_len = 8\n",
        "\n",
        "for j in range(0, (len(fine_context))):\n",
        "  # exist_flag : showing whether it is answerable or not (similar with is_unanswerable in SimpleTransforemr)\n",
        "  # 0 : No answer / 1 : Having answer\n",
        "  exist_flag = 0 \n",
        "  if fine_text == fine_context[j:j+fine_text_slide_len]: # [0:8]->[1:9]->[2:10]->..->[159:160]\n",
        "    # Assign the location of answer (start, end)\n",
        "    fine_ans_start = j\n",
        "    fine_ans_end = j + fine_text_slide_len - 1\n",
        "    # If matched, exist_flag changed to 1\n",
        "    exist_flag = 1\n",
        "    break\n",
        "\n",
        "print(\"ans_start : {}, ans_end : {}\".format(fine_ans_start, fine_ans_end))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "Vf-JRUiJLKB5",
        "outputId": "120f5574-3628-4c5f-e206-963b944f56d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ans_start : 121, ans_end : 123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check\n",
        "# context[ans_start:ans_end]\n",
        "print(fine_context[fine_ans_start:fine_ans_end+1], fine_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "6oNGiPNfLJ_g",
        "outputId": "7264e3e1-cd88-4e28-ea22-9a1cdd096076"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13042, 29386, 11432] [13042, 29386, 11432]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function calculating the length of answer in context dataset all at once\n",
        "def fine_convert_data(data_df):\n",
        "  global tokenizer\n",
        "  indices, segments, target_start, target_end = [], [], [], []\n",
        "\n",
        "  for i in tqdm(range(len(data_df))):\n",
        "    # que : List of tokenized question\n",
        "    que, _ = tokenizer.encode(data_df[FINE_QUESTION_COLUMN][i])\n",
        "    # doc : List of tokenized context\n",
        "    doc, _ = tokenizer.encode(data_df[FINE_DATA_COLUMN][i])\n",
        "\n",
        "    # [CLS] token deleted in context\n",
        "    doc.pop(0)\n",
        "\n",
        "    # Length of question & context\n",
        "    que_len = len(que)\n",
        "    doc_len = len(doc)\n",
        "    # 1. Length of question\n",
        "    # The question is cut by the length of 64\n",
        "    if que_len > 64:\n",
        "      que = que[:63]\n",
        "      que.append(102) # [SEP] token added to make it clear the question block\n",
        "    # 2. Total length of question and context\n",
        "    # The total input is cut by the length of 384\n",
        "    if len(que+doc) > SEQ_LEN:\n",
        "      while len(que+doc) != SEQ_LEN:\n",
        "        doc.pop(-1)\n",
        "      doc.pop(-1)\n",
        "      doc.append(102)\n",
        "\n",
        "    # Segment embedding\n",
        "    # Question : 0 / Context 1 / Padding : 0 (remaining part for short sentences)\n",
        "        \n",
        "    ############################\n",
        "    ###### Segment 예시 ########\n",
        "    ############################\n",
        "    \n",
        "    # question, context, padding\n",
        "    # 00000000, 1111111, 0000000\n",
        "    \n",
        "    segment = [0]*len(que) + [1]*len(doc) + [0]*(SEQ_LEN-len(que)-len(doc))\n",
        "\n",
        "    # Padding\n",
        "    if len(que+doc) <= SEQ_LEN:\n",
        "      while len(que+doc) != SEQ_LEN:\n",
        "        doc.append(0)\n",
        "\n",
        "    # Final Input 'ids' (Question + Context)\n",
        "    ids = que + doc\n",
        "\n",
        "    # Sliding Part\n",
        "    fine_text = tokenizer.encode(data_df[FINE_TEXT].astype(str)[i])[0]\n",
        "    fine_text_slide_len = len(fine_text[1:-1]) # text_slide_len = 8\n",
        "\n",
        "    for j in range(0, (len(doc))):\n",
        "      # exist_flag : showing whether it is answerable or not (similar with is_unanswerable in SimpleTransforemr)\n",
        "      # 0 : No answer / 1 : Having answer\n",
        "      exist_flag = 0 \n",
        "      if fine_text[1:-1] == doc[j:j+fine_text_slide_len]: # [0:8]->[1:9]->[2:10]->..->[159:160]\n",
        "        # Assign the location of answer (start, end)\n",
        "        fine_ans_start = j + len(que)\n",
        "        fine_ans_end = j + fine_text_slide_len - 1 + len(que)\n",
        "        # If matched, exist_flag changed to 1\n",
        "        exist_flag = 1\n",
        "        break\n",
        "\n",
        "    # When no answer case (exist_flag = 0), starting & ending value become SEQ_LEN\n",
        "    # All the data of starting, ending = 384 (SEQ_LEN) will be deleted from the list\n",
        "    if exist_flag == 0:\n",
        "      fine_ans_start = SEQ_LEN\n",
        "      fine_ans_end = SEQ_LEN\n",
        "\n",
        "    # Input(ids), Segment saving into list type (indices, segments)\n",
        "    indices.append(ids)\n",
        "    segments.append(segment)\n",
        "    # Starting and ending info saving into list type (target_start, target_end)\n",
        "    target_start.append(fine_ans_start)\n",
        "    target_end.append(fine_ans_end)\n",
        "\n",
        "  # Converting the 4 lists into numpy array\n",
        "  indices_x = np.array(indices)\n",
        "  segments = np.array(segments)\n",
        "  target_start = np.array(target_start)\n",
        "  target_end = np.array(target_end)\n",
        "\n",
        "  # The cut part saved in del_list and deleted from data\n",
        "  del_list = np.where(target_start != SEQ_LEN)[0]\n",
        "  not_del_list = np.where(target_start == SEQ_LEN)[0]\n",
        "  indices_x = indices_x[del_list]\n",
        "  segments = segments[del_list]\n",
        "  target_start = target_start[del_list]\n",
        "  target_end = target_end[del_list]\n",
        "\n",
        "  return [indices_x, segments], [target_start, target_end], not_del_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "mB_G4K7yLJ82",
        "outputId": "50f149e0-c937-426e-b493-f83c33887e6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load of the Calculator Function\n",
        "def load_data(pandas_dataframe):\n",
        "  data_df = pandas_dataframe\n",
        "  data_df[FINE_DATA_COLUMN] = data_df[FINE_DATA_COLUMN].astype(str)\n",
        "  data_df[FINE_QUESTION_COLUMN] = data_df[FINE_QUESTION_COLUMN].astype(str)\n",
        "  data_x, data_y, del_list = fine_convert_data(data_df)\n",
        "  \n",
        "  return data_x, data_y, del_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "BnxtuqUkLax0",
        "outputId": "198dd9cc-d2ac-486a-b386-7d9ddc02f723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Data Coneversion Starting\n",
        "fine_train_x, fine_train_y, z = load_data(fine_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "P-5Nnl48Lavg",
        "outputId": "3ab79b21-a68b-4503-b264-d4994f98e5c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 668/668 [00:02<00:00, 248.44it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(fine_train_x[0].shape, fine_train_y[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "rz9C9pDfLatN",
        "outputId": "b3bf7527-b909-4b9d-f260-5162324ec21a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(451, 384) (451,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load the Pretrained Model\n",
        "# model = load_trained_model_from_checkpoint (\n",
        "#     config_path,\n",
        "#     checkpoint_path,\n",
        "#     training = False,\n",
        "#     trainable = True,\n",
        "#     seq_len = SEQ_LEN\n",
        "# )\n",
        "\n",
        "# model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "5W56Lct2Laqr",
        "outputId": "fd534daf-19b3-47be-f02f-c3df2248258a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For Transfer Learning, Customized Layer needs to be added after 12 Encoder\n",
        "# by defining 'Non-masking' function, BERT Model's masked tensors disclosed\n",
        "\n",
        "class NonMasking(Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    self.supports_masking = True\n",
        "    super(NonMasking, self).__init__(**kwargs)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    input_shape = input_shape\n",
        "  \n",
        "  def compute_mask(self, input, input_mask = None):\n",
        "    return None\n",
        "\n",
        "  def call(self, x, mask = None):\n",
        "    return x\n",
        "\n",
        "  def get_output_shape_for(self, input_shape):\n",
        "    return input_shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "NoQsZ96rLaoj",
        "outputId": "f36f2ff9-552d-4a73-e1cc-fb1afbadd7fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation function definition\n",
        "def bert_gelu(x):\n",
        "  \"\"\"Gaussian Error Linear Unit.\n",
        "  This is a smoother version of the RELU.\n",
        "  Original paper: https://arxiv.org/abs/1606.08415\n",
        "  Args:\n",
        "    x : float Tensor to perform activation\n",
        "  Returns:\n",
        "    'x' with the GELU activation applied.\n",
        "  \"\"\"\n",
        "  cdf = 0.5*(1.0+ K.tanh(\n",
        "      (np.sqrt(2/np.pi)*(x+0.044715 * K.pow(x,3)))))\n",
        "  return x*cdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "TEj0b5JILein",
        "outputId": "00264fce-7d92-4ba7-aedc-1fa8799a2d2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation Function Layer attached to Transformer\n",
        "class Start_End_Prediction(Layer):\n",
        "  def __init__(self, seq_len, **kwargs):\n",
        "    self.seq_len = SEQ_LEN\n",
        "    self.supports_masking = True\n",
        "    super(Start_End_Prediction, self).__init__(**kwargs)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    # A tensor ('self.W') multiplied with the final layer (12 encoder, batch_size, 384, 768)\n",
        "    # Making Output tensor as (384, 2 dimension (768->2))\n",
        "    self.W = self.add_weight(name='kernel',\n",
        "                             shape = (input_shape[2],2),\n",
        "                             initializer = 'uniform',\n",
        "                             trainable = True)\n",
        "    super(Start_End_Prediction, self).build(input_shape)\n",
        "\n",
        "  def call(self, x):\n",
        "    # Redifine Output dimension as (384 * 768)\n",
        "    x = K.reshape(x, shape=(-1, self.seq_len, K.shape(x)[2]))\n",
        "    # Dot production between self.W and x\n",
        "    # (batch_size, 384, 768) * (384, 2) = (batch_size, 384, 2)\n",
        "    x = K.dot(x, self.W)\n",
        "    # (batch_size, 384, 2) -> (2, batch_size, 384)\n",
        "    x = K.permute_dimensions(x, (2, 0, 1))\n",
        "\n",
        "    # Split the (2, batch_size, 384) into 2 (batch_size, 384)  --> start_logits & end_logits\n",
        "    # start_logits = (batch_size, 384)\n",
        "    # end_logits = (batch_size, 384)\n",
        "    self.start_logits, self.end_logits = x[0], x[1]\n",
        "\n",
        "    # Fed into, Passed by the GELU layer\n",
        "    self.start_logits = bert_gelu(self.start_logits)\n",
        "    self.end_logits = bert_gelu(self.end_logits)\n",
        "\n",
        "    # Fed into, Passed by the Softmax layer\n",
        "    # Getting probability for 384 tokens\n",
        "    self.start_logits = K.softmax(self.start_logits, axis=-1)\n",
        "    self.end_logits = K.softmax(self.end_logits, axis=-1)\n",
        "\n",
        "    return [self.start_logits, self.end_logits]\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    # In Keras, when defining custom layer, \n",
        "    # the output dimension must be defined in compute_output_shape function\n",
        "    return [(input_shape[0], self.seq_len), (input_shape[0], self.seq_len)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "0ESWVpp3Legf",
        "outputId": "34dcc40f-3147-4c1b-ddab-a3fef26d8893"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load BERT model\n",
        "# from keras.layers import merge, dot, concatenate\n",
        "# from keras import metrics\n",
        "# import numpy as np\n",
        "\n",
        "# # Define a functoin loading BERT model\n",
        "# def get_bert_finetuning_model(model):\n",
        "#   # Input data is token-embedding and segment-embedding\n",
        "#   inputs = model.inputs[:2]\n",
        "\n",
        "#   # Output of Transformer : (batch_size, 384, 768)\n",
        "#   bert_transformer = model.layers[-1].output\n",
        "\n",
        "#   # Unmask the maksed tensors using NonMasking function\n",
        "#   x = NonMasking()(bert_transformer)\n",
        "\n",
        "#   # Show the start and end token of answer at last\n",
        "#   outputs_start, outputs_end = Start_End_Prediction(SEQ_LEN)(x)\n",
        "\n",
        "#   bert_model = keras.models.Model(inputs, [outputs_start, outputs_end])\n",
        "\n",
        "#   # Optimizer defined growing Learning_rate from 0 to 1.5e-5 gradually\n",
        "#   # Original RAdam changed to RAdamOptimizer for some issue\n",
        "#   optimizer_warmup = RAdamOptimizer(learning_rate = 1.5e-5, warmup_proportion=0.18, epsilon=1e-6, weight_decay = 0.01)\n",
        "\n",
        "#   # Final Model compile\n",
        "#   bert_model.compile(\n",
        "#       optimizer = optimizer_warmup,\n",
        "#       loss = 'sparse_categorical_crossentropy',\n",
        "#       metrics = ['accuracy']\n",
        "#   )\n",
        "\n",
        "#   return bert_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "sBOZkUliLeeL",
        "outputId": "65f82842-3599-4823-cc55-4e4d55df7f6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training -- Error\n",
        "# sess = K.get_session()\n",
        "# uninitialized_variables = set([i.decode('ascii') for i in sess.run(tf.report_uninitialized_variables())])\n",
        "# init = tf.variables_initializer([v for v in tf.global_variables() if v.name.split(':')[0] in uninitialized_variables])\n",
        "# sess.run(init)\n",
        "\n",
        "# bert_model = get_bert_finetuning_model(bert_model)\n",
        "bert_model.summary()\n",
        "\n",
        "# Training Start!\n",
        "history = bert_model.fit(fine_train_x, fine_train_y, batch_size=5, shuffle=True, verbose=1, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PF_DwEk1LhBD",
        "outputId": "8cd1642a-c359-458d-9176-84be58ea4656"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Input-Token (InputLayer)       [(None, 384)]        0           []                               \n",
            "                                                                                                  \n",
            " Input-Segment (InputLayer)     [(None, 384)]        0           []                               \n",
            "                                                                                                  \n",
            " Embedding-Token (TokenEmbeddin  [(None, 384, 768),  23440896    ['Input-Token[0][0]']            \n",
            " g)                              (30522, 768)]                                                    \n",
            "                                                                                                  \n",
            " Embedding-Segment (Embedding)  (None, 384, 768)     1536        ['Input-Segment[0][0]']          \n",
            "                                                                                                  \n",
            " Embedding-Token-Segment (Add)  (None, 384, 768)     0           ['Embedding-Token[0][0]',        \n",
            "                                                                  'Embedding-Segment[0][0]']      \n",
            "                                                                                                  \n",
            " Embedding-Position (PositionEm  (None, 384, 768)    294912      ['Embedding-Token-Segment[0][0]']\n",
            " bedding)                                                                                         \n",
            "                                                                                                  \n",
            " Embedding-Dropout (Dropout)    (None, 384, 768)     0           ['Embedding-Position[0][0]']     \n",
            "                                                                                                  \n",
            " Embedding-Norm (LayerNormaliza  (None, 384, 768)    1536        ['Embedding-Dropout[0][0]']      \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " Encoder-1-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Embedding-Norm[0][0]']         \n",
            " on (MultiHeadAttention)                                                                          \n",
            "                                                                                                  \n",
            " Encoder-1-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-1-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-1-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Embedding-Norm[0][0]',         \n",
            " on-Add (Add)                                                     'Encoder-1-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-1-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-1-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-1-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-1-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-1-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-1-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-1-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-1-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-1-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-1-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-1-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-2-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-1-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-2-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-2-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-2-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-1-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-2-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-2-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-2-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-2-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-2-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-2-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-2-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-2-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-2-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-2-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-2-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-2-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-3-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-2-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-3-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-3-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-3-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-2-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-3-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-3-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-3-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-3-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-3-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-3-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-3-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-3-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-3-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-3-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-3-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-3-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-4-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-3-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-4-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-4-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-4-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-3-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-4-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-4-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-4-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-4-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-4-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-4-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-4-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-4-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-4-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-4-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-4-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-4-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-5-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-4-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-5-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-5-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-5-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-4-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-5-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-5-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-5-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-5-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-5-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-5-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-5-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-5-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-5-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-5-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-5-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-5-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-6-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-5-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-6-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-6-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-6-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-5-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-6-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-6-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-6-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-6-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-6-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-6-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-6-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-6-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-6-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-6-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-6-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-6-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-7-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-6-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-7-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-7-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-7-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-6-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-7-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-7-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-7-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-7-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-7-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-7-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-7-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-7-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-7-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-7-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-7-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-7-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-8-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-7-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-8-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-8-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-8-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-7-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-8-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-8-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-8-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-8-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-8-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-8-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-8-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-8-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-8-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-8-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-8-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-8-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-9-MultiHeadSelfAttenti  (None, 384, 768)    2362368     ['Encoder-8-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-9-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-9-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-9-MultiHeadSelfAttenti  (None, 384, 768)    0           ['Encoder-8-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-9-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-9-MultiHeadSelfAttenti  (None, 384, 768)    1536        ['Encoder-9-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-9-FeedForward (FeedFor  (None, 384, 768)    4722432     ['Encoder-9-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-9-FeedForward-Dropout   (None, 384, 768)    0           ['Encoder-9-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-9-FeedForward-Add (Add  (None, 384, 768)    0           ['Encoder-9-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-9-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-9-FeedForward-Norm (La  (None, 384, 768)    1536        ['Encoder-9-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-10-MultiHeadSelfAttent  (None, 384, 768)    2362368     ['Encoder-9-FeedForward-Norm[0][0\n",
            " ion (MultiHeadAttention)                                        ]']                              \n",
            "                                                                                                  \n",
            " Encoder-10-MultiHeadSelfAttent  (None, 384, 768)    0           ['Encoder-10-MultiHeadSelfAttenti\n",
            " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
            "                                                                                                  \n",
            " Encoder-10-MultiHeadSelfAttent  (None, 384, 768)    0           ['Encoder-9-FeedForward-Norm[0][0\n",
            " ion-Add (Add)                                                   ]',                              \n",
            "                                                                  'Encoder-10-MultiHeadSelfAttenti\n",
            "                                                                 on-Dropout[0][0]']               \n",
            "                                                                                                  \n",
            " Encoder-10-MultiHeadSelfAttent  (None, 384, 768)    1536        ['Encoder-10-MultiHeadSelfAttenti\n",
            " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-10-FeedForward (FeedFo  (None, 384, 768)    4722432     ['Encoder-10-MultiHeadSelfAttenti\n",
            " rward)                                                          on-Norm[0][0]']                  \n",
            "                                                                                                  \n",
            " Encoder-10-FeedForward-Dropout  (None, 384, 768)    0           ['Encoder-10-FeedForward[0][0]'] \n",
            "  (Dropout)                                                                                       \n",
            "                                                                                                  \n",
            " Encoder-10-FeedForward-Add (Ad  (None, 384, 768)    0           ['Encoder-10-MultiHeadSelfAttenti\n",
            " d)                                                              on-Norm[0][0]',                  \n",
            "                                                                  'Encoder-10-FeedForward-Dropout[\n",
            "                                                                 0][0]']                          \n",
            "                                                                                                  \n",
            " Encoder-10-FeedForward-Norm (L  (None, 384, 768)    1536        ['Encoder-10-FeedForward-Add[0][0\n",
            " ayerNormalization)                                              ]']                              \n",
            "                                                                                                  \n",
            " Encoder-11-MultiHeadSelfAttent  (None, 384, 768)    2362368     ['Encoder-10-FeedForward-Norm[0][\n",
            " ion (MultiHeadAttention)                                        0]']                             \n",
            "                                                                                                  \n",
            " Encoder-11-MultiHeadSelfAttent  (None, 384, 768)    0           ['Encoder-11-MultiHeadSelfAttenti\n",
            " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
            "                                                                                                  \n",
            " Encoder-11-MultiHeadSelfAttent  (None, 384, 768)    0           ['Encoder-10-FeedForward-Norm[0][\n",
            " ion-Add (Add)                                                   0]',                             \n",
            "                                                                  'Encoder-11-MultiHeadSelfAttenti\n",
            "                                                                 on-Dropout[0][0]']               \n",
            "                                                                                                  \n",
            " Encoder-11-MultiHeadSelfAttent  (None, 384, 768)    1536        ['Encoder-11-MultiHeadSelfAttenti\n",
            " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-11-FeedForward (FeedFo  (None, 384, 768)    4722432     ['Encoder-11-MultiHeadSelfAttenti\n",
            " rward)                                                          on-Norm[0][0]']                  \n",
            "                                                                                                  \n",
            " Encoder-11-FeedForward-Dropout  (None, 384, 768)    0           ['Encoder-11-FeedForward[0][0]'] \n",
            "  (Dropout)                                                                                       \n",
            "                                                                                                  \n",
            " Encoder-11-FeedForward-Add (Ad  (None, 384, 768)    0           ['Encoder-11-MultiHeadSelfAttenti\n",
            " d)                                                              on-Norm[0][0]',                  \n",
            "                                                                  'Encoder-11-FeedForward-Dropout[\n",
            "                                                                 0][0]']                          \n",
            "                                                                                                  \n",
            " Encoder-11-FeedForward-Norm (L  (None, 384, 768)    1536        ['Encoder-11-FeedForward-Add[0][0\n",
            " ayerNormalization)                                              ]']                              \n",
            "                                                                                                  \n",
            " Encoder-12-MultiHeadSelfAttent  (None, 384, 768)    2362368     ['Encoder-11-FeedForward-Norm[0][\n",
            " ion (MultiHeadAttention)                                        0]']                             \n",
            "                                                                                                  \n",
            " Encoder-12-MultiHeadSelfAttent  (None, 384, 768)    0           ['Encoder-12-MultiHeadSelfAttenti\n",
            " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
            "                                                                                                  \n",
            " Encoder-12-MultiHeadSelfAttent  (None, 384, 768)    0           ['Encoder-11-FeedForward-Norm[0][\n",
            " ion-Add (Add)                                                   0]',                             \n",
            "                                                                  'Encoder-12-MultiHeadSelfAttenti\n",
            "                                                                 on-Dropout[0][0]']               \n",
            "                                                                                                  \n",
            " Encoder-12-MultiHeadSelfAttent  (None, 384, 768)    1536        ['Encoder-12-MultiHeadSelfAttenti\n",
            " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-12-FeedForward (FeedFo  (None, 384, 768)    4722432     ['Encoder-12-MultiHeadSelfAttenti\n",
            " rward)                                                          on-Norm[0][0]']                  \n",
            "                                                                                                  \n",
            " Encoder-12-FeedForward-Dropout  (None, 384, 768)    0           ['Encoder-12-FeedForward[0][0]'] \n",
            "  (Dropout)                                                                                       \n",
            "                                                                                                  \n",
            " Encoder-12-FeedForward-Add (Ad  (None, 384, 768)    0           ['Encoder-12-MultiHeadSelfAttenti\n",
            " d)                                                              on-Norm[0][0]',                  \n",
            "                                                                  'Encoder-12-FeedForward-Dropout[\n",
            "                                                                 0][0]']                          \n",
            "                                                                                                  \n",
            " Encoder-12-FeedForward-Norm (L  (None, 384, 768)    1536        ['Encoder-12-FeedForward-Add[0][0\n",
            " ayerNormalization)                                              ]']                              \n",
            "                                                                                                  \n",
            " non_masking (NonMasking)       (None, 384, 768)     0           ['Encoder-12-FeedForward-Norm[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " start__end__prediction (Start_  [(None, 384),       1536        ['non_masking[0][0]']            \n",
            " End_Prediction)                 (None, 384)]                                                     \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,794,880\n",
            "Trainable params: 108,794,880\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/5\n",
            "91/91 [==============================] - 30s 330ms/step - loss: 0.0157 - start__end__prediction_loss: 0.0121 - start__end__prediction_1_loss: 0.0037 - start__end__prediction_accuracy: 0.9956 - start__end__prediction_1_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "91/91 [==============================] - 30s 329ms/step - loss: 0.0176 - start__end__prediction_loss: 0.0154 - start__end__prediction_1_loss: 0.0022 - start__end__prediction_accuracy: 0.9911 - start__end__prediction_1_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "91/91 [==============================] - 30s 330ms/step - loss: 0.0151 - start__end__prediction_loss: 0.0120 - start__end__prediction_1_loss: 0.0031 - start__end__prediction_accuracy: 0.9933 - start__end__prediction_1_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "91/91 [==============================] - 30s 331ms/step - loss: 0.0214 - start__end__prediction_loss: 0.0187 - start__end__prediction_1_loss: 0.0028 - start__end__prediction_accuracy: 0.9933 - start__end__prediction_1_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "91/91 [==============================] - 30s 332ms/step - loss: 0.0225 - start__end__prediction_loss: 0.0213 - start__end__prediction_1_loss: 0.0012 - start__end__prediction_accuracy: 0.9956 - start__end__prediction_1_accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# It shows accuracy about 76% in the above\n",
        "# Now need to get f1 score as alternative\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "fine_preds = bert_model.predict(fine_train_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "jdrAPkfSLg-r",
        "outputId": "70e31b97-f0b5-43a4-8ac0-0a4e5043a2ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fine_start_indexes = np.argmax(fine_preds[0], axis=-1)\n",
        "fine_end_indexes = np.argmax(fine_preds[1], axis=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "BF355v_ULg8L",
        "outputId": "bfbc689c-2726-493e-dcb4-8645a7d10bc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing F1 score of two tokens (start, end)\n",
        "\n",
        "# Start token ('start_indexes') : 82%\n",
        "print(classification_report(fine_train_y[0], fine_start_indexes))\n",
        "# End token ('end_indexes') : 85%\n",
        "print(classification_report(fine_train_y[1], fine_end_indexes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NYYZClczLamF",
        "outputId": "138b6fac-1b34-4d04-b39a-7c90b4bb5a7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           8       1.00      1.00      1.00         4\n",
            "           9       1.00      1.00      1.00         3\n",
            "          11       1.00      1.00      1.00         3\n",
            "          12       1.00      1.00      1.00         2\n",
            "          14       1.00      1.00      1.00         2\n",
            "          15       1.00      1.00      1.00         4\n",
            "          16       1.00      1.00      1.00         5\n",
            "          17       1.00      1.00      1.00         1\n",
            "          18       1.00      1.00      1.00         1\n",
            "          19       1.00      1.00      1.00         5\n",
            "          20       1.00      1.00      1.00         2\n",
            "          21       1.00      1.00      1.00         6\n",
            "          22       1.00      1.00      1.00         2\n",
            "          24       1.00      1.00      1.00         1\n",
            "          25       1.00      1.00      1.00         2\n",
            "          26       1.00      1.00      1.00         1\n",
            "          27       1.00      1.00      1.00         3\n",
            "          28       1.00      1.00      1.00         1\n",
            "          29       1.00      1.00      1.00         1\n",
            "          30       1.00      1.00      1.00         1\n",
            "          31       1.00      1.00      1.00         6\n",
            "          32       1.00      1.00      1.00         3\n",
            "          33       1.00      0.86      0.92         7\n",
            "          34       1.00      1.00      1.00         2\n",
            "          35       1.00      1.00      1.00         3\n",
            "          36       1.00      1.00      1.00         1\n",
            "          37       1.00      1.00      1.00         1\n",
            "          38       1.00      1.00      1.00         1\n",
            "          39       1.00      1.00      1.00         2\n",
            "          40       1.00      1.00      1.00         1\n",
            "          41       1.00      1.00      1.00         2\n",
            "          42       1.00      1.00      1.00         4\n",
            "          43       1.00      1.00      1.00         5\n",
            "          44       1.00      1.00      1.00         3\n",
            "          45       1.00      1.00      1.00         5\n",
            "          46       1.00      1.00      1.00         5\n",
            "          47       1.00      1.00      1.00         4\n",
            "          48       1.00      1.00      1.00         1\n",
            "          49       1.00      1.00      1.00         8\n",
            "          50       1.00      1.00      1.00         2\n",
            "          52       1.00      1.00      1.00        10\n",
            "          53       1.00      1.00      1.00         5\n",
            "          54       0.86      1.00      0.92         6\n",
            "          55       1.00      1.00      1.00         3\n",
            "          56       1.00      0.75      0.86         4\n",
            "          57       1.00      1.00      1.00         4\n",
            "          58       1.00      1.00      1.00         7\n",
            "          59       1.00      1.00      1.00         1\n",
            "          60       1.00      1.00      1.00         6\n",
            "          61       1.00      1.00      1.00         2\n",
            "          62       1.00      1.00      1.00         2\n",
            "          63       1.00      1.00      1.00         5\n",
            "          64       1.00      1.00      1.00         3\n",
            "          65       1.00      0.86      0.92         7\n",
            "          66       0.67      1.00      0.80         2\n",
            "          67       1.00      1.00      1.00         6\n",
            "          68       1.00      1.00      1.00         3\n",
            "          69       1.00      1.00      1.00         6\n",
            "          70       1.00      1.00      1.00         4\n",
            "          71       1.00      1.00      1.00         5\n",
            "          72       1.00      1.00      1.00         7\n",
            "          73       1.00      1.00      1.00         1\n",
            "          74       1.00      1.00      1.00         3\n",
            "          75       1.00      1.00      1.00         2\n",
            "          76       1.00      1.00      1.00         2\n",
            "          77       1.00      0.67      0.80         3\n",
            "          78       1.00      1.00      1.00         7\n",
            "          79       0.80      1.00      0.89         4\n",
            "          80       1.00      1.00      1.00         2\n",
            "          81       1.00      1.00      1.00         2\n",
            "          82       1.00      1.00      1.00         5\n",
            "          83       0.80      1.00      0.89         4\n",
            "          86       0.67      1.00      0.80         2\n",
            "          87       1.00      0.60      0.75         5\n",
            "          88       1.00      1.00      1.00         3\n",
            "          89       1.00      1.00      1.00         4\n",
            "          90       1.00      1.00      1.00         5\n",
            "          91       1.00      1.00      1.00         2\n",
            "          93       1.00      1.00      1.00         5\n",
            "          94       0.80      1.00      0.89         4\n",
            "          95       1.00      1.00      1.00         3\n",
            "          96       1.00      1.00      1.00         2\n",
            "          97       1.00      1.00      1.00         3\n",
            "          98       1.00      1.00      1.00         1\n",
            "          99       1.00      1.00      1.00         2\n",
            "         100       1.00      0.67      0.80         3\n",
            "         101       1.00      1.00      1.00         4\n",
            "         102       1.00      1.00      1.00         4\n",
            "         103       1.00      1.00      1.00         8\n",
            "         104       1.00      1.00      1.00         4\n",
            "         105       1.00      1.00      1.00         2\n",
            "         107       1.00      1.00      1.00         1\n",
            "         108       1.00      0.60      0.75         5\n",
            "         109       0.71      1.00      0.83         5\n",
            "         110       1.00      1.00      1.00         3\n",
            "         111       1.00      1.00      1.00         2\n",
            "         112       1.00      1.00      1.00         1\n",
            "         113       0.67      1.00      0.80         2\n",
            "         114       1.00      1.00      1.00         3\n",
            "         115       1.00      1.00      1.00         5\n",
            "         116       1.00      1.00      1.00         2\n",
            "         117       1.00      1.00      1.00         1\n",
            "         118       1.00      1.00      1.00         2\n",
            "         119       0.00      0.00      0.00         1\n",
            "         120       1.00      1.00      1.00         5\n",
            "         122       1.00      1.00      1.00         2\n",
            "         123       1.00      1.00      1.00         1\n",
            "         124       1.00      1.00      1.00         1\n",
            "         125       1.00      1.00      1.00         3\n",
            "         126       1.00      1.00      1.00         1\n",
            "         127       1.00      1.00      1.00         4\n",
            "         128       1.00      1.00      1.00         3\n",
            "         129       1.00      1.00      1.00         1\n",
            "         130       0.50      1.00      0.67         1\n",
            "         131       1.00      1.00      1.00         1\n",
            "         132       1.00      1.00      1.00         1\n",
            "         133       1.00      1.00      1.00         1\n",
            "         134       1.00      0.50      0.67         2\n",
            "         135       1.00      1.00      1.00         2\n",
            "         136       1.00      1.00      1.00         1\n",
            "         137       1.00      1.00      1.00         1\n",
            "         138       1.00      1.00      1.00         1\n",
            "         139       1.00      1.00      1.00         1\n",
            "         140       1.00      1.00      1.00         1\n",
            "         142       0.75      1.00      0.86         3\n",
            "         143       1.00      0.50      0.67         2\n",
            "         144       1.00      1.00      1.00         1\n",
            "         145       1.00      1.00      1.00         1\n",
            "         146       1.00      1.00      1.00         1\n",
            "         147       1.00      1.00      1.00         3\n",
            "         148       1.00      1.00      1.00         2\n",
            "         149       1.00      1.00      1.00         1\n",
            "         152       1.00      1.00      1.00         3\n",
            "         153       1.00      1.00      1.00         1\n",
            "         156       1.00      1.00      1.00         1\n",
            "         158       1.00      1.00      1.00         2\n",
            "         159       1.00      0.67      0.80         3\n",
            "         162       1.00      1.00      1.00         1\n",
            "         163       1.00      1.00      1.00         2\n",
            "         164       1.00      1.00      1.00         1\n",
            "         168       1.00      1.00      1.00         2\n",
            "         170       1.00      1.00      1.00         1\n",
            "         173       1.00      1.00      1.00         1\n",
            "         174       1.00      1.00      1.00         1\n",
            "         176       1.00      1.00      1.00         1\n",
            "         178       1.00      1.00      1.00         1\n",
            "         180       1.00      1.00      1.00         1\n",
            "         181       1.00      1.00      1.00         1\n",
            "         182       1.00      1.00      1.00         1\n",
            "         184       1.00      1.00      1.00         1\n",
            "         189       1.00      1.00      1.00         1\n",
            "         192       1.00      1.00      1.00         1\n",
            "         195       1.00      1.00      1.00         1\n",
            "         197       1.00      1.00      1.00         2\n",
            "         199       1.00      1.00      1.00         2\n",
            "         203       1.00      1.00      1.00         2\n",
            "         204       1.00      1.00      1.00         1\n",
            "         205       1.00      1.00      1.00         2\n",
            "         206       1.00      1.00      1.00         1\n",
            "         209       1.00      1.00      1.00         2\n",
            "         210       1.00      1.00      1.00         1\n",
            "         216       1.00      1.00      1.00         1\n",
            "         219       1.00      1.00      1.00         2\n",
            "         225       1.00      1.00      1.00         1\n",
            "         227       1.00      1.00      1.00         1\n",
            "         228       0.00      0.00      0.00         0\n",
            "         234       0.50      1.00      0.67         1\n",
            "         236       0.00      0.00      0.00         0\n",
            "         237       1.00      1.00      1.00         1\n",
            "         262       1.00      1.00      1.00         1\n",
            "         276       1.00      1.00      1.00         1\n",
            "         287       0.00      0.00      0.00         1\n",
            "         313       1.00      1.00      1.00         1\n",
            "         323       1.00      1.00      1.00         1\n",
            "         325       1.00      1.00      1.00         1\n",
            "         334       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           0.97       451\n",
            "   macro avg       0.96      0.96      0.95       451\n",
            "weighted avg       0.98      0.97      0.97       451\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           8       1.00      1.00      1.00         2\n",
            "           9       1.00      1.00      1.00         1\n",
            "          10       1.00      1.00      1.00         1\n",
            "          11       1.00      1.00      1.00         2\n",
            "          12       1.00      1.00      1.00         2\n",
            "          14       1.00      1.00      1.00         3\n",
            "          15       1.00      1.00      1.00         4\n",
            "          16       1.00      1.00      1.00         4\n",
            "          17       1.00      1.00      1.00         2\n",
            "          18       1.00      1.00      1.00         1\n",
            "          19       1.00      1.00      1.00         2\n",
            "          20       1.00      1.00      1.00         3\n",
            "          21       1.00      1.00      1.00         1\n",
            "          22       1.00      1.00      1.00         5\n",
            "          23       1.00      1.00      1.00         3\n",
            "          24       1.00      1.00      1.00         3\n",
            "          25       1.00      1.00      1.00         1\n",
            "          26       1.00      1.00      1.00         2\n",
            "          28       1.00      1.00      1.00         4\n",
            "          29       1.00      1.00      1.00         2\n",
            "          30       1.00      1.00      1.00         1\n",
            "          31       1.00      1.00      1.00         3\n",
            "          32       1.00      1.00      1.00         2\n",
            "          33       1.00      0.75      0.86         4\n",
            "          34       1.00      1.00      1.00         4\n",
            "          35       1.00      1.00      1.00         6\n",
            "          37       1.00      1.00      1.00         1\n",
            "          39       1.00      1.00      1.00         1\n",
            "          40       1.00      1.00      1.00         2\n",
            "          41       1.00      1.00      1.00         2\n",
            "          42       1.00      1.00      1.00         4\n",
            "          43       1.00      1.00      1.00         5\n",
            "          44       1.00      1.00      1.00         4\n",
            "          45       1.00      1.00      1.00         5\n",
            "          46       1.00      1.00      1.00         5\n",
            "          47       1.00      1.00      1.00         1\n",
            "          48       1.00      1.00      1.00         3\n",
            "          49       1.00      1.00      1.00         4\n",
            "          50       1.00      1.00      1.00         5\n",
            "          51       1.00      1.00      1.00         1\n",
            "          52       1.00      1.00      1.00         4\n",
            "          53       1.00      1.00      1.00         7\n",
            "          54       1.00      1.00      1.00         3\n",
            "          55       1.00      1.00      1.00         4\n",
            "          56       1.00      1.00      1.00         5\n",
            "          57       1.00      1.00      1.00         4\n",
            "          58       1.00      1.00      1.00         4\n",
            "          59       1.00      1.00      1.00         4\n",
            "          60       1.00      1.00      1.00         4\n",
            "          61       1.00      1.00      1.00         1\n",
            "          62       1.00      1.00      1.00         3\n",
            "          63       1.00      1.00      1.00         5\n",
            "          64       1.00      1.00      1.00         2\n",
            "          65       1.00      1.00      1.00         6\n",
            "          66       1.00      1.00      1.00         5\n",
            "          67       1.00      1.00      1.00         6\n",
            "          68       1.00      1.00      1.00         2\n",
            "          69       1.00      1.00      1.00         3\n",
            "          70       1.00      1.00      1.00         3\n",
            "          71       1.00      1.00      1.00         3\n",
            "          72       1.00      1.00      1.00         7\n",
            "          73       1.00      1.00      1.00         2\n",
            "          74       1.00      1.00      1.00         4\n",
            "          75       1.00      1.00      1.00         2\n",
            "          76       1.00      1.00      1.00         2\n",
            "          77       1.00      1.00      1.00         2\n",
            "          78       0.50      1.00      0.67         1\n",
            "          79       1.00      1.00      1.00         3\n",
            "          80       1.00      1.00      1.00         3\n",
            "          81       1.00      1.00      1.00         2\n",
            "          82       1.00      1.00      1.00         2\n",
            "          83       1.00      1.00      1.00         5\n",
            "          84       1.00      1.00      1.00         4\n",
            "          85       1.00      1.00      1.00         3\n",
            "          86       1.00      1.00      1.00         2\n",
            "          87       1.00      1.00      1.00         5\n",
            "          88       1.00      1.00      1.00         1\n",
            "          89       1.00      1.00      1.00         3\n",
            "          90       1.00      1.00      1.00         6\n",
            "          91       1.00      1.00      1.00         3\n",
            "          92       1.00      1.00      1.00         2\n",
            "          93       1.00      1.00      1.00         3\n",
            "          94       1.00      1.00      1.00         2\n",
            "          95       1.00      1.00      1.00         1\n",
            "          97       1.00      1.00      1.00         6\n",
            "          99       1.00      1.00      1.00         5\n",
            "         100       1.00      1.00      1.00         6\n",
            "         101       1.00      1.00      1.00         4\n",
            "         102       1.00      1.00      1.00         5\n",
            "         103       1.00      0.80      0.89         5\n",
            "         104       0.83      1.00      0.91         5\n",
            "         105       1.00      1.00      1.00         2\n",
            "         106       1.00      1.00      1.00         2\n",
            "         107       1.00      1.00      1.00         3\n",
            "         108       1.00      1.00      1.00         5\n",
            "         109       1.00      1.00      1.00         4\n",
            "         110       1.00      1.00      1.00         2\n",
            "         111       1.00      1.00      1.00         7\n",
            "         114       1.00      1.00      1.00         1\n",
            "         115       1.00      1.00      1.00         3\n",
            "         116       1.00      1.00      1.00         2\n",
            "         117       1.00      1.00      1.00         2\n",
            "         118       1.00      1.00      1.00         5\n",
            "         119       1.00      1.00      1.00         1\n",
            "         120       1.00      0.67      0.80         3\n",
            "         121       1.00      1.00      1.00         3\n",
            "         123       1.00      1.00      1.00         2\n",
            "         124       1.00      1.00      1.00         1\n",
            "         125       1.00      1.00      1.00         1\n",
            "         126       1.00      1.00      1.00         1\n",
            "         127       1.00      1.00      1.00         5\n",
            "         128       1.00      1.00      1.00         1\n",
            "         129       1.00      1.00      1.00         1\n",
            "         130       1.00      1.00      1.00         5\n",
            "         131       1.00      1.00      1.00         4\n",
            "         132       1.00      1.00      1.00         1\n",
            "         133       1.00      1.00      1.00         1\n",
            "         134       1.00      1.00      1.00         1\n",
            "         135       1.00      1.00      1.00         2\n",
            "         137       1.00      1.00      1.00         1\n",
            "         138       0.00      0.00      0.00         0\n",
            "         139       1.00      1.00      1.00         2\n",
            "         140       1.00      1.00      1.00         1\n",
            "         141       1.00      1.00      1.00         3\n",
            "         142       1.00      1.00      1.00         2\n",
            "         143       1.00      1.00      1.00         1\n",
            "         144       1.00      1.00      1.00         1\n",
            "         145       1.00      1.00      1.00         1\n",
            "         146       1.00      1.00      1.00         1\n",
            "         147       1.00      1.00      1.00         2\n",
            "         148       1.00      1.00      1.00         3\n",
            "         151       1.00      1.00      1.00         1\n",
            "         152       1.00      1.00      1.00         1\n",
            "         153       1.00      1.00      1.00         2\n",
            "         154       1.00      0.50      0.67         2\n",
            "         157       0.00      0.00      0.00         1\n",
            "         158       1.00      1.00      1.00         3\n",
            "         159       1.00      1.00      1.00         2\n",
            "         161       1.00      1.00      1.00         3\n",
            "         162       1.00      1.00      1.00         3\n",
            "         164       1.00      1.00      1.00         2\n",
            "         165       1.00      1.00      1.00         1\n",
            "         167       1.00      1.00      1.00         1\n",
            "         168       1.00      1.00      1.00         1\n",
            "         170       1.00      1.00      1.00         2\n",
            "         172       1.00      1.00      1.00         1\n",
            "         173       0.50      1.00      0.67         1\n",
            "         174       1.00      0.50      0.67         2\n",
            "         176       1.00      1.00      1.00         1\n",
            "         178       0.00      0.00      0.00         0\n",
            "         184       0.00      0.00      0.00         1\n",
            "         187       1.00      1.00      1.00         1\n",
            "         189       1.00      1.00      1.00         1\n",
            "         190       1.00      1.00      1.00         1\n",
            "         192       1.00      1.00      1.00         2\n",
            "         196       1.00      1.00      1.00         2\n",
            "         197       1.00      1.00      1.00         1\n",
            "         202       1.00      1.00      1.00         1\n",
            "         204       1.00      1.00      1.00         1\n",
            "         205       1.00      1.00      1.00         1\n",
            "         206       1.00      1.00      1.00         3\n",
            "         210       1.00      1.00      1.00         1\n",
            "         211       1.00      1.00      1.00         1\n",
            "         215       1.00      1.00      1.00         1\n",
            "         217       1.00      1.00      1.00         1\n",
            "         224       1.00      1.00      1.00         3\n",
            "         225       1.00      1.00      1.00         1\n",
            "         226       0.00      0.00      0.00         0\n",
            "         227       1.00      1.00      1.00         1\n",
            "         230       0.50      1.00      0.67         1\n",
            "         237       1.00      1.00      1.00         2\n",
            "         240       1.00      1.00      1.00         2\n",
            "         243       1.00      1.00      1.00         1\n",
            "         262       1.00      1.00      1.00         1\n",
            "         281       1.00      1.00      1.00         1\n",
            "         299       1.00      1.00      1.00         1\n",
            "         323       1.00      1.00      1.00         1\n",
            "         326       1.00      1.00      1.00         1\n",
            "         335       1.00      1.00      1.00         1\n",
            "         339       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           0.98       451\n",
            "   macro avg       0.96      0.96      0.96       451\n",
            "weighted avg       0.99      0.98      0.99       451\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Fine_tuned trained model save\n",
        "# # Since it takes too much time, trained model saved in the 'gdrive'\n",
        "# path = \"gdrive/MyDrive/Colab Notebooks/squad\"\n",
        "# bert_model.save_weights(path+\"/(Uncased)Squad.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "o1BcwlvZLks_",
        "outputId": "79df7346-e009-4eba-dca4-5b1ba45a8798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Code for loading pretrained BERT and fine_tuned trained model\n",
        "# bert_model = get_bert_finetuning_model(model)\n",
        "# path = \"gdrive/MyDrive/Colab Notebooks/squad\"\n",
        "# bert_model.load_weights(path+\"/(Uncased)Squad.h5\")"
      ],
      "metadata": {
        "id": "3BvzR_cjLC5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Customized Dataset"
      ],
      "metadata": {
        "id": "IBZgY4YWu3fO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test dataset loading\n",
        "import pandas as pd\n",
        "test_context = pd.read_excel('Test_QA_OneByOne.xlsx')\n",
        "test_question = pd.read_excel('Question.xlsx')"
      ],
      "metadata": {
        "id": "A7ZzHKgAu50G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "8e13e204-7f29-4adb-b155-9904a730ef58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_context"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Yv2juRnzwVs4",
        "outputId": "0b52b558-dd6d-4911-94e5-52eeff47492b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b9fd6e54-ffcf-4d6c-a520-b7eae82fbfe9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Index</th>\n",
              "      <th>Paper</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Target</th>\n",
              "      <th>Method In-Vivo</th>\n",
              "      <th>Method In-Vitro</th>\n",
              "      <th>Method Others</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>371</td>\n",
              "      <td>Ginkgo biloba prevents mobile phone-induced ox...</td>\n",
              "      <td>Background: The widespread use of mobile phone...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>372</td>\n",
              "      <td>Effects of radiofrequency exposure on the GABA...</td>\n",
              "      <td>The widespread use of cellular phones raises t...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>376</td>\n",
              "      <td>Long-term exposure to electromagnetic radiatio...</td>\n",
              "      <td>The aim of the present study was the investiga...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>377</td>\n",
              "      <td>Effect of mobile phone signal radiation on epi...</td>\n",
              "      <td>Exponential increase in mobile phone uses, giv...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>378</td>\n",
              "      <td>Effects of a Single Head Exposure to GSM-1800 ...</td>\n",
              "      <td>Mobile communications are propagated by electr...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>379</td>\n",
              "      <td>Melatonin Modulates NMDA-Receptor 2B/Calpain-1...</td>\n",
              "      <td>Aim: To investigate the potential protective e...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>382</td>\n",
              "      <td>Hippocampal lipidome and transcriptome profile...</td>\n",
              "      <td>Background: The widespread use of wireless dev...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>383</td>\n",
              "      <td>Exposure to 835 MHz RF-EMF decreases the expre...</td>\n",
              "      <td>The exponential increase in the use of mobile ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>386</td>\n",
              "      <td>Effects of 2G mobile phone exposure on both be...</td>\n",
              "      <td>Mobile communications are expanded day by day ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>387</td>\n",
              "      <td>Evaluation of bax, bcl-2, p21 and p53 genes ex...</td>\n",
              "      <td>Objectives: The increasing rate of over using ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>388</td>\n",
              "      <td>The effect of 1800MHz radio-frequency radiatio...</td>\n",
              "      <td>Background: The aim of this study was to evalu...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>389</td>\n",
              "      <td>Effects of acute and chronic exposure to both ...</td>\n",
              "      <td>Purpose: To demonstrate the molecular effects ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>390</td>\n",
              "      <td>Proteomic analysis of continuous 900-MHz radio...</td>\n",
              "      <td>Although cell phones have been used worldwide,...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>391</td>\n",
              "      <td>Activation of autophagy at cerebral cortex and...</td>\n",
              "      <td>With the explosive increase in exposure to rad...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>393</td>\n",
              "      <td>Long-term exposure to 835 MHz RF-EMF induces h...</td>\n",
              "      <td>Radiofrequency electromagnetic field (RF-EMF) ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>394</td>\n",
              "      <td>Study of potential health effects of electroma...</td>\n",
              "      <td>The objective of this study is to investigate ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>395</td>\n",
              "      <td>Mobile-phone radiation-induced perturbation of...</td>\n",
              "      <td>The daily use by people of wireless communicat...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>401</td>\n",
              "      <td>Long term and excessive use of 900 MHz radiofr...</td>\n",
              "      <td>Purpose: We still do not have any information ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>404</td>\n",
              "      <td>Analysis of rat testicular proteome following ...</td>\n",
              "      <td>The use of electromagnetic field (EMF) generat...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>406</td>\n",
              "      <td>Effect of 3G Cell Phone Exposure with Computer...</td>\n",
              "      <td>Cell phone radiation exposure and its biologic...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>415</td>\n",
              "      <td>Brain proteome response following whole body e...</td>\n",
              "      <td>The objective of this study was to investigate...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>417</td>\n",
              "      <td>Effects of electromagnetic radiation produced ...</td>\n",
              "      <td>Objective: The effects of electromagnetic radi...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>424</td>\n",
              "      <td>Qualitative Effect on mRNAs of Injury-Associat...</td>\n",
              "      <td>Rats were exposed to cell phone radiation for ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>431</td>\n",
              "      <td>Upregulation of specific mRNA levels in rat br...</td>\n",
              "      <td>Adult Sprague-Dawley rats were exposed to regu...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>433</td>\n",
              "      <td>No evidence of major transcriptional changes i...</td>\n",
              "      <td>To analyze possible effects of microwaves on g...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>446</td>\n",
              "      <td>Exposure of rat brain to 915 MHz GSM microwave...</td>\n",
              "      <td>We investigated whether exposure of rat brain ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>449</td>\n",
              "      <td>Expression of the immediate early gene, c-fos,...</td>\n",
              "      <td>Aims: To study the effect of acute exposure to...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>453</td>\n",
              "      <td>Bone morphogenetic protein expression in newbo...</td>\n",
              "      <td>Effects of nonthermal radiofrequency radiation...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>455</td>\n",
              "      <td>IRIDIUM exposure increases c-fos expression in...</td>\n",
              "      <td>With the rapid development of wireless communi...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b9fd6e54-ffcf-4d6c-a520-b7eae82fbfe9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b9fd6e54-ffcf-4d6c-a520-b7eae82fbfe9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b9fd6e54-ffcf-4d6c-a520-b7eae82fbfe9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    Index  ... Method Others\n",
              "0     371  ...             0\n",
              "1     372  ...             0\n",
              "2     376  ...             0\n",
              "3     377  ...             0\n",
              "4     378  ...             0\n",
              "5     379  ...             0\n",
              "6     382  ...             0\n",
              "7     383  ...             0\n",
              "8     386  ...             0\n",
              "9     387  ...             0\n",
              "10    388  ...             0\n",
              "11    389  ...             0\n",
              "12    390  ...             0\n",
              "13    391  ...             0\n",
              "14    393  ...             0\n",
              "15    394  ...             0\n",
              "16    395  ...             0\n",
              "17    401  ...             0\n",
              "18    404  ...             0\n",
              "19    406  ...             0\n",
              "20    415  ...             0\n",
              "21    417  ...             0\n",
              "22    424  ...             0\n",
              "23    431  ...             0\n",
              "24    433  ...             0\n",
              "25    446  ...             0\n",
              "26    449  ...             0\n",
              "27    453  ...             0\n",
              "28    455  ...             0\n",
              "\n",
              "[29 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_temp = test_context['Abstract']\n",
        "test_context = df_temp.to_frame()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "YmpqWv-dGzH2",
        "outputId": "f1c38ff1-a64c-4054-e7e9-367c55b9766f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_context"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 996
        },
        "id": "Pl8nGxM1QUAE",
        "outputId": "ebac3d99-f7f0-49a4-ffda-d1e180c633bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-70b6096c-a2bd-46c3-a99b-0976e308cea2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Background: The widespread use of mobile phone...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The widespread use of cellular phones raises t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The aim of the present study was the investiga...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Exponential increase in mobile phone uses, giv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Mobile communications are propagated by electr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Aim: To investigate the potential protective e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Background: The widespread use of wireless dev...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>The exponential increase in the use of mobile ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Mobile communications are expanded day by day ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Objectives: The increasing rate of over using ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Background: The aim of this study was to evalu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Purpose: To demonstrate the molecular effects ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Although cell phones have been used worldwide,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>With the explosive increase in exposure to rad...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Radiofrequency electromagnetic field (RF-EMF) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>The objective of this study is to investigate ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>The daily use by people of wireless communicat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Purpose: We still do not have any information ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>The use of electromagnetic field (EMF) generat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Cell phone radiation exposure and its biologic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>The objective of this study was to investigate...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Objective: The effects of electromagnetic radi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Rats were exposed to cell phone radiation for ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Adult Sprague-Dawley rats were exposed to regu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>To analyze possible effects of microwaves on g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>We investigated whether exposure of rat brain ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>Aims: To study the effect of acute exposure to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Effects of nonthermal radiofrequency radiation...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>With the rapid development of wireless communi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-70b6096c-a2bd-46c3-a99b-0976e308cea2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-70b6096c-a2bd-46c3-a99b-0976e308cea2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-70b6096c-a2bd-46c3-a99b-0976e308cea2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                             Abstract\n",
              "0   Background: The widespread use of mobile phone...\n",
              "1   The widespread use of cellular phones raises t...\n",
              "2   The aim of the present study was the investiga...\n",
              "3   Exponential increase in mobile phone uses, giv...\n",
              "4   Mobile communications are propagated by electr...\n",
              "5   Aim: To investigate the potential protective e...\n",
              "6   Background: The widespread use of wireless dev...\n",
              "7   The exponential increase in the use of mobile ...\n",
              "8   Mobile communications are expanded day by day ...\n",
              "9   Objectives: The increasing rate of over using ...\n",
              "10  Background: The aim of this study was to evalu...\n",
              "11  Purpose: To demonstrate the molecular effects ...\n",
              "12  Although cell phones have been used worldwide,...\n",
              "13  With the explosive increase in exposure to rad...\n",
              "14  Radiofrequency electromagnetic field (RF-EMF) ...\n",
              "15  The objective of this study is to investigate ...\n",
              "16  The daily use by people of wireless communicat...\n",
              "17  Purpose: We still do not have any information ...\n",
              "18  The use of electromagnetic field (EMF) generat...\n",
              "19  Cell phone radiation exposure and its biologic...\n",
              "20  The objective of this study was to investigate...\n",
              "21  Objective: The effects of electromagnetic radi...\n",
              "22  Rats were exposed to cell phone radiation for ...\n",
              "23  Adult Sprague-Dawley rats were exposed to regu...\n",
              "24  To analyze possible effects of microwaves on g...\n",
              "25  We investigated whether exposure of rat brain ...\n",
              "26  Aims: To study the effect of acute exposure to...\n",
              "27  Effects of nonthermal radiofrequency radiation...\n",
              "28  With the rapid development of wireless communi..."
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_question"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "6-fvGGU5QVa_",
        "outputId": "25c1187e-5509-433a-f552-6663fada70c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-5aed0822-9938-4844-909a-ab442617eb16\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What animal has been used?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How many animals were used?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What is the signal frequency?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How much W/kg was used?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5aed0822-9938-4844-909a-ab442617eb16')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5aed0822-9938-4844-909a-ab442617eb16 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5aed0822-9938-4844-909a-ab442617eb16');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                        Question\n",
              "0     What animal has been used?\n",
              "1    How many animals were used?\n",
              "2  What is the signal frequency?\n",
              "3        How much W/kg was used?"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "8GRLlRTwSrOA",
        "outputId": "0d3aef0a-82bd-47ee-c545-3b091dd9b45c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copied & Pasted, and a bit modified from the above function\n",
        "# Define a function calculating the length of answer in context dataset all at once\n",
        "def convert_data(test_context, test_question):\n",
        "  global tokenizer\n",
        "  indices, segments = [], []\n",
        "\n",
        "  que, _ = tokenizer.encode(test_question['Question'])\n",
        "  doc, _ = tokenizer.encode(test_context['Abstract'])\n",
        "  doc.pop(0)\n",
        "\n",
        "  que_len = len(que)\n",
        "  doc_len = len(doc)\n",
        "\n",
        "  # 1. Length of question\n",
        "  # The question is cut by the length of 64\n",
        "  if que_len > 64:\n",
        "    que = que[:63]\n",
        "    que.append(102) # [SEP] token added to make it clear the question block\n",
        "  \n",
        "  # 2. Total length of question and context\n",
        "  # The total input is cut by the length of 384\n",
        "  if len(que+doc) > SEQ_LEN:\n",
        "    while len(que+doc) != SEQ_LEN:\n",
        "      doc.pop(-1)\n",
        "    doc.pop(-1)\n",
        "    doc.append(102)\n",
        "\n",
        "    # Segment embedding\n",
        "    # Question : 0 / Context 1 / Padding : 0 (remaining part for short sentences)\n",
        "        \n",
        "    ############################\n",
        "    ###### Segment 예시 ########\n",
        "    ############################\n",
        "    \n",
        "    # question, context, padding\n",
        "    # 00000000, 1111111, 0000000\n",
        "    \n",
        "  segment = [0]*len(que) + [1]*len(doc) + [0]*(SEQ_LEN-len(que)-len(doc))\n",
        "\n",
        "  # Padding\n",
        "  if len(que+doc) <= SEQ_LEN:\n",
        "    while len(que+doc) != SEQ_LEN:\n",
        "      doc.append(0)\n",
        "\n",
        "  # Final Input 'ids' (Question + Context)\n",
        "  ids = que + doc\n",
        "\n",
        "  # Input(ids), Segment saving into list type (indices, segments)\n",
        "  indices.append(ids)\n",
        "  segments.append(segment)\n",
        "\n",
        "  # Converting the 4 lists into numpy array\n",
        "  indices = np.array(indices)\n",
        "  segments = np.array(segments)\n",
        "  \n",
        "  # print(indices, segments)\n",
        "  return [indices, segments]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "DMwTMIjcwWZw",
        "outputId": "78dfc6db-c635-4303-80a9-6247774e88cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert_data(test_question.iloc[0], test_context[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "-aSljit0_OcV",
        "outputId": "f1bca43e-6eaf-41e1-b5c4-5932018660a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_letter(df_context, df_question):\n",
        "  test_input = convert_data(df_context, df_question)\n",
        "  test_start, test_end = bert_model.predict(test_input)\n",
        "\n",
        "  indexes = test_input[0].tolist()[0]\n",
        "  start = np.argmax(test_start, axis=1).item()\n",
        "  end = np.argmax(test_end, axis=1).item()\n",
        "  start_tok = indexes[start]\n",
        "  end_tok = indexes[end]\n",
        "\n",
        "  # # print(\"Context: \", end = \" \")\n",
        "  # print(\"Context\")\n",
        "  # print(\"-\"*100)\n",
        "\n",
        "  # def split_text(text, n):\n",
        "  #   for line in text.splitlines():\n",
        "  #     while len(line) > n:\n",
        "  #       x, line = line[:n], line[n:]\n",
        "  #       yield x\n",
        "  #     yield line\n",
        "  \n",
        "  # for line in split_text(df_context['Abstract'], 100):\n",
        "  #   print(line)\n",
        "\n",
        "  # print(\"-\"*100)\n",
        "  \n",
        "  print(\"Question: \", df_question['Question'])\n",
        "  print(\"ANSWER: \", end = \" \")\n",
        "  # print(\"\\n\")\n",
        "  sentences = []\n",
        "\n",
        "  for i in range(start, end+1):\n",
        "    token_based_word = reverse_token_dict[indexes[i]]\n",
        "    sentences.append(token_based_word)\n",
        "    # print(token_based_word, end= \" \")\n",
        "  # print(\"\\n\")\n",
        "  # print(\"Untokenized Answer: \", end= \"\")\n",
        "  \n",
        "  for w in sentences:\n",
        "    if w.startswith(\"##\"):\n",
        "      w = w.replace(\"##\", \"\")\n",
        "    else:\n",
        "      w = \" \" + w\n",
        "    print(w, end=\"\")\n",
        "  # print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "aMjhGuIZXDtA",
        "outputId": "74d372c5-810f-475f-863a-846b32c96630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  \n",
        "  print(\"Context [\", i+1, \"]\")\n",
        "  print(\"-\"*100)\n",
        "  def split_text(text, n):\n",
        "    for line in text.splitlines():\n",
        "      while len(line) > n:\n",
        "        x, line = line[:n], line[n:]\n",
        "        yield x\n",
        "      yield line\n",
        "  \n",
        "  for line in split_text(test_context.iloc[i]['Abstract'], 100):\n",
        "    print(line)\n",
        "\n",
        "  print(\"-\"*100)\n",
        "\n",
        "  for j in range(4):\n",
        "  # answers = test['answers'][i]\n",
        "    predict_letter(test_context.iloc[i], test_question.iloc[j])\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "  print(\"\")\n",
        "  print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CjQq9VXPXRJp",
        "outputId": "cccff747-d23a-4952-fabf-67db24931d66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context [ 1 ]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Background: The widespread use of mobile phones (MP) in recent years has raised the research activit\n",
            "ies in many countries to determine the consequences of exposure to the low-intensity electromagnetic\n",
            " radiation (EMR) of mobile phones. Since several experimental studies suggest a role of reactive oxy\n",
            "gen species (ROS) in EMR-induced oxidative damage in tissues, in this study, we investigated the eff\n",
            "ect of Ginkgo biloba (Gb) on MP-induced oxidative damage in brain tissue of rats.\n",
            "\n",
            "Methods: Rats (EMR+) were exposed to 900 MHz EMR from MP for 7 days (1 h/day). In the EMR+Gb groups,\n",
            " rats were exposed to EMR and pretreated with Gb. Control and Gb-administrated groups were produced \n",
            "by turning off the mobile phone while the animals were in the same exposure conditions. Subsequently\n",
            ", oxidative stress markers and pathological changes in brain tissue were examined for each groups.\n",
            "\n",
            "Results: Oxidative damage was evident by the: (i) increase in malondialdehyde (MDA) and nitric oxide\n",
            " (NO) levels in brain tissue, (ii) decrease in brain superoxide dismutase (SOD) and glutathione pero\n",
            "xidase (GSH-Px) activities and (iii) increase in brain xanthine oxidase (XO) and adenosine deaminase\n",
            " (ADA) activities. These alterations were prevented by Gb treatment. Furthermore, Gb prevented the M\n",
            "P-induced cellular injury in brain tissue histopathologically.\n",
            "\n",
            "Conclusion: Reactive oxygen species may play a role in the mechanism that has been proposed to expla\n",
            "in the biological side effects of MP, and Gb prevents the MP-induced oxidative stress to preserve an\n",
            "tioxidant enzymes activity in brain tissue.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question:  What animal has been used?\n",
            "ANSWER:   rats\n",
            "\n",
            "Question:  How many animals were used?\n",
            "ANSWER:   rats ( emr + ) were exposed to 900 mhz emr from mp for 7 days ( 1 h / day ) .\n",
            "\n",
            "Question:  What is the signal frequency?\n",
            "ANSWER:   900 mhz\n",
            "\n",
            "Question:  How much W/kg was used?\n",
            "ANSWER:   rats . methods : rats ( emr + ) were exposed to 900 mhz emr from mp for 7 days ( 1 h / day )\n",
            "\n",
            "\n",
            "\n",
            "Context [ 2 ]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "The widespread use of cellular phones raises the problem of interaction of electromagnetic fields wi\n",
            "th the central nervous system (CNS). In order to measure these effects on neurotransmitter content i\n",
            "n the CNS, we developed a protocol of neurotransmitter detection based on immunohistochemistry and i\n",
            "mage analysis. Gamma-vinyl-GABA (GVG), an inhibitor of the GABA-transaminase was injected in rats to\n",
            " increase GABA concentration in the CNS. The cellular GABA contents were then revealed by immunohist\n",
            "ochemistry and semi-quantified by image analysis thanks to three parameters: optical density (O.D.),\n",
            " staining area, and number of positive cells. The increase in cerebellar GABA content induced by GVG\n",
            " 1200 mg/kg was reflected in these three parameters in the molecular and the granular layers. Theref\n",
            "ore, control of immunohistochemistry parameters, together with appropriate image analysis, allowed b\n",
            "oth the location and the detection of variations in cellular neurotransmitter content. This protocol\n",
            " was used to investigate the effects of exposure to 900 MHz radiofrequencies on cerebellar GABA cont\n",
            "ent. Both pulsed emission with a specific absorption rate (SAR) of 4 W/kg and continuous emission wi\n",
            "th high SAR (32 W/kg) were tested. We observed a selective diminution of the stained processes area \n",
            "in the Purkinje cell layer after exposure to pulsed radiofrequency and, in addition, a decrease in O\n",
            ".D. in the three cell layers after exposure to continuous waves. Whether this effect is, at least pa\n",
            "rtly, due to a local heating of the tissues is not known. Overall, it appears that high energetic ra\n",
            "diofrequency exposure induces a diminution in cellular GABA content in the cerebellum.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question:  What animal has been used?\n",
            "ANSWER:   rats\n",
            "\n",
            "Question:  How many animals were used?\n",
            "ANSWER:   gamma - vinyl - gaba ( gvg ) , an inhibitor of the gaba - transaminase was injected in rats to increase gaba concentration in the cns . the cellular gaba contents were then revealed by immunohistochemistry and semi - quantified by image analysis thanks to three parameters : optical density ( o . d . ) , staining area , and number of positive cells . the increase in cerebellar gaba content induced by gvg 1200 mg / kg was reflected in these three parameters in the molecular and the granular layers . therefore , control of immunohistochemistry parameters , together with appropriate image analysis , allowed both the location and the detection of variations in cellular neurotransmitter content . this protocol was used to investigate the effects of exposure to 900 mhz radiofrequencies on cerebellar gaba content . both pulsed emission with a specific absorption rate ( sar ) of 4 w / kg and continuous emission with high sar ( 32 w / kg )\n",
            "\n",
            "Question:  What is the signal frequency?\n",
            "ANSWER:   900 mhz\n",
            "\n",
            "Question:  How much W/kg was used?\n",
            "ANSWER:   4 w / kg\n",
            "\n",
            "\n",
            "\n",
            "Context [ 3 ]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "The aim of the present study was the investigation of the effects of mobile phones at different dail\n",
            "y exposure times on the hippocampal expression of two apoptotic genes. Forty-eight male BALB/c mice \n",
            "were randomly divided into six groups with 8 animals in each group. Four experimental groups were re\n",
            "spectively exposed to electromagnetic waves for 0.5, 1, 2 and 4 hours twice a day for 30 consecutive\n",
            " days. One experimental group was radiated for 4 hours once a day, while the control group did not r\n",
            "eceive any radiation during the experiment. The expression of both Bax and Bcl2 mRNAs was upregulate\n",
            "d in the mice exposed for one and two hours. Whilst the highest expressions were observed in the two\n",
            "-hours radiation in the exposed group, the expression of both studied genes was downregulated in ani\n",
            "mals with longer exposure to radiation in a duration-dependent manner. The highest ratio of Bax/Bcl2\n",
            " expression was observed in the mice that received radiation for four hours twice a day. These resul\n",
            "ts revealed that mobile phone radiation can cause considerable changes in the balance of Bax/Bcl2 mR\n",
            "NA expression in laboratory mice hippocampus.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question:  What animal has been used?\n",
            "ANSWER:   balb / c mice\n",
            "\n",
            "Question:  How many animals were used?\n",
            "ANSWER:   forty - eight\n",
            "\n",
            "Question:  What is the signal frequency?\n",
            "ANSWER:  \n",
            "\n",
            "Question:  How much W/kg was used?\n",
            "ANSWER:   forty - eight\n",
            "\n",
            "\n",
            "\n",
            "Context [ 4 ]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Exponential increase in mobile phone uses, given rise to public concern regarding the alleged delete\n",
            "rious health hazards as a consequence of prolonged exposure. In 2018, the U.S. National toxicology p\n",
            "rogram reported, two year toxicological studies for potential health hazards from exposure to cell p\n",
            "hone radiations. Epigenetic modulations play a critical regulatory role in many cellular functions a\n",
            "nd pathological conditions. In this study, we assessed the dose-dependent and frequency-dependent ep\n",
            "igenetic modulation (DNA and Histone methylation) in the hippocampus of Wistar rats. A Total of 96 m\n",
            "ale Wistar rats were segregated into 12 groups exposed to 900 MHz, 1800 MHz and 2450 MHz RF-MW at a \n",
            "specific absorption rate (SAR) of 5.84 × 10-4 W/kg, 5.94 × 10-4 W/kg and 6.4 × 10-4 W/kg respectivel\n",
            "y for 2 h per day for 1-month, 3-month and 6-month periods. At the end of the exposure duration, ani\n",
            "mals were sacrificed to collect the hippocampus. Global hippocampal DNA methylation and histone meth\n",
            "ylation were estimated by ELISA. However, DNA methylating enzymes, DNA methyltransferase1 (DNMT1) an\n",
            "d histone methylating enzymes euchromatic histone methylthransferase1 (EHMT1) expression was evaluat\n",
            "ed by real-time PCR, as well as further validated with Western blot. Alteration in epigenetic modula\n",
            "tion was observed in the hippocampus. Global DNA methylation was decreased and histone methylation w\n",
            "as increased in the hippocampus. We observed that microwave exposure led to significant epigenetic m\n",
            "odulations in the hippocampus with increasing frequency and duration of exposure. Microwave exposure\n",
            " with increasing frequency and exposure duration brings significant (p < 0.05) epigenetic modulation\n",
            "s which alters gene expression in the hippocampus.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question:  What animal has been used?\n",
            "ANSWER:   wistar rats\n",
            "\n",
            "Question:  How many animals were used?\n",
            "ANSWER:   96\n",
            "\n",
            "Question:  What is the signal frequency?\n",
            "ANSWER:   900 mhz , 1800 mhz and 2450 mhz\n",
            "\n",
            "Question:  How much W/kg was used?\n",
            "ANSWER:   5 . 84 × 10 - 4 w / kg\n",
            "\n",
            "\n",
            "\n",
            "Context [ 5 ]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Mobile communications are propagated by electromagnetic fields (EMFs), and since the 1990s, they ope\n",
            "rate with pulse-modulated signals such as the GSM-1800 MHz. The biological effects of GSM-EMF in hum\n",
            "ans affected by neuropathological processes remain seldom investigated. In this study, a 2-h head-on\n",
            "ly exposure to GSM-1800 MHz was applied to (i) rats undergoing an acute neuroinflammation triggered \n",
            "by a lipopolysaccharide (LPS) treatment, (ii) age-matched healthy rats, or (iii) transgenic hSOD1G93\n",
            "A rats that modeled a presymptomatic phase of human amyotrophic lateral sclerosis (ALS). Gene respon\n",
            "ses were assessed 24 h after the GSM head-only exposure in a motor area of the cerebral cortex (mCx)\n",
            " where the mean specific absorption rate (SAR) was estimated to be 3.22 W/kg. In LPS-treated rats, a\n",
            " genome-wide mRNA profiling was performed by RNA-seq analysis and revealed significant (adjusted p v\n",
            "alue < 0.05) but moderate (fold changes < 2) upregulations or downregulations affecting 2.7% of the \n",
            "expressed genes, including genes expressed predominantly in neuronal or in glial cell types and grou\n",
            "ps of genes involved in protein ubiquitination or dephosphorylation. Reverse transcription-quantitat\n",
            "ive PCR analyses confirmed gene modulations uncovered by RNA-seq data and showed that in a set of 15\n",
            " PCR-assessed genes, significant gene responses to GSM-1800 MHz depended upon the acute neuroinflamm\n",
            "atory state triggered in LPS-treated rats, because they were not observed in healthy or in hSOD1G93A\n",
            " rats. Together, our data specify the extent of cortical gene modulations triggered by GSM-EMF in th\n",
            "e course of an acute neuroinflammation and indicate that GSM-induced gene responses can differ accor\n",
            "ding to pathologies affecting the CNS.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question:  What animal has been used?\n",
            "ANSWER:   rats\n",
            "\n",
            "Question:  How many animals were used?\n",
            "ANSWER:   ( i ) rats undergoing an acute neuroinflammation triggered by a lipopolysaccharide ( lps ) treatment , ( ii ) age - matched healthy rats , or ( iii ) transgenic hsod1g93a rats\n",
            "\n",
            "Question:  What is the signal frequency?\n",
            "ANSWER:   gsm - 1800 mhz\n",
            "\n",
            "Question:  How much W/kg was used?\n",
            "ANSWER:   3 . 22 w / kg\n",
            "\n",
            "\n",
            "\n",
            "Context [ 6 ]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Aim: To investigate the potential protective effects of melatonin on the chronic radiation emitted b\n",
            "y third generation mobile phones on the brain.\n",
            "\n",
            "Material and methods: A total of 24 male Wistar albino rats were divided into four equal groups. Thr\n",
            "oughout a 90-day experiment, no application was performed on the control group. The second group was\n",
            " exposed to 2100 MHz radiation for 30 minutes. Subcutaneous melatonin was injected into the third gr\n",
            "oup. Subcutaneous melatonin injection was applied 40 minutes before radiation and then the fourth gr\n",
            "oup was exposed to radiation for 30 minutes. At the end of the experiment, brain (cerebrum and cereb\n",
            "ellum) tissues were taken from the subjects. Histochemical, immunohistochemical, ultrastructural and\n",
            " western blot analyses were applied. In addition to brain weight, Purkinje cellsâ€™ number, immunohi\n",
            "stochemical H Score analyses and the results of the Western blot were examined statistically.\n",
            "\n",
            "Results: With the application of radiation, neuronal edema, relatively-decreased numbers of neurons \n",
            "on hippocampal CA1 and CA3 regions, displacement of the Purkinje neurons and dark neurons findings w\n",
            "ere observed as a result of histochemical stainings. Radiation also activated the NMDA-receptor 2B/C\n",
            "alpain-1/Caspase-12 pathway, NMDA-receptor 2B and Calpain-1 with the findings being supported by wes\n",
            "tern blot analyses. Pre-increased protein synthesis before apoptosis was identified by electron micr\n",
            "oscopy.\n",
            "\n",
            "Conclusion: Mobile phone radiation caused certain (ultra) structural changes on the brain and activa\n",
            "ted the NMDA-receptor 2B/ Calpain-1/Caspase-12 pathway; in addition, melatonin was found to be effec\n",
            "tive, but insufficient in demonstrating the protective effects.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question:  What animal has been used?\n",
            "ANSWER:   wistar albino rats\n",
            "\n",
            "Question:  How many animals were used?\n",
            "ANSWER:   24\n",
            "\n",
            "Question:  What is the signal frequency?\n",
            "ANSWER:   2100 mhz\n",
            "\n",
            "Question:  How much W/kg was used?\n",
            "ANSWER:   40 minutes before radiation and then the fourth group was exposed to radiation for 30 minutes .\n",
            "\n",
            "\n",
            "\n",
            "Context [ 7 ]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Background: The widespread use of wireless devices during the last decades is raising concerns about\n",
            " adverse health effects of the radiofrequency electromagnetic radiation (RF-EMR) emitted from these \n",
            "devices. Recent research is focusing on unraveling the underlying mechanisms of RF-EMR and potential\n",
            " cellular targets. The \"omics\" high-throughput approaches are powerful tools to investigate the glob\n",
            "al effects of RF-EMR on cellular physiology.\n",
            "\n",
            "Methods: In this work, C57BL/6 adult male mice were whole-body exposed (nExp = 8) for 2 hr to GSM 18\n",
            "00 MHz mobile phone radiation at an average electric field intensity range of 4.3-17.5 V/m or sham-e\n",
            "xposed (nSE = 8), and the RF-EMR effects on the hippocampal lipidome and transcriptome profiles were\n",
            " assessed 6 hr later.\n",
            "\n",
            "Results: The data analysis of the phospholipid fatty acid residues revealed that the levels of four \n",
            "fatty acids [16:0, 16:1 (6c + 7c), 18:1 9c, eicosapentaenoic acid omega-3 (EPA, 20:5 ω3)] and the tw\n",
            "o fatty acid sums of saturated and monounsaturated fatty acids (SFA and MUFA) were significantly alt\n",
            "ered (p < 0.05) in the exposed group. The observed changes indicate a membrane remodeling response o\n",
            "f the tissue phospholipids after nonionizing radiation exposure, reducing SFA and EPA, while increas\n",
            "ing MUFA residues. The microarray data analysis demonstrated that the expression of 178 genes change\n",
            "d significantly (p < 0.05) between the two groups, revealing an impact on genes involved in critical\n",
            " biological processes, such as cell cycle, DNA replication and repair, cell death, cell signaling, n\n",
            "ervous system development and function, immune system response, lipid metabolism, and carcinogenesis\n",
            ".\n",
            "\n",
            "Conclusions: This study provides preliminary evidence that mobile phone radiation induces hippocampa\n",
            "l lipidome and transcriptome changes that may explain the brain proteome changes and memory deficits\n",
            " previously shown by our group.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question:  What animal has been used?\n",
            "ANSWER:   c57bl / 6 adult male mice\n",
            "\n",
            "Question:  How many animals were used?\n",
            "ANSWER:   c57bl / 6\n",
            "\n",
            "Question:  What is the signal frequency?\n",
            "ANSWER:   1800 mhz\n",
            "\n",
            "Question:  How much W/kg was used?\n",
            "ANSWER:   c57bl / 6 adult male mice were whole - body exposed ( nexp = 8 ) for 2 hr to gsm 1800 mhz mobile phone radiation at an average electric field intensity range of 4 . 3 - 17 . 5 v / m or sham - exposed ( nse = 8 )\n",
            "\n",
            "\n",
            "\n",
            "Context [ 8 ]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "The exponential increase in the use of mobile communication has triggered public concerns about the \n",
            "potential adverse effects of radiofrequency electromagnetic fields (RF-EMF) emitted by mobile phones\n",
            " on the central nervous system (CNS). In this study, we explored the relationship between calcium ch\n",
            "annels and apoptosis or autophagy in the hippocampus of C57BL/6 mice after RF-EMF exposure with a sp\n",
            "ecific absorption rate (SAR) of 4.0 W/kg for 4 weeks. Firstly, the expression level of voltage-gated\n",
            " calcium channels (VGCCs), a key regulator of the entry of calcium ions into the cell, was confirmed\n",
            " by immunoblots. We investigated and confirmed that pan-calcium channel expression in hippocampal ne\n",
            "urons were significantly decreased after exposure to RF-EMF. With the observed accumulation of autol\n",
            "ysosomes in hippocampal neurons via TEM, the expressions of autophagy-related genes and proteins (e.\n",
            "g., LC3B-II) had significantly increased. However, down-regulation of the apoptotic pathway may cont\n",
            "ribute to the decrease in calcium channel expression, and thus lower levels of calcium in hippocampa\n",
            "l neurons. These results suggested that exposure of RF-EMF could alter intracellular calcium homeost\n",
            "asis by decreasing calcium channel expression in the hippocampus; presumably by activating the autop\n",
            "hagy pathway, while inhibiting apoptotic regulation as an adaptation process for 835 MHz RF-EMF expo\n",
            "sure.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question:  What animal has been used?\n",
            "ANSWER:   c57bl / 6 mice\n",
            "\n",
            "Question:  How many animals were used?\n",
            "ANSWER:   c57bl / 6 mice\n",
            "\n",
            "Question:  What is the signal frequency?\n",
            "ANSWER:   835 mhz\n",
            "\n",
            "Question:  How much W/kg was used?\n",
            "ANSWER:   4 . 0 w / kg\n",
            "\n",
            "\n",
            "\n",
            "Context [ 9 ]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Mobile communications are expanded day by day and bring along their potential adverse effects on hum\n",
            "an brain. One of the affected brain regions is known as hippocampus which is the most studied struct\n",
            "ure because of its well documented role in learning and memory. The major intracellular signaling pa\n",
            "thway implemented in memory formation in the hippocampus is N-methyl-D-aspartate (NMDA)-dependent pa\n",
            "thway including activation of kinases. Experimental animal studies have demonstrated several effects\n",
            " of short and/or long term RF-EMR exposure on cognitive functions and behaviors of animals. In the l\n",
            "iterature, little is known about the effects of RF-EMR exposure on NMDA receptor signalling pathway.\n",
            " Therefore, in the present study an attempt was taken to demonstrate possible effects of acute and c\n",
            "hronic 900 MHz RF-EMR exposure on both passive avoidance behavior and hippocampal level of the enzym\n",
            "es from NMDA receptor related signalling pathway including p44/42 MAPK and it is phosphorylated form\n",
            " (phopsho p44/42 MAPK) using Western Blotting technique. Rats were divided into following groups: Sh\n",
            "am rats, rats exposed to 900 MHz RF-EMR for 2h/day for 1 (acute) or 10 (chronic) weeks, respectively\n",
            ". Overall results indicated that both acute and chronic exposure to 900 MHz RF-EMR can impair passiv\n",
            "e avoidance behavior with minor effect on behavior of chronic group of rats. In addition, hippocampa\n",
            "l levels of both p44/42 MAPK and phopsho p44/42 MAPK were significantly higher in chronic group of r\n",
            "ats. These findings demonstrated that different duration time (1 week versus 10 weeks) of 900 MHz RF\n",
            "-EMR exposure has different effects on both passive avoidance behavior of rats and hippocampal level\n",
            "s of p44/42 MAPK and phopsho p44/42 MAPK from NMDA dependent pathway.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question:  What animal has been used?\n",
            "ANSWER:   rats\n",
            "\n",
            "Question:  How many animals were used?\n",
            "ANSWER:  \n",
            "\n",
            "Question:  What is the signal frequency?\n",
            "ANSWER:   900 mhz\n",
            "\n",
            "Question:  How much W/kg was used?\n",
            "ANSWER:   900 mhz\n",
            "\n",
            "\n",
            "\n",
            "Context [ 10 ]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Objectives: The increasing rate of over using cell phones has been considerable in youths and pregna\n",
            "nt women. We examined the effect of mobile phones radiation on genes expression variation on cerebel\n",
            "lum of BALB/c mice before and after of the birth.\n",
            "\n",
            "Materials and methods: In this study, a mobile phone jammer, which is an instrument to prevent recei\n",
            "ving signals between cellular phones and base transceiver stations (two frequencies 900 and 1800 MHz\n",
            ") for exposure was used and twelve pregnant mice (BALB/c) divided into two groups (n=6), first group\n",
            " irradiated in pregnancy period (19th day), the second group did not irradiate in pregnancy period. \n",
            "After childbirth, offspring were classified into four groups (n=4): Group1: control, Group 2: B1 (Ir\n",
            "radiated after birth), Group 3: B2 (Irradiated in pregnancy period and after birth), Group 4: B3 (Ir\n",
            "radiated in pregnancy period). When maturity was completed (8-10 weeks old), mice were dissected and\n",
            " cerebellum was isolated. The expression level of bax, bcl-2, p21 and p53 genes examined by real-tim\n",
            "e reverse transcription polymerase chain reaction (Real-Time RT- PCR).\n",
            "\n",
            "Results: The data showed that mobile phone radio waves were ineffective on the expression level of b\n",
            "cl-2 and p53 genes) P>0.05(. Also gene expression level of bax decreased and gene expression level o\n",
            "f p21 increased comparing to the control group (P<0.05).\n",
            "\n",
            "Conclusion: From the obtained data it could be concluded that the mobile phone radiations did not in\n",
            "duce apoptosis in cells of the cerebellum and the injured cells can be repaired by cell cycle arrest\n",
            ".\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question:  What animal has been used?\n",
            "ANSWER:   balb / c mice\n",
            "\n",
            "Question:  How many animals were used?\n",
            "ANSWER:   twelve\n",
            "\n",
            "Question:  What is the signal frequency?\n",
            "ANSWER:   900 and 1800 mhz\n",
            "\n",
            "Question:  How much W/kg was used?\n",
            "ANSWER:   twelve\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "NE5v3CmhzrhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the length of test data\n",
        "length_fine = len(test_context)"
      ],
      "metadata": {
        "id": "A68XkIxzhmJ0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "c263ab57-606d-4043-8d2f-ecd3cabdafb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# It shows accuracy about 76% in the above\n",
        "# Now need to get f1 score as alternative\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "preds = bert_model.predict(fine_train_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "1nE3UUpixZ1q",
        "outputId": "6fd1072f-7282-41dd-e3bb-75a0d9f5ccbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_indexes = np.argmax(preds[0], axis=-1)\n",
        "end_indexes = np.argmax(preds[1], axis=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "jjAKjOM2xXPC",
        "outputId": "cafa94a7-d6e9-4946-85b6-74201a19b0cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indexes = dev_bert_input[0]"
      ],
      "metadata": {
        "id": "EVW3jiPzzfmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_sentences = []\n",
        "fine_untokenized = []\n",
        "\n",
        "for j in range(len(start_indexes)):\n",
        "  sentence = []\n",
        "  # Saving each tokenized word into the list of sentence (sentence = [])\n",
        "  for i in range(start_indexes[j], end_indexes[j]+1):\n",
        "    token_based_word = reverse_token_dict[indexes[j][i]]\n",
        "    sentence.append(token_based_word)\n",
        "  sentence_string = \"\"\n",
        "\n",
        "  for w in sentence:\n",
        "    # Special token ## delete, if a token starts with ##\n",
        "    if w.startswith(\"##\"):\n",
        "      w = w.replace(\"##\", \"\")\n",
        "    # If a token has no ##, a space added to the word\n",
        "    else:\n",
        "      w = \" \" + w\n",
        "    # Putting together all the tokens in list format\n",
        "    sentence_string += w\n",
        "  \n",
        "  # If senetence_string starts with a space (\" \"), delete the space\n",
        "  if sentence_string.startswith(\" \"):\n",
        "    sentence_string = \"\" + sentence_string[1:]\n",
        "  \n",
        "  # After putting together all the list of tokens, and assigning it in the list of the 'Untokenized'\n",
        "  untokenized.append(sentence_string)\n",
        "  sentences.append(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "5WCrTICQ0FHY",
        "outputId": "b429cfd0-f09f-4bf2-e328-90ec43dea313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-98-68161038ed14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m# Saving each tokenized word into the list of sentence (sentence = [])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_indexes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_indexes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtoken_based_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreverse_token_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_based_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0msentence_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'indexes' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(fine_sentences[:30])\n",
        "print('\\n')\n",
        "print(fine_untokenized[:30])"
      ],
      "metadata": {
        "id": "nh_C4fZE0fDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the answers into list format\n",
        "fine_test_answers = []\n",
        "for i in range(length_fine):\n",
        "  fine_test_answer = []\n",
        "  texts_dict = test_context['answers'][i]\n",
        "\n",
        "  for j in range(len(texts_dict)):\n",
        "    dev_answer.append(texts_dict[j]['text'])\n",
        "  dev_answers.append(dev_answer)"
      ],
      "metadata": {
        "id": "jSQ75_Lb0gsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_answers[0:10]"
      ],
      "metadata": {
        "id": "AUPUThek1EwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the answers\n",
        "dev_tokens = []\n",
        "for i in dev_answers:\n",
        "  dev_tokened = []\n",
        "  for j in i:\n",
        "    temp_token = tokenizer.tokenize(j)\n",
        "    # Tokenize an answer\n",
        "    temp_token.pop(0)\n",
        "    # [CLS] elimination\n",
        "    temp_token.pop(-1)\n",
        "    # [SEP] elimination\n",
        "    dev_tokened.append(temp_token)\n",
        "  dev_tokens.append(dev_tokened)"
      ],
      "metadata": {
        "id": "mY6j6LUt1Ftl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dev_tokens[:5])"
      ],
      "metadata": {
        "id": "VKjqSjfw1HZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting tokenized answers into sentences\n",
        "# And putting all together\n",
        "dev_answer_lists = []\n",
        "for dev_answers in dev_tokens:\n",
        "  dev_answer_list = []\n",
        "  for dev_answer in dev_answers:\n",
        "    dev_answer_string = \" \".join(dev_answer)\n",
        "    dev_answer_list.append(dev_answer_string)\n",
        "  dev_answer_lists.append(dev_answer_list)"
      ],
      "metadata": {
        "id": "rFh5iO6b1Ipa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dev_answer_lists[:5])"
      ],
      "metadata": {
        "id": "sbwATbmn1KGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Untokenizing (including deleting \" ##\" etc)\n",
        "dev_strings_end = []\n",
        "for dev_strings in dev_answer_lists:\n",
        "  dev_strings_processed = []\n",
        "  for dev_string in dev_strings:\n",
        "    dev_string = dev_string.replace(\" ##\", \"\")\n",
        "    dev_strings_processed.append(dev_string)\n",
        "  dev_strings_end.append(dev_strings_processed)"
      ],
      "metadata": {
        "id": "9Pnxs8Ro1LMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_answers = dev_strings_end"
      ],
      "metadata": {
        "id": "5VhcS7vC1MxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dev_answers[:5])"
      ],
      "metadata": {
        "id": "xgnNN7Bv1NvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# F1 & EM function defined by myself\n",
        "from collections import Counter\n",
        "import string\n",
        "\n",
        "# In normalize step, doing some works such as converting words into lower case,\n",
        "# and deleting punctuations, clearing unnecessary spaces etc\n",
        "def normalize_answer(s):\n",
        "  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "  def remove_articles(text):\n",
        "    return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "  \n",
        "  def white_space_fix(text):\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "  def remove_punc(text):\n",
        "    exclude = set(string.punctuation)\n",
        "    return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "  def lower(text):\n",
        "    return text.lower()\n",
        "  \n",
        "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "  prediction_tokens = normalize_answer(prediction).split()\n",
        "  ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "  common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "  num_same = sum(common.values())\n",
        "  if num_same == 0:\n",
        "    return 0\n",
        "  precision = 1.0 * num_same / len(prediction_tokens)\n",
        "  recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "  f1 = (2 * precision * recall) / (precision + recall)\n",
        "  return f1\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "  return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "  scores_for_ground_truths = []\n",
        "  for ground_truth in ground_truths:\n",
        "    score = metric_fn(prediction, ground_truth)\n",
        "    scores_for_ground_truths.append(score)\n",
        "  return max(scores_for_ground_truths)"
      ],
      "metadata": {
        "id": "Ysb0xAlt1PF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating F1 Score\n",
        "# Theoretical Max : 88%\n",
        "f1_sum_fine = 0\n",
        "\n",
        "for i in range(len(untokenized)):\n",
        "  f1 = metric_max_over_ground_truths(f1_score, untokenized[i], dev_answers[i])\n",
        "  f1_sum_fine += f1\n",
        "print(\"f1 score: \", f1_sum_fine/length_fine)"
      ],
      "metadata": {
        "id": "TKKgCe121Rwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating EM score\n",
        "# Theoretical Max : 80%\n",
        "EM_sum_fine = 0\n",
        "\n",
        "for i in range(len(untokenized)):\n",
        "  EM = metric_max_over_ground_truths(exact_match_score, untokenized[i], dev_answers[i])\n",
        "  EM_sum_fine += EM\n",
        "print(\"EM score: \", EM_sum_fine/length_fine)"
      ],
      "metadata": {
        "id": "539fb3v41TK-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}